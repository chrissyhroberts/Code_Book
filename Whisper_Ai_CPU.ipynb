{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Whisper Ai - General Purpose CPU Method\n",
        "\n",
        "Whisper Ai from Open Ai is a powerful tool for automatic transcription and translation of audio files.\n",
        "\n",
        "Many users need to carry out transcription and translation in a secure context, i.e. on a personal or work laptop. They are less likely to have high GPU availability and so may need to use CPU for compute. This can be slow, but works reasonably well if you have some time on your hands.\n",
        "\n",
        "This python script provides a simple method for using Whisper Ai on a laptop's CPUs. It works on Macs which currently don't have support for running analysis like this on the GPUs because the torch package doesn't yet support the Apple Metal framework for GPU based computation.\n",
        "\n",
        "python.\n",
        "\n",
        "\n",
        "```{import torch}\n",
        "import whisper\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import ffmpeg\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "if len(sys.argv) < 2:\n",
        "    print(\"Usage: python whisper_transcribe.py <audio_file>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "file_path = sys.argv[1]\n",
        "\n",
        "if not os.path.isfile(file_path):\n",
        "    print(f\"Error: File '{file_path}' not found.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# ‚úÖ Force CPU execution\n",
        "device = \"cpu\"\n",
        "print(f\"Using device: {device} (MPS not fully supported yet)\")\n",
        "\n",
        "# Load Whisper model on CPU using the tiny model\n",
        "model = whisper.load_model(\"tiny\").to(device)\n",
        "\n",
        "def get_audio_duration(file_path):\n",
        "    \"\"\"Returns the duration of the audio file in seconds using FFmpeg.\"\"\"\n",
        "    try:\n",
        "        probe = ffmpeg.probe(file_path)\n",
        "        return float(probe['format']['duration'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting audio duration: {e}\")\n",
        "        return None\n",
        "\n",
        "def transcribe_segment(segment):\n",
        "    \"\"\"Transcribes a segment of audio (returns timestamped result).\"\"\"\n",
        "    return segment[\"start\"], segment[\"text\"]\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    \"\"\"Multithreaded transcription of an audio file using Whisper.\"\"\"\n",
        "    print(f\"üé§ Processing: {file_path}\")\n",
        "\n",
        "    audio_duration = get_audio_duration(file_path)\n",
        "    if audio_duration is None:\n",
        "        print(\"‚ö†Ô∏è Could not determine audio duration. Progress will be approximate.\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    results = []\n",
        "\n",
        "    # ‚úÖ Run Whisper transcription\n",
        "    with tqdm(total=audio_duration, unit=\" sec\", dynamic_ncols=True) as pbar:\n",
        "        result = model.transcribe(file_path, verbose=False, fp16=False)\n",
        "\n",
        "        # ‚úÖ Use a process pool to manage parallel execution\n",
        "        with ProcessPoolExecutor(max_workers=4) as executor:  # Use 4 workers (adjust as needed)\n",
        "            future_to_segment = {executor.submit(transcribe_segment, seg): seg for seg in result[\"segments\"]}\n",
        "            \n",
        "            for future in future_to_segment:\n",
        "                start, text = future.result()\n",
        "                results.append((start, text))\n",
        "                \n",
        "                # ‚úÖ Update progress bar\n",
        "                elapsed_time = time.time() - start_time\n",
        "                pbar.update(start - pbar.n)\n",
        "                pbar.set_description(\"‚è≥ Transcribing...\")\n",
        "    \n",
        "    # ‚úÖ Sort results by start time\n",
        "    results.sort()\n",
        "\n",
        "    # ‚úÖ Save transcript\n",
        "    transcript_path = file_path + \".txt\"\n",
        "    with open(transcript_path, \"w\") as f:\n",
        "        f.write(\" \".join(text for _, text in results))\n",
        "\n",
        "    print(f\"\\n‚úÖ Transcription complete! Saved to: {transcript_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    transcribe_audio(file_path)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "To run this you simply type\n",
        "\n",
        "\\`python whisper_transcribe.py <audio_file>\\`\n",
        "\n",
        "and let it run.\n",
        "\n",
        "You'll need to add the various packages to python before you begin. Examples available at this repo\n",
        "\n",
        "<https://github.com/chrissyhroberts/WHISPER_AI_TRANSCRIPTS_GENERAL_PURPOSE>"
      ],
      "id": "ba9722fc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}