[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Code Book",
    "section": "",
    "text": "Introduction\nThis code book brings together useful pieces of code that I have created over the years.\n.\n\nUseful Resources Elswhere\nR for Data Science\nTidy Modelling with R\n\n\nPublications & Documents\nDocuments published by our team",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "R_basics.html",
    "href": "R_basics.html",
    "title": "1  Basics of R",
    "section": "",
    "text": "1.1 Load Libraries\nLibraries (also known as packages) allow R to do things it can’t do out of the box.\nYou can install packages using the install.packages(\"packagename\") syntax. If you haven’t used R before then you should first install the tidyverse packages by typing install.packages(\"tidyverse\")\nYou should always load the libraries you’ll be using at the top of your script.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#write-csv-files",
    "href": "R_basics.html#write-csv-files",
    "title": "1  Basics of R",
    "section": "1.2 Write CSV files",
    "text": "1.2 Write CSV files\nWe’ll be using the iris dataset, which is built in and which can be accessed by typing iris.\nWe need a toy dataset to work with, so let’s start by saving a copy of iris as a csv file. This is backwards from what we normally do, which is to save a data set at the end, but will serve our purpose here.\n\nwrite_csv(iris, \"iris.csv\")  # Saving to CSV\n\nCheck that it appears in the files list on the right",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#read-csv-files",
    "href": "R_basics.html#read-csv-files",
    "title": "1  Basics of R",
    "section": "1.3 Read CSV files",
    "text": "1.3 Read CSV files\nNext let’s read the file back in and assign it to a data frame\nThe &lt;- symbol assigns the data contained in the file to an object in the R environment.\nYou can have as many or as few objects as you need.\n\niris_data &lt;- read_csv(\"iris.csv\")\n\nRows: 150 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Species\ndbl (4): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe messages are important. Check which variables have been assigned to which class\n\nchr:\n\nFull Form: character\nUsed for columns containing text data (strings).\n\ndbl or num:\n\nFull Form: double/number\nRepresents floating-point numbers (numeric values with decimal places).\nThis is the most common numeric data type used in R for real numbers.\n\nint:\n\nFull Form: integer\nUsed for whole numbers.\n\nlgl:\n\nFull Form: logical\nUsed for boolean (TRUE/FALSE) values.\n\nfct:\n\nFull Form: factor\nUsed for categorical data (discrete values like categories, levels, or groups).\n\ndttm:\n\nFull Form: date-time\nRepresents date and time objects (POSIXct class) which include both date and time information.\n\ndate:\n\nFull Form: Date\nUsed for date objects, containing only date information (year, month, day).\n\ntime:\n\nFull Form: time\nUsed for time objects (sometimes seen when working with time series data, though less common than date or POSIXct).\n\nlst:\n\nFull Form: list\nRepresents a list column, which can hold any type of R object, including vectors, data frames, or even other lists.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#look-at-the-structure-of-an-object",
    "href": "R_basics.html#look-at-the-structure-of-an-object",
    "title": "1  Basics of R",
    "section": "1.4 Look at the structure of an object",
    "text": "1.4 Look at the structure of an object\n\nstr(iris_data)\n\nspc_tbl_ [150 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Sepal.Length = col_double(),\n  ..   Sepal.Width = col_double(),\n  ..   Petal.Length = col_double(),\n  ..   Petal.Width = col_double(),\n  ..   Species = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#look-at-the-contents-of-an-object",
    "href": "R_basics.html#look-at-the-contents-of-an-object",
    "title": "1  Basics of R",
    "section": "1.5 Look at the contents of an object",
    "text": "1.5 Look at the contents of an object\n\niris_data\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n\n1.5.1 Summary of the data set\n\nsummary(iris_data)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n   Species         \n Length:150        \n Class :character  \n Mode  :character",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#pipes",
    "href": "R_basics.html#pipes",
    "title": "1  Basics of R",
    "section": "1.6 Pipes",
    "text": "1.6 Pipes\nThe %&gt;% pipe operator, commonly called the “pipe,” is one of the most important tools in the tidyverse. It is used to pass the result of one function into the next function, making your code cleaner and easier to read by chaining operations together.\n\n1.6.1 How the Pipe Works:\nThe pipe takes the output of the expression on its left and passes it as the first argument to the function on its right.\n\\[ \\texttt{result} \\leftarrow \\texttt{data} \\\\ \\hspace{2cm} \\% \\! &gt; \\! \\% \\, \\texttt{operation}_1 \\\\ \\hspace{2cm} \\% \\! &gt; \\! \\% \\, \\texttt{operation}_2 \\\\ \\hspace{2cm} \\vdots \\\\ \\hspace{2cm} \\% \\! &gt; \\! \\% \\, \\texttt{operation}_n\\]\n\n\n1.6.2 Simple Explanation:\n\nWithout the pipe, you would need to nest functions, which can make the code harder to read:\n\nsummarise(group_by(filter(df, Species == \"setosa\"), Species), mean_length = mean(Sepal.Length))\n\nWith the %&gt;% pipe, you can write it more readable by breaking each step down:\n\niris_data %&gt;%\n  filter(Sepal.Length &gt;5.8) %&gt;%\n  group_by(Species) %&gt;%\n  summarise(mean_length = mean(Sepal.Length))\n\n# A tibble: 2 × 2\n  Species    mean_length\n  &lt;chr&gt;            &lt;dbl&gt;\n1 versicolor        6.34\n2 virginica         6.72\n\n\n\n\n\n1.6.3 Benefits of Using the Pipe:\n\nImproves readability: It reads like a logical sequence of steps.\nReduces the need for intermediate variables: You don’t need to create multiple intermediate objects.\nSimplifies function chaining: Functions are applied one after the other, making it clear what happens at each step.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#single-table-verbs-basic",
    "href": "R_basics.html#single-table-verbs-basic",
    "title": "1  Basics of R",
    "section": "1.7 Single Table Verbs (basic)",
    "text": "1.7 Single Table Verbs (basic)\nAll the main actions in tidyverse take a tibble (the new name for a dataframe), do something with it and then return another tibble. These are the ‘single table verbs’.\n\nThese are the main functions you’ll need to learn.\nAll of them accept lists, where you separate items with a comma.\n\n\n1.7.1 Filter\n\nfilter():\n\nFilters rows based on specified conditions.\nReturns only the rows that meet the condition(s).\n\n\n\niris_data %&gt;% \n  filter(\n    Species == \"setosa\",\n    Sepal.Length &gt; 4.3)\n\n# A tibble: 49 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 39 more rows\n\n\n\n\n1.7.2 Logic\n\nYou can provide logical operators` to any verb to make complex queries\n\n== Equals\n!= Not Equals\n&gt; more than\n&lt; less than\n&gt;= more than or equal to\n&lt;= less than or equal to\n| or\n& and\n! not\n+ add\n- subtract\n%% modulo\n\n\n\n\n1.7.3 AND (&)\nFiltering rows where Sepal.Length is greater than 5 AND Species is not setosa\n\niris_data %&gt;% \n  filter(\n    Sepal.Length &gt; 5 & Species != \"setosa\"\n    )\n\n# A tibble: 96 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          6.6         2.9          4.6         1.3 versicolor\n 9          5.2         2.7          3.9         1.4 versicolor\n10          5.9         3            4.2         1.5 versicolor\n# ℹ 86 more rows\n\n\n\n\n1.7.4 OR (|)\nFiltering rows where Sepal.Length is less than 5 OR Species is setosa\n\niris_data %&gt;% \n  filter(\n    Sepal.Length &lt; 5 | Species == \"setosa\"\n    )\n\n# A tibble: 52 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 42 more rows\n\n\n\n\n1.7.5 NOT (!)\nFiltering rows where Species is NOT “setosa” by negating the test with ! placed at the start. Compare to above where != was used for not-equal. Here it tests if the species equals setosa, then returns all rows where that is NOT true.\n\niris_data %&gt;% \n  filter(\n    !(Species == \"setosa\")\n    )\n\n# A tibble: 100 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# ℹ 90 more rows\n\n\n\n\n1.7.6 \n\n\n1.7.7 Arrange\n\narrange():\n\nOrders rows of a tibble by one or more columns.\nCan sort in ascending or descending order.\nUsing a list will sort by item 1, then item 2, then item 3.\nAs with filter, this function can accept a list of actions that are carried out sequentially\n\n\n\niris_data %&gt;% \n  arrange(\n    Sepal.Length,\n    Sepal.Width,\n    Species\n    )\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.2          1.4         0.2 setosa \n 8          4.6         3.4          1.4         0.3 setosa \n 9          4.6         3.6          1           0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# ℹ 140 more rows\n\n\n\niris_data %&gt;% \n  arrange(\n    !Sepal.Length,\n    Sepal.Width,\n    Species\n    )\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     \n 1          5           2            3.5         1   versicolor\n 2          6           2.2          4           1   versicolor\n 3          6.2         2.2          4.5         1.5 versicolor\n 4          6           2.2          5           1.5 virginica \n 5          4.5         2.3          1.3         0.3 setosa    \n 6          5.5         2.3          4           1.3 versicolor\n 7          6.3         2.3          4.4         1.3 versicolor\n 8          5           2.3          3.3         1   versicolor\n 9          4.9         2.4          3.3         1   versicolor\n10          5.5         2.4          3.8         1.1 versicolor\n# ℹ 140 more rows\n\n\n\n\n1.7.8 Select\n\nselect():\n\nSelects specific columns from a data frame or tibble.\nUseful for reducing data to only the columns of interest.\nAs with other verbs, a sequential set of actions is possible\n\n\n\niris_data %&gt;% \n  select(\n    Sepal.Length, \n    Species\n    )\n\n# A tibble: 150 × 2\n   Sepal.Length Species\n          &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1 setosa \n 2          4.9 setosa \n 3          4.7 setosa \n 4          4.6 setosa \n 5          5   setosa \n 6          5.4 setosa \n 7          4.6 setosa \n 8          5   setosa \n 9          4.4 setosa \n10          4.9 setosa \n# ℹ 140 more rows\n\n\n\nIt’s also possible to negate an action,\n\n\niris_data %&gt;% \n  select(\n    !Species\n    )\n\n# A tibble: 150 × 4\n   Sepal.Length Sepal.Width Petal.Length Petal.Width\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2\n 2          4.9         3            1.4         0.2\n 3          4.7         3.2          1.3         0.2\n 4          4.6         3.1          1.5         0.2\n 5          5           3.6          1.4         0.2\n 6          5.4         3.9          1.7         0.4\n 7          4.6         3.4          1.4         0.3\n 8          5           3.4          1.5         0.2\n 9          4.4         2.9          1.4         0.2\n10          4.9         3.1          1.5         0.1\n# ℹ 140 more rows\n\n\n\n\n1.7.9 Mutate\n\nmutate():\n\nAdds new columns or modifies existing columns in a tibble.\nCommonly used for creating calculated columns.\n\n\n\niris_data %&gt;%\n  mutate(\n    Petal.Ratio = Petal.Length / Petal.Width,\n    Petal.Area = Petal.Length * Petal.Width\n         )\n\n# A tibble: 150 × 7\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Petal.Ratio\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2 setosa         7   \n 2          4.9         3            1.4         0.2 setosa         7   \n 3          4.7         3.2          1.3         0.2 setosa         6.5 \n 4          4.6         3.1          1.5         0.2 setosa         7.5 \n 5          5           3.6          1.4         0.2 setosa         7   \n 6          5.4         3.9          1.7         0.4 setosa         4.25\n 7          4.6         3.4          1.4         0.3 setosa         4.67\n 8          5           3.4          1.5         0.2 setosa         7.5 \n 9          4.4         2.9          1.4         0.2 setosa         7   \n10          4.9         3.1          1.5         0.1 setosa        15   \n# ℹ 140 more rows\n# ℹ 1 more variable: Petal.Area &lt;dbl&gt;\n\n\n\n\n1.7.10 Pivot_longer\n\npivot_longer():\n\nConverts wide data to long format (stacking columns into rows).\nUseful for transforming data when working with multiple measurement columns.\nWe’ll use relig_income as an example data set. This is in WIDE format.\n\n\n\nrelig_income\n\n# A tibble: 18 × 11\n   religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Agnostic      27        34        60        81        76       137        122\n 2 Atheist       12        27        37        52        35        70         73\n 3 Buddhist      27        21        30        34        33        58         62\n 4 Catholic     418       617       732       670       638      1116        949\n 5 Don’t k…      15        14        15        11        10        35         21\n 6 Evangel…     575       869      1064       982       881      1486        949\n 7 Hindu          1         9         7         9        11        34         47\n 8 Histori…     228       244       236       238       197       223        131\n 9 Jehovah…      20        27        24        24        21        30         15\n10 Jewish        19        19        25        25        30        95         69\n11 Mainlin…     289       495       619       655       651      1107        939\n12 Mormon        29        40        48        51        56       112         85\n13 Muslim         6         7         9        10         9        23         16\n14 Orthodox      13        17        23        32        32        47         38\n15 Other C…       9         7        11        13        13        14         18\n16 Other F…      20        33        40        46        49        63         46\n17 Other W…       5         2         3         4         2         7          3\n18 Unaffil…     217       299       374       365       341       528        407\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\n\nTo pivot this data set, we provide\n\ncols = : The columns that will be used to pivot in to the new ‘values’ column. Here we want all columns from the dataset except the religion column which provides the labels, so simply exclude that one with !religion. You could also specify specific columns with a list cols = c(\"&lt;$10k\",\"$10-20k\",\"$20-30k\")\n\n\n\nrelig_income_long &lt;- relig_income %&gt;%\n    pivot_longer(\n      cols = !religion, \n      names_to = \"income\", \n      values_to = \"n\")\n\nrelig_income_long\n\n# A tibble: 180 × 3\n   religion income                 n\n   &lt;chr&gt;    &lt;chr&gt;              &lt;dbl&gt;\n 1 Agnostic &lt;$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic &gt;150k                 84\n10 Agnostic Don't know/refused    96\n# ℹ 170 more rows\n\n\n\n\n1.7.11 Pivot_wider\nThis is the exact opposite of pivot_longer. You’ll be taking the values of a column (here income) that you want to pivot to be the new column names in the wide format tibble, then distributing the values of another column (here count) to the appropriate columns and rows (here religion).\n\nrelig_income_long %&gt;%\n  pivot_wider(\n    id_cols = religion,\n    names_from = income, \n    values_from = n\n    )\n\n# A tibble: 18 × 11\n   religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k`\n   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 Agnostic      27        34        60        81        76       137        122\n 2 Atheist       12        27        37        52        35        70         73\n 3 Buddhist      27        21        30        34        33        58         62\n 4 Catholic     418       617       732       670       638      1116        949\n 5 Don’t k…      15        14        15        11        10        35         21\n 6 Evangel…     575       869      1064       982       881      1486        949\n 7 Hindu          1         9         7         9        11        34         47\n 8 Histori…     228       244       236       238       197       223        131\n 9 Jehovah…      20        27        24        24        21        30         15\n10 Jewish        19        19        25        25        30        95         69\n11 Mainlin…     289       495       619       655       651      1107        939\n12 Mormon        29        40        48        51        56       112         85\n13 Muslim         6         7         9        10         9        23         16\n14 Orthodox      13        17        23        32        32        47         38\n15 Other C…       9         7        11        13        13        14         18\n16 Other F…      20        33        40        46        49        63         46\n17 Other W…       5         2         3         4         2         7          3\n18 Unaffil…     217       299       374       365       341       528        407\n# ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;,\n#   `Don't know/refused` &lt;dbl&gt;\n\n\n\n\n1.7.12 Using pipes and verbs together\nTo do a series of things to a tibble, you simply pipe the verbs\n\nrelig_income %&gt;%\n    pivot_longer(  \n      cols = !religion, \n      names_to = \"income\", \n      values_to = \"count\") %&gt;% \n\n    filter(\n      income != \"Don't know/refused\"\n          ) %&gt;% \n\n    arrange(\n      income,\n      religion\n          )\n\n# A tibble: 162 × 3\n   religion                income  count\n   &lt;chr&gt;                   &lt;chr&gt;   &lt;dbl&gt;\n 1 Agnostic                $10-20k    34\n 2 Atheist                 $10-20k    27\n 3 Buddhist                $10-20k    21\n 4 Catholic                $10-20k   617\n 5 Don’t know/refused      $10-20k    14\n 6 Evangelical Prot        $10-20k   869\n 7 Hindu                   $10-20k     9\n 8 Historically Black Prot $10-20k   244\n 9 Jehovah's Witness       $10-20k    27\n10 Jewish                  $10-20k    19\n# ℹ 152 more rows\n\n\n\nYou can include notes if it helps you.\nUsing indentation also really helps\n\n\nrelig_income %&gt;%\n  \n    # Pivot the table to be long format\n    pivot_longer(  \n      cols = !religion, \n      names_to = \"income\", \n      values_to = \"count\") %&gt;% \n  \n    # Remove lines where no income data was provided\n    filter(\n      income != \"Don't know/refused\"\n          ) %&gt;% \n  \n    # Sort the data to sho \n    arrange(\n      income,\n      religion\n          )\n\n# A tibble: 162 × 3\n   religion                income  count\n   &lt;chr&gt;                   &lt;chr&gt;   &lt;dbl&gt;\n 1 Agnostic                $10-20k    34\n 2 Atheist                 $10-20k    27\n 3 Buddhist                $10-20k    21\n 4 Catholic                $10-20k   617\n 5 Don’t know/refused      $10-20k    14\n 6 Evangelical Prot        $10-20k   869\n 7 Hindu                   $10-20k     9\n 8 Historically Black Prot $10-20k   244\n 9 Jehovah's Witness       $10-20k    27\n10 Jewish                  $10-20k    19\n# ℹ 152 more rows\n\n\n\n\n1.7.13 Group_by\n\nThese verbs create a data frame with one row per group, where the variables are a summary of values. group_by groups data by one or more columns,\n\ngroup_by allows you to create groupwise calculations like group means\nThis approach does not collapse the data, so if you wanted three rows, with a single mean value for each one, you need to do something else (see below).\nAlways ungroup() if you plan to do further calculations on individual rows.\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  mutate(\n    Sepal.length.mean = mean(Sepal.Length),\n    weight = Sepal.Length - Sepal.length.mean\n  )%&gt;% \n  ungroup()\n\n# A tibble: 150 × 7\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species Sepal.length.mean\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1          5.1         3.5          1.4         0.2 setosa               5.01\n 2          4.9         3            1.4         0.2 setosa               5.01\n 3          4.7         3.2          1.3         0.2 setosa               5.01\n 4          4.6         3.1          1.5         0.2 setosa               5.01\n 5          5           3.6          1.4         0.2 setosa               5.01\n 6          5.4         3.9          1.7         0.4 setosa               5.01\n 7          4.6         3.4          1.4         0.3 setosa               5.01\n 8          5           3.4          1.5         0.2 setosa               5.01\n 9          4.4         2.9          1.4         0.2 setosa               5.01\n10          4.9         3.1          1.5         0.1 setosa               5.01\n# ℹ 140 more rows\n# ℹ 1 more variable: weight &lt;dbl&gt;\n\n\n\nSummarise() and reframe() are almost always used in combination with group_by().\nAs with all verbs, you can provide lists to group_by(), reframe() and summarise().\n1.7.14 Reframe\n\nPurpose: A more flexible way to return multiple results for each group without reducing it to one row per group.\nTypical Use: Used when you want to keep multiple rows per group but still perform summary operations. i.e. when creating denominators etc.\nBehavior: Allows for returning multiple rows or multiple values for each group, so it doesn’t necessarily collapse the data.\nIf you choose to include any of the original variables in your reframed tibble, the resulting tibble will have the same dimensions as your original. Here the groupwise counts are added to count and the groupwise mean sepal lengths are added to mean.sepal.length.\n\n\niris_data %&gt;%\n  group_by(Species) %&gt;%\n  reframe(\n    Sepal.Length,\n    Sepal.Width,\n    count = n(),\n    sepal.length.mean = mean(Sepal.Length),\n    weight = Sepal.Length-sepal.length.mean\n  ) \n\n# A tibble: 150 × 6\n   Species Sepal.Length Sepal.Width count sepal.length.mean   weight\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;             &lt;dbl&gt;    &lt;dbl&gt;\n 1 setosa           5.1         3.5    50              5.01  0.0940 \n 2 setosa           4.9         3      50              5.01 -0.106  \n 3 setosa           4.7         3.2    50              5.01 -0.306  \n 4 setosa           4.6         3.1    50              5.01 -0.406  \n 5 setosa           5           3.6    50              5.01 -0.00600\n 6 setosa           5.4         3.9    50              5.01  0.394  \n 7 setosa           4.6         3.4    50              5.01 -0.406  \n 8 setosa           5           3.4    50              5.01 -0.00600\n 9 setosa           4.4         2.9    50              5.01 -0.606  \n10 setosa           4.9         3.1    50              5.01 -0.106  \n# ℹ 140 more rows\n\n\n\nIf you choose not to include your original variables, reframe() will present only the new variables.\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  reframe(\n    count = n(),\n    mean.sepal.length = mean(Sepal.Length),\n    median.petal.length = median(Petal.Length),\n    sd.petal.length = sd(Petal.Length)\n  )\n\n# A tibble: 3 × 5\n  Species    count mean.sepal.length median.petal.length sd.petal.length\n  &lt;fct&gt;      &lt;int&gt;             &lt;dbl&gt;               &lt;dbl&gt;           &lt;dbl&gt;\n1 setosa        50              5.01                1.5            0.174\n2 versicolor    50              5.94                4.35           0.470\n3 virginica     50              6.59                5.55           0.552\n\n\n\nsummarise():\n\nCreates summary statistics for each group, such as mean, median, or sum.\nIn this case summarise() creates the same table as the previous example of reframe\nThis shows how you can always use reframe so generally forget about using summarise()\n\n\n\niris_data %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    count = n(),\n    mean.sepal.length = mean(Sepal.Length),\n    median.petal.length = median(Petal.Length)\n            )\n\n# A tibble: 3 × 4\n  Species    count mean.sepal.length median.petal.length\n  &lt;chr&gt;      &lt;int&gt;             &lt;dbl&gt;               &lt;dbl&gt;\n1 setosa        50              5.01                1.5 \n2 versicolor    50              5.94                4.35\n3 virginica     50              6.59                5.55\n\n\n\n\n1.7.15 Rename\n\nrename():\n\nRenames columns in a tibble.\nHelpful for cleaning up column names for clarity.\n\n\n\niris_data %&gt;% \n  rename(\n    var_a = Petal.Length,\n    var_b = Petal.Width\n    )\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width var_a var_b Species\n          &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n 1          5.1         3.5   1.4   0.2 setosa \n 2          4.9         3     1.4   0.2 setosa \n 3          4.7         3.2   1.3   0.2 setosa \n 4          4.6         3.1   1.5   0.2 setosa \n 5          5           3.6   1.4   0.2 setosa \n 6          5.4         3.9   1.7   0.4 setosa \n 7          4.6         3.4   1.4   0.3 setosa \n 8          5           3.4   1.5   0.2 setosa \n 9          4.4         2.9   1.4   0.2 setosa \n10          4.9         3.1   1.5   0.1 setosa \n# ℹ 140 more rows",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#joining",
    "href": "R_basics.html#joining",
    "title": "1  Basics of R",
    "section": "1.8 Joining",
    "text": "1.8 Joining\n\nleft_join() / right_join() / inner_join() / full_join():\\\n\nJoins add columns from one tibble to another, matching the observations using key variables.\nThere are three types of join\n\nA left_join() keeps all observations in x.\nA right_join() keeps all observations in y.\nA full_join() keeps all observations in x and y.\n\nWe’ll use the band_members and band_instruments data frames for this\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\n\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nYou can see that both tibbles contain two variables, of which one is called name. This will be the key variable that is used for joining. R will automatically look for matching variables, and will merge the data semi-automatically. It even works if there’s more than one key variable.\n\n1.8.1 Left join\n\nband_members %&gt;% \nleft_join(\n  band_instruments\n)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nYou can see that the new left_joined tibble now contains three variables.\nAll three band members who were in the band_members tibble are still represented here, but Keith is not included in this tibble because left_join adds new columns to observations that already exist in band_members\n\n\n1.8.2 Right Join\nThe right_join works in exactly the opposite way. Here the right_join adds new columns to the observations of the right hand tibble (i.e. to band_instruments).\n\nright_join(\n  band_members,\n  band_instruments\n)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith &lt;NA&gt;    guitar\n\n\n\n\n1.8.3 Full Join\nThe full_join keeps all the observations and all the columns of both data sets.\n\nfull_join(\n  band_members,\n  band_instruments\n)\n\nJoining with `by = join_by(name)`\n\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "R_basics.html#ggplot",
    "href": "R_basics.html#ggplot",
    "title": "1  Basics of R",
    "section": "1.9 ggplot",
    "text": "1.9 ggplot\nNow we’ve covered the basics of managing and manipulating tibbles, let’s look at the basics of drawing charts in ggplot.\nTo understand the syntax of ggplot, you have to understand how charts created with this system are built in layers. Just like how we pipe data through %&gt;% when handling tibbles, we add new layers to ggplot charts using +\nggplot accepts piped data as an input. The initial ggplot is a blank chart with no axes. Let’s pipe the iris tibble in to a ggplot.\n\niris %&gt;% \n  ggplot() \n\n\n\n\n\n\n\n\nNext we want to describe the ‘aesthetics’ of the plot. This is how we define the variables that will contribute to the axes, groups, points, shapes, fills, areas and so on.\nLet’s provide ggplot with some aesthetics in the form of an x (Sepal.Length) and y (Sepal.Width)\nThis should add the axes which will be appropriately scaled according to the limits of the two variables.\n\niris %&gt;% \n  ggplot(aes(x = Sepal.Length,y = Sepal.Width)) \n\n\n\n\n\n\n\n\nTo add some of the data points to the chart, we need to add a new layer. The type of chart is defined by which “geom\" layer you choose to add next.\nLet’s start simple and draw some points with geom_point(). Remember to add the new layer by putting a + at the end of the last line\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width)\n          ) +\n      geom_point()\n\n\n\n\n\n\n\n\nThis is useful, but doesn’t tell us anything about the points. Colours, grouping, fills etc are defined in the aesthethics, so let’s add some colours to the points according to which species they represent.\nYou can encode colours with color= or colour=\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species\n        )\n          ) +\n      geom_point()\n\n\n\n\n\n\n\n\nIf you provide a continuous variable to color you’ll get a nice result too.\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Petal.Length\n        )\n          ) +\n      geom_point()\n\n\n\n\n\n\n\n\nYou can also use the shape= aesthetic to add different shapes. Here we can now see information on sepal length (x), sepal width (y), species (shape) and petal length (color).\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Petal.Length,\n        shape=Species\n        )\n          ) +\n      geom_point()\n\n\n\n\n\n\n\n\nYou can also use the size= aesthetic to get a very different plot.\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        size=Petal.Length\n        )\n          ) +\n      geom_point()\n\n\n\n\n\n\n\n\n\n1.9.1 Line charts\ngeom_line() draws lines between points\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        )\n          ) +\n      geom_line()\n\n\n\n\n\n\n\n\nIn this case, that makes little sense, because there’s three species. Adding the colour aesthethic groups the data and the lines will be drawn by group\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_line()\n\n\n\n\n\n\n\n\nYou can combine more than one geom by adding extra layers\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_line()+\n      geom_point()\n\n\n\n\n\n\n\n\nand there’s variations like geom_smooth() which makes a nicer line\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_smooth()+\n      geom_point()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n1.9.2 Facets\nFacets allow you to draw multiple panels. facet_grid is a nice way to do this.\nWe’ll facet the line chart by Species. The facet is an arrangement of panels in columns and rows.\nto arrange your facets in rows you provide facet_grid(.~Species)\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_smooth()+\n      geom_point()+\n  facet_grid(.~Species)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTo arrange your facets in columns you provide `facet_grid(Species~.)\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_smooth()+\n      geom_point()+\n  facet_grid(Species~.)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTo arrange your facets around species in columns and on petal size in rows you could do\n\niris %&gt;% \n  mutate(\n    petal_bigger_than_average = Petal.Length &gt;= mean(Petal.Length) \n  ) %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_smooth()+\n      geom_point()+\n  facet_grid(\n    petal_bigger_than_average ~  Species\n    )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nA problem with facets can be to do with axes being locked to the minimum and maximum values of the variable. You may wish to ‘free’ the axes using scales=\"free\", scales=\"free_x\" or scales=\"free_y\"\n\niris %&gt;% \n    ggplot(\n      aes(\n        Sepal.Length,\n        Sepal.Width,\n        color=Species,\n        )\n          ) +\n      geom_smooth()+\n      geom_point()+\n  facet_grid(\n            Petal.Length &gt;= mean(Petal.Length) ~ Species, \n            scales = \"free\"\n            )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally we can see a linear relationship in some of the data. Setosa for instance has a strong correlation between sepal length and sepal width, which does not appear to be true of the other species, regardless of the petal length division.\n\n\n1.9.3 Bar chart\ngeom_bar() – Creates bar plots, either stacked or grouped.\n-   Example: `geom_bar(stat = \"identity\")` for bar heights based on a variable.\nTo plot a count of occurrances in the data use stat=\"count\"\nThe default is a stacked bar chart. For this you only need to provide x.\n\niris_data %&gt;% \n    ggplot(\n      aes(\n        Petal.Length &gt; 4,\n        fill = Species\n        )\n          )+\n  geom_bar(stat=\"count\")\n\n\n\n\n\n\n\n\nYou can change this to a side-by-side chart using `position=“dodge”\n\niris %&gt;% \n    ggplot(\n      aes(\n        Petal.Length &gt; 4,\n        fill = Species\n        )\n          )+\n  geom_bar(stat=\"count\",\n           position = \"dodge\"\n           )\n\n\n\n\n\n\n\n\n\niris %&gt;% \n    ggplot(\n      aes(\n        Petal.Length &gt; 4,\n        fill = Species\n        )\n          )+\n  geom_bar(stat=\"count\"           )+\n  facet_grid(.~Species)\n\n\n\n\n\n\n\n\nIn lots of cases you’ll have precomputed some summaries and will want to print the exact identity values. Let’s reframe the iris data as a set of averages (see the reframe section) and then pipe the result in to a ggplot using stat=\"identity\" Unlike with the stat=\"count\" default, you need to provide both x (grouping) and x (value) data to stat = \"identity.\n\niris_data %&gt;%\n  group_by(Species) %&gt;%\n  reframe(\n    count = n(),\n    mean.sepal.length = mean(Sepal.Length),\n    median.petal.length = median(Petal.Length)\n  ) %&gt;% \n  \n  ggplot(\n    aes(\n      Species,\n      mean.sepal.length,\n      fill=Species  \n      )\n        )+\n  geom_bar(stat=\"identity\")\n\n\n\n\n\n\n\n\n\n\n1.9.4 Columnar charts\ngeom_col() – Similar to geom_bar(), but used when heights are defined by variables instead of counts.\n-   Example: `geom_col()`\nThis is essentially identical to using stat=\"identity\" with a geom_bar()\nWe can also use geom_errorbar() to add confidence intervals.\nNote that geom_errorbar has its own set of aesthethics, to cover the upper (ymax) and lower (ymin) limits.\nLet’s start by making some statistics\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  reframe(\n    count = n(),\n    mean.sepal.length = mean(Sepal.Length),\n    sd.sepal.length = sd(Sepal.Length),\n    lower = mean.sepal.length - (1.96*sd.sepal.length),\n    upper = mean.sepal.length + (1.96*sd.sepal.length)\n  ) \n\n# A tibble: 3 × 6\n  Species    count mean.sepal.length sd.sepal.length lower upper\n  &lt;fct&gt;      &lt;int&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 setosa        50              5.01           0.352  4.32  5.70\n2 versicolor    50              5.94           0.516  4.92  6.95\n3 virginica     50              6.59           0.636  5.34  7.83\n\n\nThen pipe this in to ggplot\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  reframe(\n    count = n(),\n    mean.sepal.length = mean(Sepal.Length),\n    sd.sepal.length = sd(Sepal.Length),\n    lower = mean.sepal.length - (1.96*sd.sepal.length),\n    upper = mean.sepal.length + (1.96*sd.sepal.length)\n  ) %&gt;% \n  \n  ggplot(\n    aes(\n      Species,\n      mean.sepal.length,\n      fill=Species  \n      )\n        )+\n  geom_col()+\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2)\n\n\n\n\n\n\n\n\n\n\n1.9.5 Histograms\ngeom_histogram() – Plots the frequency distribution of continuous data by creating bins.\n-   Example: `geom_histogram()`\nHistograms require only values of x\n\niris %&gt;%\n  ggplot(\n    aes(\n      Petal.Length\n        )\n        )+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nYou can control the number of bins\n\niris %&gt;%\n  ggplot(\n    aes(\n      Petal.Length\n        )\n        )+\n  geom_histogram(bins = 50)\n\n\n\n\n\n\n\n\nYou can also add grouping as before\n\niris %&gt;%\n  ggplot(\n    aes(\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n1.9.6 Density plots\ngeom_density() – Shows the distribution of a continuous variable with a smooth density curve.\n-   Example: `geom_density()`\nDensity plots are very useful when you want to look at distributions of data in different classes. They are similar in many respects to histograms.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_density()\n\n\n\n\n\n\n\n\nYou’ll want to see what’s going on in the overlapping regions, so you can add transparency with alpha=. Transparency can be used in any ggplot.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_density(alpha=0.4)\n\n\n\n\n\n\n\n\n\n\n1.9.7 Boxplots\ngeom_boxplot() – Visualizes the distribution of a variable through quartiles and potential outliers.\n-   Example: `geom_boxplot()`\nThese are a mainstay of epidemiology.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n1.9.8 Violin plots\ngeom_violin() – A hybrid of boxplot and density plot, showing distribution shape along with quartiles.\n-   Example: `geom_violin()`\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n1.9.9 Violin & Box together\nAdding a geom_box() layer to a violin plot can be useful.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_boxplot()+\n  geom_violin()\n\n\n\n\n\n\n\n\nBut this is ugly. and the boxplots are obscured by the violins. The order of the layers in a ggplot matters\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length,\n      fill = Species\n        )\n        )+\n  geom_violin()+\n  geom_boxplot()\n\n\n\n\n\n\n\n\nChanging the order of the layers improves things, but we can control each layer individually by changing it’s mappings. This is the reason why the geoms all have brackets!\nLet’s fill the violin plots, adding some transparency. We’ll also make the boxes on the boxplots a bit narrower so that we can see all the violin data, and let’s remove the outlier points.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length\n        )\n        )+\n  geom_violin(mapping = aes(fill=Species))+\n    geom_boxplot(width=0.1,outliers = F)\n\n\n\n\n\n\n\n\nFinally, let’s add the points, jittering them so that they don’t all line up along the midlines with geom_jitter.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length\n        )\n        )+\n  geom_violin(mapping = aes(fill=Species))+\n    geom_boxplot(width=0.1,outliers = F)+\n  geom_jitter(size=0.3,width = 0.2)\n\n\n\n\n\n\n\n\n\n\n1.9.10 Tile plots\ngeom_tile() – Creates heatmap-like visuals by filling rectangular areas based on values.\n-   Example: `geom_tile()`\nThis kind of heatmap is great for showing the value of a variable (defined with fill) in a grid representing several classes on x and y axis.\n\niris %&gt;%\n  ggplot(\n    aes(\n      Species,\n      Petal.Length&gt;3,\n      fill = Petal.Width\n        )\n        )+\ngeom_tile()\n\n\n\n\n\n\n\n\ngeom_jitter() – Adds small random noise to points, useful for avoiding overplotting.\n-   Example: `geom_jitter()`\ngeom_ribbon() – Fills the area between two y-values (usually a line and its confidence interval).\n-   Example: `geom_ribbon()`\ngeom_text() – Adds text annotations to points in the plot.\n-   Example: `geom_text(aes(label = ...))`\ngeom_errorbar() – Adds error bars to plots (e.g., for displaying variability or uncertainty).\n-   Example: `geom_errorbar()`\n\n\n1.9.11 Separate wider\n\ndf &lt;- tibble(id = 1:3, patient_id = c(\"m-123\", \"f-455\", \"f-123\"))\ndf\n\n# A tibble: 3 × 2\n     id patient_id\n  &lt;int&gt; &lt;chr&gt;     \n1     1 m-123     \n2     2 f-455     \n3     3 f-123     \n\n\nThere are three basic ways to split up a string into pieces\n\n1.9.11.1 With a delimiter\n\ndf %&gt;% \n  separate_wider_delim (\n    patient_id, \n    delim = \"-\", \n    names = c(\"gender\", \"unit\")\n    )\n\n# A tibble: 3 × 3\n     id gender unit \n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 m      123  \n2     2 f      455  \n3     3 f      123  \n\n\n\n\n1.9.11.2 By string length\nHere you provide a set of widths to map new columns to various characters.\nThe example data are in the form m-123 where the m represents gender, the - is just a delimiter and the 123 is the participant ID.\nwe can assign characters with var = n where n is the width of the string in characters.\nwidths = c(gender = 1)\n\nwill assign the first character in the strong to a new variable gender.\n\nwidths = c(gender = 1, 1)\n\nAssigns the first character in the strong to a new variable gender.\nThe next character will be dropped\n\nwidths = c(gender = 1, 1, unit=3)\n\nAssigns the first character in the strong to a new variable gender.\nThe next character will be dropped\nFinally assigns the last 3 characters to a new variable unit\n\n\n\n1.9.11.3 Or by REGEX\nRegular expressions are a poweful language for string matching.\n\ndf %&gt;% \n  separate_wider_regex(\n    patient_id, c(gender = \".\", \".\", unit = \"\\\\d+\"))\n\n# A tibble: 3 × 3\n     id gender unit \n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 m      123  \n2     2 f      455  \n3     3 f      123  \n\n\nA full example is like this\n\ndf %&gt;% \n  separate_wider_position (\n    cols = patient_id,\n    widths = c(gender = 1, 1, unit=3))\n\n# A tibble: 3 × 3\n     id gender unit \n  &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;\n1     1 m      123  \n2     2 f      455  \n3     3 f      123  \n\n\n\n\n\n1.9.12 Unite\nUnite joins columns, or merges them.\n\ndf &lt;- expand_grid(x = c(\"a\", NA), y = c(\"b\", NA))\ndf\n\n# A tibble: 4 × 2\n  x     y    \n  &lt;chr&gt; &lt;chr&gt;\n1 a     b    \n2 a     &lt;NA&gt; \n3 &lt;NA&gt;  b    \n4 &lt;NA&gt;  &lt;NA&gt; \n\n\n\n1.9.12.1 Unite, dropping NAs\n\ndf %&gt;% \n  unite(\n    \"z\",\n    x:y,\n    na.rm = FALSE,\n    remove = FALSE)\n\n# A tibble: 4 × 3\n  z     x     y    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 a_b   a     b    \n2 a_NA  a     &lt;NA&gt; \n3 NA_b  &lt;NA&gt;  b    \n4 NA_NA &lt;NA&gt;  &lt;NA&gt; \n\n\n\n\n1.9.12.2 Unite, removing originals and shirt\n\ndf %&gt;% \n  unite(\n    \"z\",\n    x:y, \n    na.rm = TRUE,\n    remove = FALSE\n    )\n\n# A tibble: 4 × 3\n  z     x     y    \n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 \"a_b\" a     b    \n2 \"a\"   a     &lt;NA&gt; \n3 \"b\"   &lt;NA&gt;  b    \n4 \"\"    &lt;NA&gt;  &lt;NA&gt; \n\n\n\n\n\n1.9.13 Summary of Exclusive Helper Functions:\n\nConditional Operations: case_when(), if_else()\nRange Check: between()\nMissing Value Handling: coalesce(), is.na()\nCumulative Functions: cumsum(), cummean(), cumall(), cumany()\nRow-based Operations: lag(), lead(), nth(), row_number()\nSummarizing or Counting: n(), pmin(), pmax(), any(), all()\n\nThese helper functions are used specifically within verbs like mutate(), filter(), summarise(), arrange(), and others to perform specialized operations inside the context of a single table.\n\n\n1.9.14 Case_when\n\n\n1.9.15",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Basics of R</span>"
    ]
  },
  {
    "objectID": "extract_model_from_geom_smooth.html",
    "href": "extract_model_from_geom_smooth.html",
    "title": "2  Extract Model from geom_smooth model",
    "section": "",
    "text": "Example code to extract x and y values from a geom_smooth.\nThe practical example used her is to take discrete values of a value n at monthly intervals across a year. The geom smooth is used to draw a smoothed line across the chart. Then the values of n are extracted from the model before being floored so that we can estimate a new value for each month.\nThis would be helpful where you have a situation in which there’s a month missing in the middle, or where there’s an outlier or two. You could achieve the same thing using a regression, but this is a useful approach that ultimately leans back on to polynomial regressions.\n\nlibrary(tidyverse)\nlibrary(knitr)\n\nMake a dummy data set with 11 observations and increasing number\n\ndf&lt;-tibble(\n           month=1:11,\n           n = c(100,140,200,260,360,470,560,630,770,990,1100)\n           )\nkable(df)\n\n\n\n\nmonth\nn\n\n\n\n\n1\n100\n\n\n2\n140\n\n\n3\n200\n\n\n4\n260\n\n\n5\n360\n\n\n6\n470\n\n\n7\n560\n\n\n8\n630\n\n\n9\n770\n\n\n10\n990\n\n\n11\n1100\n\n\n\n\n\nMake a ggplot object with a geom_smooth()\n\np &lt;- ggplot(df, aes(month, n))+geom_smooth(method = \"lm\") +geom_point()\np\n\n\n\n\n\n\n\n\nUse ggplot_build to find the code behind the curve - the first element of data in the result is the geom_smooth curve\n\npp&lt;- ggplot_build(p)$data[[1]] %&gt;% \n  rename (\n    month = x,\n    n = y\n  )\nkable(pp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nn\nymin\nymax\nse\nflipped_aes\nPANEL\ngroup\ncolour\nfill\nlinewidth\nlinetype\nweight\nalpha\n\n\n\n\n1.000000\n5.00000\n-80.699424\n90.69942\n37.88394\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.126582\n17.71577\n-66.439648\n101.87118\n37.20140\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.253165\n30.43153\n-52.191714\n113.05477\n36.52410\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.379747\n43.14730\n-37.956292\n124.25088\n35.85232\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.506329\n55.86306\n-23.734100\n135.46022\n35.18640\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.632911\n68.57883\n-9.525902\n146.68355\n34.52666\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.759494\n81.29459\n4.667484\n157.92170\n33.87347\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n1.886076\n94.01036\n18.845183\n169.17553\n33.22721\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.012658\n106.72612\n33.006263\n180.44598\n32.58830\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.139241\n119.44189\n47.149728\n191.73405\n31.95718\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.265823\n132.15765\n61.274512\n203.04079\n31.33431\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.392405\n144.87342\n75.379479\n214.36736\n30.72021\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.518987\n157.58918\n89.463417\n225.71495\n30.11540\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.645570\n170.30495\n103.525033\n237.08486\n29.52046\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.772152\n183.02071\n117.562952\n248.47848\n28.93599\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n2.898734\n195.73648\n131.575706\n259.89725\n28.36265\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.025317\n208.45224\n145.561741\n271.34275\n27.80112\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.151899\n221.16801\n159.519403\n282.81662\n27.25213\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.278481\n233.88377\n173.446945\n294.32060\n26.71646\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.405063\n246.59954\n187.342517\n305.85656\n26.19492\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.531646\n259.31530\n201.204173\n317.42644\n25.68837\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.658228\n272.03107\n215.029868\n329.03227\n25.19772\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.784810\n284.74684\n228.817460\n340.67621\n24.72391\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n3.911392\n297.46260\n242.564717\n352.36048\n24.26794\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.037975\n310.17837\n256.269325\n364.08741\n23.83081\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.164557\n322.89413\n269.928894\n375.85937\n23.41360\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.291139\n335.60990\n283.540975\n387.67882\n23.01738\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.417721\n348.32566\n297.103075\n399.54825\n22.64325\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.544304\n361.04143\n310.612678\n411.47018\n22.29233\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.670886\n373.75719\n324.067267\n423.44712\n21.96573\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.797468\n386.47296\n337.464355\n435.48156\n21.66454\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n4.924051\n399.18872\n350.801513\n447.57593\n21.38985\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.050633\n411.90449\n364.076404\n459.73257\n21.14269\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.177215\n424.62025\n377.286822\n471.95368\n20.92402\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.303797\n437.33602\n390.430726\n484.24131\n20.73476\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.430380\n450.05178\n403.506283\n496.59728\n20.57571\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.556962\n462.76755\n416.511896\n509.02320\n20.44759\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.683544\n475.48331\n429.446245\n521.52038\n20.35096\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.810127\n488.19908\n442.308311\n534.08985\n20.28629\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n5.936709\n500.91484\n455.097402\n546.73229\n20.25387\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.063291\n513.63061\n467.813167\n559.44805\n20.25387\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.189873\n526.34638\n480.455607\n572.23714\n20.28629\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.316456\n539.06214\n493.025071\n585.09921\n20.35096\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.443038\n551.77791\n505.522253\n598.03356\n20.44759\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.569620\n564.49367\n517.948170\n611.03917\n20.57571\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.696203\n577.20944\n530.304144\n624.11473\n20.73476\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.822785\n589.92520\n542.591770\n637.25863\n20.92402\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n6.949367\n602.64097\n554.812883\n650.46905\n21.14269\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.075949\n615.35673\n566.969522\n663.74394\n21.38985\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.202532\n628.07250\n579.063895\n677.08110\n21.66454\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.329114\n640.78826\n591.098338\n690.47819\n21.96573\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.455696\n653.50403\n603.075278\n703.93278\n22.29233\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.582279\n666.21979\n614.997206\n717.44238\n22.64325\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.708861\n678.93556\n626.866636\n731.00448\n23.01738\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.835443\n691.65132\n638.686086\n744.61656\n23.41360\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n7.962025\n704.36709\n650.458047\n758.27613\n23.83081\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.088608\n717.08285\n662.184970\n771.98074\n24.26794\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.215190\n729.79862\n673.869244\n785.72799\n24.72391\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.341772\n742.51438\n685.513182\n799.51559\n25.19772\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.468354\n755.23015\n697.119018\n813.34128\n25.68837\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.594937\n767.94591\n708.688892\n827.20294\n26.19492\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.721519\n780.66168\n720.224851\n841.09851\n26.71646\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.848101\n793.37745\n731.728840\n855.02605\n27.25213\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n8.974683\n806.09321\n743.202708\n868.98371\n27.80112\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.101266\n818.80898\n754.648204\n882.96975\n28.36265\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.227848\n831.52474\n766.066979\n896.98250\n28.93599\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.354430\n844.24051\n777.460591\n911.02042\n29.52046\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.481013\n856.95627\n788.830505\n925.08204\n30.11540\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.607595\n869.67204\n800.178098\n939.16598\n30.72021\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.734177\n882.38780\n811.504661\n953.27094\n31.33431\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.860760\n895.10357\n822.811408\n967.39573\n31.95718\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n9.987342\n907.81933\n834.099474\n981.53919\n32.58830\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.113924\n920.53510\n845.369924\n995.70027\n33.22721\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.240506\n933.25086\n856.623755\n1009.87797\n33.87347\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.367089\n945.96663\n867.861900\n1024.07136\n34.52666\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.493671\n958.68239\n879.085233\n1038.27955\n35.18640\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.620253\n971.39816\n890.294571\n1052.50175\n35.85232\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.746835\n984.11392\n901.490680\n1066.73717\n36.52410\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n10.873418\n996.82969\n912.674276\n1080.98510\n37.20140\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n11.000000\n1009.54545\n923.846030\n1095.24488\n37.88394\nFALSE\n1\n-1\n#3366FF\ngrey60\n1\n1\n1\n0.4\n\n\n\n\n\nPlot the data from what we just extracted, to prove the principle.\n\nggplot() + \n  geom_smooth(aes(month, n), df,color=\"blue\",lwd=2)+\n  geom_line(aes(month, n), pp,color=\"red\",lwd=1)\n\n\n\n\n\n\n\n\nTo find a start of month amount, find the floor of each integer on x axis\n\npp$month&lt;-floor(pp$month)\n\nRemove replicates and select the values of x and y\n\npp &lt;- pp %&gt;% mutate(dup = duplicated(month)) %&gt;% \n             filter(dup==FALSE) %&gt;% \n             select(month,n)\n\ndf&lt;-bind_cols(df,pp[,2])\nkable(df)\n\n\n\n\nmonth\nn\n…3\n\n\n\n\n1\n100\n5.0000\n\n\n2\n140\n106.7261\n\n\n3\n200\n208.4522\n\n\n4\n260\n310.1784\n\n\n5\n360\n411.9045\n\n\n6\n470\n513.6306\n\n\n7\n560\n615.3567\n\n\n8\n630\n717.0829\n\n\n9\n770\n818.8090\n\n\n10\n990\n920.5351\n\n\n11\n1100\n1009.5455",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Extract Model from geom_smooth model</span>"
    ]
  },
  {
    "objectID": "filename_changer.html",
    "href": "filename_changer.html",
    "title": "3  Rename files procedurally",
    "section": "",
    "text": "3.1 Libraries\nlibrary(tidyverse)",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rename files procedurally</span>"
    ]
  },
  {
    "objectID": "filename_changer.html#procedure",
    "href": "filename_changer.html#procedure",
    "title": "3  Rename files procedurally",
    "section": "3.2 Procedure",
    "text": "3.2 Procedure\n\n3.2.1 Get a list of the original file names\n\nxx&lt;-list.files(\"data/filenames/\",full.names = T,recursive = T)\nxx\n\n[1] \"data/filenames//01_C.csv\" \"data/filenames//02_D.csv\"\n[3] \"data/filenames//03_A.csv\" \"data/filenames//04_B.csv\"\n\n\n\n\n3.2.2 \nManipulate the strings\n\nxxx&lt;-case_when(\n               str_detect (xx,\"_A\") ~ str_replace(xx, \"_A\", \"_D\"),\n               str_detect (xx,\"_B\") ~ str_replace(xx, \"_B\", \"_A\"),\n               str_detect (xx,\"_C\") ~ str_replace(xx, \"_C\", \"_B\"),\n               str_detect (xx,\"_D\") ~ str_replace(xx, \"_D\", \"_C\")\n                )\nxxx\n\n[1] \"data/filenames//01_B.csv\" \"data/filenames//02_C.csv\"\n[3] \"data/filenames//03_D.csv\" \"data/filenames//04_A.csv\"\n\n\n\n\n3.2.3 \nLoop across all names, replacing the filename where needed\n\nfor(i in 1:length(xx)){\n  print(i)\n  print(str_c(xx[i],\" \",xxx[i]))\n\n  if(xx[i]!=xxx[i]){\n  system(\n        str_c(\"mv \",xx[i],\" \",xxx[i])\n      )\n  }\n  }\n\n[1] 1\n[1] \"data/filenames//01_C.csv data/filenames//01_B.csv\"\n[1] 2\n[1] \"data/filenames//02_D.csv data/filenames//02_C.csv\"\n[1] 3\n[1] \"data/filenames//03_A.csv data/filenames//03_D.csv\"\n[1] 4\n[1] \"data/filenames//04_B.csv data/filenames//04_A.csv\"\n\n\n\n\n3.2.4",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Rename files procedurally</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html",
    "href": "Global_Political_Map.html",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "",
    "text": "4.1 Background\nThe sf package provides a seamless integration between R, ggplot, shapefiles, geoJSON and other GIS formats. A bonus here is that sf objects behave like dataframes so you can bind your own data on to an sf object and use your bound data to control things like putting chloropleths on the map.\nThis shamelessly copies a tutorial found here.\nThe rnaturalearth project has a great set of shape data for countries of the globe.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#libraries",
    "href": "Global_Political_Map.html#libraries",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.2 Libraries",
    "text": "4.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(knitr)\nlibrary(ggspatial)\nlibrary(\"rnaturalearth\")\nlibrary(\"rnaturalearthdata\")\n\n\nAttaching package: 'rnaturalearthdata'\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#data",
    "href": "Global_Political_Map.html#data",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.3 Data",
    "text": "4.3 Data\n\n#call the polygons\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#draw-basic-polygons",
    "href": "Global_Political_Map.html#draw-basic-polygons",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.4 Draw basic polygons",
    "text": "4.4 Draw basic polygons\n\nggplot(data = world) +\n    geom_sf()",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#colour-the-map-with-continuous-variable",
    "href": "Global_Political_Map.html#colour-the-map-with-continuous-variable",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.5 Colour the map with continuous variable",
    "text": "4.5 Colour the map with continuous variable\n\nggplot(data = world) +\n    geom_sf(aes(fill = pop_est)) +\n    scale_fill_viridis_c(option = \"plasma\", trans = \"sqrt\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#colour-the-map-with-factor-variable",
    "href": "Global_Political_Map.html#colour-the-map-with-factor-variable",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.6 Colour the map with factor variable",
    "text": "4.6 Colour the map with factor variable\n\nggplot(data = world) +\n    geom_sf(aes(fill = continent))",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#change-the-projection-with-coord_sf",
    "href": "Global_Political_Map.html#change-the-projection-with-coord_sf",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.7 Change the projection with coord_sf",
    "text": "4.7 Change the projection with coord_sf\n\nggplot(data = world) +\n    geom_sf(aes(fill = continent))+\n    coord_sf(crs = st_crs(3035))",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#focus-on-a-specific-region",
    "href": "Global_Political_Map.html#focus-on-a-specific-region",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.8 Focus on a specific region",
    "text": "4.8 Focus on a specific region\n\n    ggplot(data = world) +\n    geom_sf(aes(fill = continent))+\n      coord_sf(xlim = c(-20.15, 34.12), ylim = c(40, 60), expand = FALSE)",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#decorate-with-scale-bar-compass",
    "href": "Global_Political_Map.html#decorate-with-scale-bar-compass",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.9 Decorate with scale bar / compass",
    "text": "4.9 Decorate with scale bar / compass\n\n     ggplot(data = world) +\n              geom_sf(aes(fill = continent))+\n              coord_sf(xlim = c(-20.15, 34.12), ylim = c(40, 60), expand = FALSE) +\n              annotation_scale(location = \"br\", width_hint = 0.4) +\n              annotation_north_arrow(\n                    location = \"bl\", which_north = \"true\", \n                    pad_x = unit(0.1, \"cm\"), pad_y = unit(0.3, \"cm\"),\n                    style = north_arrow_fancy_orienteering) \n\nScale on map varies by more than 10%, scale bar may be inaccurate",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#identify-centroids-of-each-polygon",
    "href": "Global_Political_Map.html#identify-centroids-of-each-polygon",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.10 Identify centroids of each polygon",
    "text": "4.10 Identify centroids of each polygon\nThis will be used to place country name labels\n\nworld = st_make_valid(world)\nworld_points&lt;- st_centroid(world)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nworld_points &lt;- cbind(world, st_coordinates(st_centroid(world$geometry)))",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#add-labels",
    "href": "Global_Political_Map.html#add-labels",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.11 Add labels",
    "text": "4.11 Add labels\n\nggplot(data = world) +\n              geom_sf(aes(fill = continent))+\n              coord_sf(xlim = c(-20.15, 34.12), ylim = c(40, 60), expand = FALSE) +\n              annotation_scale(location = \"br\", width_hint = 0.4) +\n              annotation_north_arrow(\n                    location = \"bl\", which_north = \"true\", \n                    pad_x = unit(0.1, \"cm\"), pad_y = unit(0.3, \"cm\"),\n                    style = north_arrow_fancy_orienteering) +\n              geom_text(data= world_points,aes(x=X, y=Y, label=name),\n                              color = \"darkblue\", \n                              fontface = \"bold\", \n                              check_overlap = FALSE,\n                              size=1.5\n                        )\n\nScale on map varies by more than 10%, scale bar may be inaccurate",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#add-other-pretty-things",
    "href": "Global_Political_Map.html#add-other-pretty-things",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.12 Add other pretty things",
    "text": "4.12 Add other pretty things\n\nggplot(data = world) +\n              geom_sf(aes(fill = continent))+\n              coord_sf(xlim = c(-20.15, 34.12), ylim = c(40, 60), expand = FALSE) +\n              annotation_scale(location = \"br\", width_hint = 0.4) +\n              annotation_north_arrow(\n                    location = \"bl\", which_north = \"true\", \n                    pad_x = unit(0.1, \"cm\"), pad_y = unit(0.3, \"cm\"),\n                    style = north_arrow_fancy_orienteering) +\n              geom_text(data= world_points,aes(x=X, y=Y, label=name),\n                              color = \"darkblue\", \n                              fontface = \"bold\", \n                              check_overlap = FALSE,\n                              size=1.5\n                        )+\n              theme(panel.grid.major = element_line(color = gray(.5), \n                                                    linetype = \"dashed\", \n                                                    size = 0.5\n                                         \n                                                    )\n                  )\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\nScale on map varies by more than 10%, scale bar may be inaccurate\n\n\n\n\n\n\n\n\n\n\n4.12.1 Save as PDF\n\nggsave(filename = \"output/political_map.pdf\",dpi = 300)\n\nSaving 7 x 5 in image\nScale on map varies by more than 10%, scale bar may be inaccurate",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#test-on-new-data-from-some-shape-files",
    "href": "Global_Political_Map.html#test-on-new-data-from-some-shape-files",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.13 Test on new data from some shape files",
    "text": "4.13 Test on new data from some shape files\nUsing shape files can be quite slow, so here we will only use a few polygons\n\ndownload.file(\"https://www.abs.gov.au/ausstats/subscriber.nsf/log?openagent&1270055004_sua_2016_aust_shape.zip&1270.0.55.004&Data%20Cubes&1E24D1FB300696D2CA2581B1000E15A5&0&July%202016&09.10.2017&Latest\",destfile = \"data/SUA_2016_AUST.zip\")\n\nunzip(zipfile = \"data/SUA_2016_AUST.zip\",exdir = \"data/\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#open-data",
    "href": "Global_Political_Map.html#open-data",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.14 Open Data",
    "text": "4.14 Open Data\n\nmap = read_sf(\"data/SUA_2016_AUST.shp\") \nmap = map [1:20,]",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#plot-map",
    "href": "Global_Political_Map.html#plot-map",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.15 Plot map",
    "text": "4.15 Plot map\n\nggplot(data = map) +\n              geom_sf()+\n              theme(panel.grid.major = element_line(color = gray(.5), \n                                                    linetype = \"dashed\", \n                                                    size = 0.5\n                                                    ),  \n                    legend.position=\"none\"\n\n                  )",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "Global_Political_Map.html#remove-data",
    "href": "Global_Political_Map.html#remove-data",
    "title": "4  Draw Political Maps (sf geom_sf)",
    "section": "4.16 Remove data",
    "text": "4.16 Remove data\n\nsystem(\"rm data/SUA*\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Draw Political Maps (sf geom_sf)</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html",
    "href": "photo_cleaning.html",
    "title": "5  Photo Cleaning For Sharing",
    "section": "",
    "text": "5.1",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#background",
    "href": "photo_cleaning.html#background",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.2 Background",
    "text": "5.2 Background\nWhen sharing images, it is useful to be able to…\n\n5.2.0.1 Remove EXIF data, which can include GPS locations where the photos were taken\nImagemagick is used for this\n\n\n5.2.0.2 Rename files to have nonsensical names\nUsing either date or gdate on linux or OSX respectively. gdate is part of the coreutils package\nThis tutorial is aimed at OSX users and assumes you have homebrew (brew) installed. Using it on a PC through the linux subsystem for windows should be easy. Replace brew with apt-get or similar.\n\n\n5.2.0.3 Flag any files with very high luminance, for instance photos of labels, consent forms etc\nThis is also done with Imagemagick.\nThis script assumes that you’ve put a lot of pictures in the folder data/photos_for_cleaning",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#libraries",
    "href": "photo_cleaning.html#libraries",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.3 Libraries",
    "text": "5.3 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mixtools)\n\nmixtools package, version 2.0.0, Released 2022-12-04\nThis package is based upon work supported by the National Science Foundation under Grant No. SES-0518772 and the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\nset.seed(23423346)",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#copy-some-photos-in-to-the-working-directory",
    "href": "photo_cleaning.html#copy-some-photos-in-to-the-working-directory",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.4 Copy some photos in to the working directory",
    "text": "5.4 Copy some photos in to the working directory\n\nsystem(\"rm data/photos_for_cleaning/*.JPG\")\nsystem(\"unzip data/photos_for_cleaning/photos_for_cleaning.zip -d data/photos_for_cleaning/\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#remove-exif-data",
    "href": "photo_cleaning.html#remove-exif-data",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.5 Remove EXIF data",
    "text": "5.5 Remove EXIF data\nInstall Imagemagick (hash this line if you already have Imagemagick unless you want a long wait)\n\n#system(\"brew install imagemagick\")\n\nUse imagemagick’s mogrify to make an in place copy of each file with the EXIF data stripped out. This has the benefit of making a new creation datestamp and md5 hash for each file.\nBefore running this on thousands of photos, we will test that this works by reading an EXIF metadata from a single file.\nLet’s see what files are available in the folder data/photos_for_cleaning\n\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nNow use the identify command to look the content of the EXIF for one file\n\nsystem(str_c(r\"(identify -format '%[EXIF:*]' )\", pictures[1]))\n\nYou’ll see a load of data from the EXIF. If you want to capture anything, like the aperture settings etc, now is the time to do it. It should be simple to design a loop and function that will capture this info.\nAfter applying the following command from ImageMagick\n\nsystem(str_c(\"mogrify -strip \",pictures[1]))\n\nYou should be able to rerun this\n\nsystem(str_c(r\"(identify -format '%[EXIF:*]')\", pictures[1]))\n\nand you should now see an empty EXIF, i.e. nothing will be shown in the console. If you still see EXIF data, something went wrong.\nTo apply the function to all files in the folder, we won’t use the built in mogrify -strip data/photos_for_cleaning/*.JPG because it halts when it hits a problem. Instead we will wrap it in an R code with ‘try’.\n\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nfor(i in pictures){\n                  (system(str_c(\"mogrify -strip \",i)))\n                  }",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#rename-all-files",
    "href": "photo_cleaning.html#rename-all-files",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.6 Rename all files",
    "text": "5.6 Rename all files\n\n5.6.1 Define a function to create a unique ID\n\n  randomid &lt;- function(n = 1,path,file) {\n    a &lt;- do.call(paste0, replicate(5, sample(LETTERS, n, TRUE), FALSE))\n    id = paste0(a, sprintf(\"%04d\", sample(9999, n, TRUE)), sample(LETTERS, n, TRUE))\n    file.rename(from=file, to=str_c(path,id,\".JPG\",sep=\"\"))\n  }\n\n\n\n5.6.2 Apply the function, renaming all photos\n\npath = \"data/photos_for_cleaning/\"\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nfor(i in pictures){randomid(path = path,file = i)}\n\n\n\n5.6.3 Flag any files with high luminance\n\ndf = tibble(\n  pictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T),\n  luminance = NA\n)\n\nfor (i in 1:nrow(df)){\n\ndf$luminance[i] = as.numeric(system(str_c(r\"(convert )\",df$pictures[i],r\"( -colorspace LAB -channel r -separate +channel -format \"%[mean]\\n\" info: )\",sep=\"\"),intern = T))\n\n}\n\n\n\n5.6.4 Run an Expectation Maximisation to find clusters\n\nset.seed(12341)\nem&lt;-normalmixEM(df$luminance, k=2,maxit = 3000)\n\nnumber of iterations= 6 \n\n\nPlot the results, with 99.99 Confidence interval\n\nggplot(df, aes(x = luminance)) +\n  geom_histogram(binwidth = 0.007,color=\"white\",fill=\"grey\") +\n  mapply(\n    function(mean, sd, lambda, n, binwidth) {\n      stat_function(\n        fun = function(x) {\n          (dnorm(x, mean = mean, sd = sd)) * n * binwidth * lambda\n        }\n      )\n    },\n    mean = em[[\"mu\"]], #mean\n    sd = em[[\"sigma\"]], #standard deviation\n    lambda = em[[\"lambda\"]], #amplitude\n    n = length(df$luminance), #sample size\n    binwidth = 0.007 #binwidth used for histogram\n  )+\n  geom_vline(xintercept = em$mu[1]+(3.29*em$sigma[1]),lty=2,lwd=1,col=\"red\")\n\n\n\n\n\n\n\n\n\n\n5.6.5 Rename files with high luminance\n\nfor (i in 1:nrow(df)){\n\nif (df$luminance[i] &gt; 40000){file.rename(from = df$pictures[i],to = gsub(x = df$pictures[i],pattern = \".JPG\",replacement = \".light.JPG\"))}\n  \nmessage(str_c(df$pictures[i],\" luminance = \",df$luminance[i]))\n}\n\ndata/photos_for_cleaning//ERCVC3682S.JPG luminance = 21065.1\n\n\ndata/photos_for_cleaning//NPHTK8426K.JPG luminance = 20564.3\n\n\ndata/photos_for_cleaning//OYMPF5938B.JPG luminance = 10660.8\n\n\ndata/photos_for_cleaning//TGLWU3648Q.JPG luminance = 50894.2\n\n\ndata/photos_for_cleaning//VATLY3953X.JPG luminance = 52021\n\n\ndata/photos_for_cleaning//VRAXW6073Z.JPG luminance = 55293.9\n\n\ndata/photos_for_cleaning//VTEEG4048F.JPG luminance = 16634.8\n\n\ndata/photos_for_cleaning//WNQZB2575C.JPG luminance = 17054.4\n\n\ndata/photos_for_cleaning//YPZJU4391B.JPG luminance = 53881.3\n\n\ndata/photos_for_cleaning//YQFYO8330G.JPG luminance = 18454.5\n\n\ndata/photos_for_cleaning//ZIIYN8173Q.JPG luminance = 14055.4",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "photo_cleaning.html#results",
    "href": "photo_cleaning.html#results",
    "title": "5  Photo Cleaning For Sharing",
    "section": "5.7 Results",
    "text": "5.7 Results\nA file flagged as having high luminance\n\nknitr::include_graphics(gsub(x=df$pictures[which(df$luminance==max(df$luminance))],pattern = \".JPG\",replacement = \".light.JPG\"),error = T)\n\n\n\n\n\n\n\n\nA file flagged as having low luminance is displayed here\n\nknitr::include_graphics(df$pictures[which(df$luminance==min\n                                          (df$luminance))])",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Photo Cleaning For Sharing</span>"
    ]
  },
  {
    "objectID": "remove_accents.html",
    "href": "remove_accents.html",
    "title": "6  Remove_Accents",
    "section": "",
    "text": "6.1 Background\nAccents in text can cause all manner of headaches in R.\nI’ve seen a lot of different ways to remove accents, none of which seemed to work for me.\nThis brute-force approach is not sophisticated but is highly effective.\nIt works by simply doing a find and replace with gsub, but providing an exhaustive list of special characters and a second list of their non-accented counterparts.\nDoes the job very effectively. You can modify this by adding other special characters, remembering that the vector position of a character in list a needs to map to the vector position of its counterpart in list b",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Remove_Accents</span>"
    ]
  },
  {
    "objectID": "remove_accents.html#define-function",
    "href": "remove_accents.html#define-function",
    "title": "6  Remove_Accents",
    "section": "6.2 Define Function",
    "text": "6.2 Define Function\n\nremoveAccents&lt;-function(x)\n{\na &lt;- c('À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ń', 'Ņ', 'ņ', 'Ň', 'ň', 'ŉ', 'Ō', 'ō', 'Ŏ', 'ŏ', 'Ő', 'ő', 'Œ', 'œ', 'Ŕ', 'ŕ', 'Ŗ', 'ŗ', 'Ř', 'ř', 'Ś', 'ś', 'Ŝ', 'ŝ', 'Ş', 'ş', 'Š', 'š', 'Ţ', 'ţ', 'Ť', 'ť', 'Ŧ', 'ŧ', 'Ũ', 'ũ', 'Ū', 'ū', 'Ŭ', 'ŭ', 'Ů', 'ů', 'Ű', 'ű', 'Ų', 'ų', 'Ŵ', 'ŵ', 'Ŷ', 'ŷ', 'Ÿ', 'Ź', 'ź', 'Ż', 'ż', 'Ž', 'ž', 'ſ', 'ƒ', 'Ơ', 'ơ', 'Ư', 'ư', 'Ǎ', 'ǎ', 'Ǐ', 'ǐ', 'Ǒ', 'ǒ', 'Ǔ', 'ǔ', 'Ǖ', 'ǖ', 'Ǘ', 'ǘ', 'Ǚ', 'ǚ', 'Ǜ', 'ǜ', 'Ǻ', 'ǻ', 'Ǽ', 'ǽ', 'Ǿ', 'ǿ');\nb &lt;- c('A', 'A', 'A', 'A', 'A', 'A', 'AE', 'C', 'E', 'E', 'E', 'E', 'I', 'I', 'I', 'I', 'D', 'N', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'U', 'U', 'U', 'Y', 's', 'a', 'a', 'a', 'a', 'a', 'a', 'ae', 'c', 'e', 'e', 'e', 'e', 'i', 'i', 'i', 'i', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'u', 'u', 'u', 'u', 'y', 'y', 'A', 'a', 'A', 'a', 'A', 'a', 'C', 'c', 'C', 'c', 'C', 'c', 'C', 'c', 'D', 'd', 'D', 'd', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'G', 'g', 'G', 'g', 'G', 'g', 'G', 'g', 'H', 'h', 'H', 'h', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'IJ', 'ij', 'J', 'j', 'K', 'k', 'L', 'l', 'L', 'l', 'L', 'l', 'L', 'l', 'l', 'l', 'N', 'n', 'N', 'n', 'N', 'n', 'n', 'O', 'o', 'O', 'o', 'O', 'o', 'OE', 'oe', 'R', 'r', 'R', 'r', 'R', 'r', 'S', 's', 'S', 's', 'S', 's', 'S', 's', 'T', 't', 'T', 't', 'T', 't', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'W', 'w', 'Y', 'y', 'Y', 'Z', 'z', 'Z', 'z', 'Z', 'z', 's', 'f', 'O', 'o', 'U', 'u', 'A', 'a', 'I', 'i', 'O', 'o', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'A', 'a', 'AE', 'ae', 'O', 'o');\nfor(i in 1:length(a))\n{\nx&lt;-gsub(x = x,pattern = a[i],replacement = b[i])\n}\nreturn(x)\n}",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Remove_Accents</span>"
    ]
  },
  {
    "objectID": "remove_accents.html#test-function",
    "href": "remove_accents.html#test-function",
    "title": "6  Remove_Accents",
    "section": "6.3 Test function",
    "text": "6.3 Test function\n\nremoveAccents(c(\"Féàltê\",\"Ýüćčæ\"))\n\n[1] \"Fealte\" \"Yuccae\"",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Remove_Accents</span>"
    ]
  },
  {
    "objectID": "text_to_speech_R.html",
    "href": "text_to_speech_R.html",
    "title": "7  Text to Speech in R",
    "section": "",
    "text": "7.1 Background\nThis is a simple bit of script that reaches in to the linux/OS-X system and runs a text to speech function.\nIt may have some practical benefits. I quite like to use it to tell me that a long running script has finished.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Text to Speech in R</span>"
    ]
  },
  {
    "objectID": "text_to_speech_R.html#define-functions",
    "href": "text_to_speech_R.html#define-functions",
    "title": "7  Text to Speech in R",
    "section": "7.2 Define Functions",
    "text": "7.2 Define Functions\n\nspeakr &lt;- function(message=\"hello, world\")\n{\n  if(.Platform$OS.type == \"unix\"){system(paste(\"say \",message,sep=\"\"))}\n    else {system(paste(\"echo \",message,\"|ptts\",sep=\"\"))}\n}",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Text to Speech in R</span>"
    ]
  },
  {
    "objectID": "text_to_speech_R.html#example",
    "href": "text_to_speech_R.html#example",
    "title": "7  Text to Speech in R",
    "section": "7.3 Example",
    "text": "7.3 Example\n\n# usage examples\n\nspeakr()\nspeakr(\"Who lives in a pineapple under the sea?\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Text to Speech in R</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html",
    "href": "UK_Postcodes.html",
    "title": "8  UK Postcode Maps",
    "section": "",
    "text": "8.1 Background\nThis is a basic tutorial on how to load shapefiles and geopoints to a leaflet map.\nThere is a lot more detailed information here [https://rstudio.github.io/leaflet/](https://rstudio.github.io/leaflet/)\nThe example is hopefully useful as it uses UK postcode and district shapefiles, which are commonly useful for a variety of purposes.\nUK Postcode data can be found here\nhttps://www.freemaptools.com/map-tools.htm\nUK district shapefiles are available here\nhttps://www.ordnancesurvey.co.uk/opendatadownload/products.html",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#libraries",
    "href": "UK_Postcodes.html#libraries",
    "title": "8  UK Postcode Maps",
    "section": "8.2 Libraries",
    "text": "8.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(knitr)\nlibrary(leaflet)",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#data",
    "href": "UK_Postcodes.html#data",
    "title": "8  UK Postcode Maps",
    "section": "8.3 Data",
    "text": "8.3 Data\n\n8.3.1 Create a folder to house data\n\nif(!dir.exists(\"data/ukpostcodes\"))(dir.create(\"data/ukpostcodes\"))\n\n[1] TRUE\n\nsystem(\"rm -rf data/ukpostcodes/*\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#download-a-list-of-uk-postcodes-and-gps-locations",
    "href": "UK_Postcodes.html#download-a-list-of-uk-postcodes-and-gps-locations",
    "title": "8  UK Postcode Maps",
    "section": "8.4 Download a list of UK postcodes and gps locations",
    "text": "8.4 Download a list of UK postcodes and gps locations\n\ndownload.file(url = \"https://data.freemaptools.com/download/full-uk-postcodes/ukpostcodes.zip\",destfile =paste(\"data/ukpostcodes/ukpostcodes.zip\"))\n\nsystem (\"unzip data/ukpostcodes/ukpostcodes.zip -d data/ukpostcodes/\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#download-a-list-of-uk-district-shapefiles",
    "href": "UK_Postcodes.html#download-a-list-of-uk-district-shapefiles",
    "title": "8  UK Postcode Maps",
    "section": "8.5 Download a list of UK district shapefiles",
    "text": "8.5 Download a list of UK district shapefiles\n\n# set a timeout proportional to the size of the dataset and the speed of the connection\noptions(timeout=300)\n\ndownload.file(url = \"https://api.os.uk/downloads/v1/products/BoundaryLine/downloads?area=GB&format=ESRI%C2%AE+Shapefile&redirect\",destfile =paste(\"data/ukpostcodes/bdline_essh_gb.zip\"))\n\nsystem (\"unzip data/ukpostcodes/bdline_essh_gb.zip -d data/ukpostcodes/\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#read-postcode-data",
    "href": "UK_Postcodes.html#read-postcode-data",
    "title": "8  UK Postcode Maps",
    "section": "8.6 Read Postcode data",
    "text": "8.6 Read Postcode data\n\nukpostcodes &lt;- read_csv(\"data/ukpostcodes/ukpostcodes.csv\")\n\nRows: 1801375 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): postcode\ndbl (3): id, latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#make-a-small-sample-of-all-postcodes",
    "href": "UK_Postcodes.html#make-a-small-sample-of-all-postcodes",
    "title": "8  UK Postcode Maps",
    "section": "8.7 Make a small sample of all postcodes",
    "text": "8.7 Make a small sample of all postcodes\n\nsample&lt;-ukpostcodes[sample(1:nrow(ukpostcodes),size = 100),] %&gt;% \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude)\n  )\nhead(sample)\n\n# A tibble: 6 × 4\n       id postcode latitude longitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1  208216 TD2 6TJ      55.7    -2.75 \n2 1187209 EX36 3QN     51.0    -3.77 \n3 2611118 JE3 2FD      NA      NA    \n4 1692549 AB15 7SB     57.1    -2.15 \n5 1690047 B17 8DS      52.5    -1.96 \n6  566796 PE38 9LZ     52.6     0.408",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "UK_Postcodes.html#read-in-shapefiles-to-a-map",
    "href": "UK_Postcodes.html#read-in-shapefiles-to-a-map",
    "title": "8  UK Postcode Maps",
    "section": "8.8 Read in shapefiles to a map",
    "text": "8.8 Read in shapefiles to a map\n\nmap = read_sf(\"data/ukpostcodes/Data/GB/county_region.dbf\")\n\n\nmapproj &lt;- st_transform(map, st_crs(\"+proj=longlat +init=epsg:4326 +ellps=WGS84 +datum=WGS84 +no_defs\"))\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\nleaflet(mapproj) %&gt;% \n    addTiles() %&gt;% \n  addMeasure(primaryLengthUnit=\"kilometers\", secondaryLengthUnit=\"meters\")  %&gt;%\n  addScaleBar(options = c(imperial = FALSE)) %&gt;%  \n  addPolygons(color = \"green\",label = mapproj$NAME) %&gt;% \n  addCircleMarkers(lng = sample$longitude,lat = sample$latitude,label = sample$postcode,radius = 0.3)\n\n\n\n\n\n\n8.8.1 remove data\n\nsystem(\"rm -rf data/ukpostcodes/\")",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>UK Postcode Maps</span>"
    ]
  },
  {
    "objectID": "update_R.html",
    "href": "update_R.html",
    "title": "9  Update R - Bare Bones",
    "section": "",
    "text": "9.1 Bare bones updater for R.\nAn annoying feature of R is that updating the R version doesn’t always (if ever) pull all of your installed packages in to the new version.\nUpdater packages for R are also a bit hit and miss.\nThe updateR package is quite popular, but I haven’t had much luck with it.\nIt is here if you want to try it…\nMy approach takes the long way round, by creating an installer script for all the currently installed packages. It then saves this script, which you can run once you’ve updated R.\nDoing it this way accounts for weird behaviour where installing packages from within loops or apply commands doesn’t handle dependencies well and leads to lots of packages failing to install.\nThis script contains some code from the updateR package, which has never worked for me on its own.\nThis method can take a while, but in my opinion does a much better job of comprehensively rebuilding your catalogue of libraries\nObviously this only works with CRAN packages, but will spit out a list of packages you need to manually install at the end.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Update R - Bare Bones</span>"
    ]
  },
  {
    "objectID": "update_R.html#bare-bones-updater-for-r.",
    "href": "update_R.html#bare-bones-updater-for-r.",
    "title": "9  Update R - Bare Bones",
    "section": "",
    "text": "devtools::install_github(“AndreaCirilloAC/updateR”)",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Update R - Bare Bones</span>"
    ]
  },
  {
    "objectID": "update_R.html#update-r",
    "href": "update_R.html#update-r",
    "title": "9  Update R - Bare Bones",
    "section": "9.2 Update R",
    "text": "9.2 Update R\n\n9.2.1 Libraries\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(xml2)\nlibrary(rvest)\nlibrary(askpass)\n\n\n\n9.2.2 Define functions\nMost of these are copies or modifications of the updateR functions.\n\n9.2.2.1 list_packages\n\nlist_packages &lt;- function() {\n  all_pkg &lt;- installed.packages() %&gt;%\n    as.data.frame() %&gt;%\n    pull(Package)\n  base_pkg &lt;- installed.packages() %&gt;%\n    as.data.frame() %&gt;%\n    filter(Priority == \"base\") %&gt;%\n    pull(Package)\n  all_pkg[!all_pkg %in% base_pkg]\n}\n\n\n\n9.2.2.2 list available versions\n\nlist_available_versions&lt;-function(){\n  cran &lt;- \"http://cran.rstudio.com/bin/macosx/\"\n  version_regex &lt;- \"(\\\\d+)?\\\\.(\\\\d+)?\\\\.(\\\\d+)?\"\n  page &lt;- read_html(cran)\n  file_url &lt;- page %&gt;%\n    html_nodes(xpath = \"//td/a\")\n  file_url\n}\n\n\n\n9.2.2.3 latest_r_version\nChange ver to match your system requirements. Default is my own need for macOS non arm. (ver = 3)\n\nlatest_r_version &lt;- function(ver=3) {\n  cran &lt;- \"http://cran.rstudio.com/bin/macosx/\"\n  version_regex &lt;- \"(\\\\d+)?\\\\.(\\\\d+)?\\\\.(\\\\d+)?\"\n  page &lt;- read_html(cran)\n  file_url &lt;- page %&gt;%\n    html_nodes(xpath = \"//td/a\") %&gt;% .[ver] %&gt;%\n    html_attr(\"href\") %&gt;%\n    paste(cran, ., sep = \"\")\n  minimal &lt;- page %&gt;%\n    html_nodes(xpath = '//table[1]//tr[1]//td[2]') %&gt;%\n    html_text() %&gt;%\n    trimws() %&gt;%\n    regmatches(., regexpr(\"macOS.*higher.\", .)) %&gt;%\n    regmatches(., regexpr(\"\\\\d+.\\\\d+\", .))\n  r_latest &lt;- regmatches(file_url, regexpr(version_regex, file_url))\n  r_current &lt;- paste(version$major, version$minor, sep = \".\")\n  r_latest_numeric &lt;- as.numeric(sub(\"^(\\\\d)\\\\.\", \"\\\\1\", r_latest))\n  r_current_numeric &lt;- as.numeric(sub(\"^(\\\\d)\\\\.\", \"\\\\1\", r_current))\n  \n  structure(\n    list(update_avail = ifelse(r_latest_numeric &gt; r_current_numeric, T, F),\n         latest = r_latest,\n         url = file_url),\n    current = paste(version$major, version$minor, sep = \".\"),\n    OS_minimal = as.numeric(minimal)\n  )\n}\n\n\n\n9.2.2.4 ask_password\n\nask_password &lt;- function() {\n  askpass::askpass(sprintf(\"Enter password for %s: \", system2(\"whoami\", stdout = TRUE)))\n}\n\n\n\n9.2.2.5 write_package_reinstaller\n\nwrite_package_reinstaller &lt;- function(){\n      # Make a list of currently installed packages\n      packcmds&lt;-NA\n      packages&lt;-list_packages()\n    \n      # Loop through and create an install script\n      for (i in 1:length(packages))\n      {\n      packcmds[i]&lt;-paste(\"install.packages('\",packages[i],\"',dependencies = T)\",sep = \"\")\n      }\n    \n    #write installer script to file\n    write.table(x = packcmds,file = \"packages.install.script.R\",quote = F,row.names = F,col.names=FALSE)\n \n}\n\n\n\n9.2.2.6 update_R\nThis is a modified version of the update_R function from updateR.\n\nupdate_R&lt;-function(force=FALSE,ver)\n{\n  check&lt;-latest_r_version(ver = ver)\n  \n  if(check$update_avail==FALSE){message(\"Current version is up to date, nothing to be done\")}\n  \n  if(check$update_avail==TRUE | force == TRUE)\n  {\n   \n    admin_password &lt;- ask_password()\n    username &lt;- system('whoami', intern = TRUE)\n    command &lt;- paste0(\"echo '\", admin_password, \"' | sudo -S -l\")\n    out &lt;- system(command, intern = TRUE)\n    if (length(out) == 0) {\n      stop(sprintf(\"current user %s does not have admin privileges\", username))\n    }\n    folderpath &lt;- sprintf(\"/Users/%s/Downloads/\",\n                          system2(\"whoami\", stdout = TRUE))\n    pkgfile &lt;- regmatches(check$url, regexpr(\"R.*$\", check$url))\n    fullpath &lt;- sprintf(\"%s%s\", folderpath, pkgfile)\n    # download package, set folder for download\n    message(\"Downloading new version of R\")\n    download.file(check$url, fullpath)\n    \n    message(\"Installinf new version of R\")\n    {\n      message(paste0(\"Installing R-\", check$latest, \"…please wait\"))\n      command &lt;-\n        paste0(\"echo '\",\n               admin_password,\n               \"' | sudo -S installer -pkg \",\n               \"'\",\n               fullpath,\n               \"'\",\n               \" -target /\")\n      system(command, ignore.stdout = TRUE)\n      arg &lt;- paste0(\"--check-signature \", fullpath)\n      system2(\"pkgutil\", arg)\n    }\n  }\n}\n\n\n\n9.2.2.7 reinstall_all_packages\n\nreinstall_all_packages &lt;- function(){\n\n    message(\"Reinstalling packages from source file\")\n    source(\"packages.install.script.R\")\n    \n    new.packages&lt;-list_packages()\n    needinstall&lt;-packages[packages%in%new.packages==FALSE]\n    needinstall&lt;-factor(unique(needinstall))\n    \n    message(needinstall)\n  }\n\n\n\n\n9.2.3 Run the Updater\n\n9.2.3.1 Save the List of packages\n\n#write_package_reinstaller()\n\n\n\n9.2.3.2 Find the appropriate version of R for your machine\n\n#list_available_versions()\n\n\n\n9.2.3.3 Update R\n\n#update_R(ver=3)\n\n\n\n9.2.3.4 Update R\n\n#reinstall_all_packages()",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Update R - Bare Bones</span>"
    ]
  },
  {
    "objectID": "utf8_encoding.html",
    "href": "utf8_encoding.html",
    "title": "10  Normalise utf8 accented text",
    "section": "",
    "text": "10.1 Background\nAccented text can cause problems for R.\nThis method shows how to convert all accents from mixed encodings to UTF-8 text .\nProblems like this often emerge with data shared between PC/Mac/Linux and with data exported from Excel. The simple solution is to use the Encoding() function from the utf8 package.",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Normalise utf8 accented text</span>"
    ]
  },
  {
    "objectID": "utf8_encoding.html#background",
    "href": "utf8_encoding.html#background",
    "title": "10  Normalise utf8 accented text",
    "section": "",
    "text": "10.1.1 Libraries\n\nlibrary(utf8)\n\n\n\n10.1.2 Dummy data\nHere we define a vector x, which has accents. To simulate the problem, we set encoding to be mixed between UTF-8 and bytes, but the second entry is actually encoded in Latin byte format with the leading byte 0xE7 rather than in UTF-8.\n\nx &lt;- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) &lt;- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\nIf we try to convert all entries in the vector to UTF-8, it fails\n\ntry(as_utf8(x))\n\nError in as_utf8(x) : \n  entry 2 has wrong Encoding; marked as \"UTF-8\" but leading byte 0xE7 followed by invalid continuation byte (0x69) at position 4\n\n\nThe simple fix is to change the encoding to match the real data. Here entry two is switched to the correct encoding and we are then able to re-encode it\n\nEncoding(x[2]) &lt;- \"latin1\"\nas_utf8(x)\n\n[1] \"façile\" \"façile\" \"façile\"",
    "crumbs": [
      "R | RStudio",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Normalise utf8 accented text</span>"
    ]
  },
  {
    "objectID": "Cox_PH.html",
    "href": "Cox_PH.html",
    "title": "11  Survival Analysis with Cox-PH",
    "section": "",
    "text": "11.1 Create dummy dataset\nYou would provide a real data set at this point. The data are basically a tibble/df in which you provide a list of times at which a case became either an event/failure or censored (lost-to-followup or end of study). The key variables are some kind of time to event variable and a status variable indicating whether the case is an event of censored at that time to event. Additional covariates to the model can be added at this stage (here age and sex are included).\n# Number of observations\nn &lt;- 400  \n\n# Create a dummy dataset with group-specific event probabilities\nset.seed(123)\ndummy_data &lt;- tibble(\n  time_to_event = rexp(n, rate = 0.1),  # Generate random survival times\n  sex = sample(c(\"Male\", \"Female\"), size = n, replace = TRUE),\n  age = rnorm(n, mean = 50, sd = 10),\n  status = ifelse(sex == \"Male\", \n                  rbinom(n, size = 1, prob = 0.8),  # Higher event probability for males\n                  rbinom(n, size = 1, prob = 0.4))  # Lower event probability for females\n)\n\n# Display the first 50 rows\nkable(dummy_data[1:50,])\n\n\n\n\ntime_to_event\nsex\nage\nstatus\n\n\n\n\n8.4345726\nFemale\n53.58856\n1\n\n\n5.7661027\nFemale\n43.91443\n1\n\n\n13.2905487\nFemale\n47.97759\n1\n\n\n0.3157736\nFemale\n47.26752\n0\n\n\n0.5621098\nMale\n45.31300\n0\n\n\n3.1650122\nFemale\n57.04167\n0\n\n\n3.1422729\nMale\n38.02636\n0\n\n\n1.4526680\nMale\n58.66366\n1\n\n\n27.2623646\nMale\n58.64152\n1\n\n\n0.2915345\nFemale\n38.01378\n0\n\n\n10.0483006\nMale\n56.39492\n1\n\n\n4.8021473\nFemale\n74.30227\n0\n\n\n2.8101363\nFemale\n44.42785\n0\n\n\n3.7711783\nMale\n58.44904\n1\n\n\n1.8828404\nFemale\n42.17798\n1\n\n\n8.4978613\nMale\n61.10711\n1\n\n\n15.6320354\nFemale\n52.49825\n0\n\n\n4.7876042\nMale\n66.51915\n0\n\n\n5.9093484\nMale\n35.41029\n1\n\n\n40.4101171\nFemale\n49.48702\n0\n\n\n8.4314973\nFemale\n44.73075\n1\n\n\n9.6587121\nFemale\n48.02735\n0\n\n\n14.8527579\nFemale\n43.70421\n0\n\n\n13.4804449\nFemale\n41.66156\n1\n\n\n11.6852898\nFemale\n55.78722\n1\n\n\n16.0585234\nFemale\n39.12419\n1\n\n\n14.9674287\nFemale\n64.84031\n0\n\n\n15.7065255\nMale\n38.13793\n1\n\n\n0.3176774\nMale\n51.01079\n1\n\n\n5.9784969\nMale\n55.32989\n0\n\n\n21.6783975\nFemale\n55.86735\n1\n\n\n5.0661573\nMale\n46.98253\n1\n\n\n2.5955782\nMale\n50.79502\n1\n\n\n25.9689212\nMale\n59.61264\n1\n\n\n12.2902573\nFemale\n35.43534\n1\n\n\n7.9068176\nFemale\n42.18260\n0\n\n\n6.2928008\nMale\n53.20402\n1\n\n\n12.5464100\nFemale\n45.55218\n0\n\n\n5.8868464\nMale\n63.70004\n1\n\n\n11.2929003\nFemale\n56.73254\n1\n\n\n4.2036480\nMale\n50.72167\n1\n\n\n72.1100758\nFemale\n34.92243\n1\n\n\n8.4572197\nFemale\n50.26100\n0\n\n\n2.2554201\nFemale\n46.83584\n1\n\n\n11.0033882\nMale\n48.97653\n0\n\n\n22.4830569\nMale\n38.18441\n1\n\n\n13.6373430\nFemale\n54.98658\n1\n\n\n5.7639167\nFemale\n39.61044\n0\n\n\n27.2527585\nFemale\n47.73778\n0\n\n\n13.1216304\nFemale\n53.81426\n1",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Survival Analysis with Cox-PH</span>"
    ]
  },
  {
    "objectID": "Cox_PH.html#fit-a-cox-proportional-hazards-cox-ph-model",
    "href": "Cox_PH.html#fit-a-cox-proportional-hazards-cox-ph-model",
    "title": "11  Survival Analysis with Cox-PH",
    "section": "11.2 Fit a Cox Proportional Hazards (Cox-PH) model",
    "text": "11.2 Fit a Cox Proportional Hazards (Cox-PH) model\nThe example below applies a Cox-PH model which tests whether survival is explained by age and sex.\n\n# Fit a Cox Proportional Hazards model\ncox_model &lt;- coxph(Surv(time_to_event, status) ~ age + sex, data = dummy_data)\n\n# Summary of the Cox PH model\nsummary(cox_model)\n\nCall:\ncoxph(formula = Surv(time_to_event, status) ~ age + sex, data = dummy_data)\n\n  n= 400, number of events= 241 \n\n             coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nage     -0.003303  0.996702  0.006417 -0.515    0.607    \nsexMale  0.802430  2.230955  0.140936  5.694 1.24e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nage        0.9967     1.0033    0.9842     1.009\nsexMale    2.2310     0.4482    1.6925     2.941\n\nConcordance= 0.594  (se = 0.02 )\nLikelihood ratio test= 36.97  on 2 df,   p=9e-09\nWald test            = 34.31  on 2 df,   p=4e-08\nScore (logrank) test = 36.18  on 2 df,   p=1e-08\n\n\nexp(coef) is essentially an odds ratio similar to those in a logistic regression.\nIn this example, being male carries a proportional hazard of 1.04 (95% CI 0.76 - 1.22) compared to being female.\nIf you like P values, Pr(&gt;|z|) is exactly that.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Survival Analysis with Cox-PH</span>"
    ]
  },
  {
    "objectID": "Cox_PH.html#plot-the-survival-curve-as-a-null-model-no-strata",
    "href": "Cox_PH.html#plot-the-survival-curve-as-a-null-model-no-strata",
    "title": "11  Survival Analysis with Cox-PH",
    "section": "11.3 Plot the survival curve as a null model (no strata)",
    "text": "11.3 Plot the survival curve as a null model (no strata)\n\nggsurvplot(survfit(cox_model), \n           data = dummy_data, \n           pval = TRUE,\n           risk.table = TRUE, \n           risk.table.title = \"Survival Table\",\n           surv.scale = \"percent\", # You can change this to other scales like \"probability\"\n           )",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Survival Analysis with Cox-PH</span>"
    ]
  },
  {
    "objectID": "Cox_PH.html#plot-the-survival-curves-strata",
    "href": "Cox_PH.html#plot-the-survival-curves-strata",
    "title": "11  Survival Analysis with Cox-PH",
    "section": "11.4 Plot the survival curve’s strata",
    "text": "11.4 Plot the survival curve’s strata\n\n# Plot separate survival curves for each sex without covariates\nggsurvplot(survfit(Surv(time_to_event, status) ~ sex, data = dummy_data), \n           data = dummy_data, pval = TRUE, \n           risk.table = TRUE, risk.table.title = \"Survival Table\",\n           surv.scale = \"percent\", # You can change this to other scales like \"probability\"\n           palette = c(\"blue\", \"red\"), # Colors for the curves\n           conf.int = TRUE, # Show confidence intervals\n           ggtheme = theme_minimal())",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Survival Analysis with Cox-PH</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html",
    "href": "Cumulative_Incidence.html",
    "title": "12  Cumulative Incidence",
    "section": "",
    "text": "12.1 Load Libraries\n# Load required packages\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(cmprsk)\n\nLoading required package: survival\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Modify the creation of the dummy dataset to include 'age' as a significant factor\nset.seed(1214223)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#dummy-data",
    "href": "Cumulative_Incidence.html#dummy-data",
    "title": "12  Cumulative Incidence",
    "section": "12.2 Dummy Data",
    "text": "12.2 Dummy Data\nCreate a dummy data set with two regions of London, an ‘age’ variable and an event status.\nThe age will be a significant predictor of the cumulative incidence\nThe region will be significant predictor of the cumulative incidence\n\n# Number of observations\nn &lt;- 4000\n\n\n# Create a dummy dataset with group-specific event probabilities\ndummy_data &lt;- tibble(\n  time_to_event = rexp(n, rate = 0.1),  # Generate random survival times\n  region = sample(c(\"Outside London\", \"Inside London\"), size = n, replace = TRUE),\n  age = rnorm(n, mean = 8, sd = 10),\n  # Adjust the probability to make both age and region significantly affect the event status\n  status = ifelse(region == \"Inside London\",\n                  rbinom(n, size = 1, prob = plogis(-1.5 - 0.02 * age)),  # Stronger logistic effect of age for \"Inside London\"\n                  rbinom(n, size = 1, prob = plogis(-0.5 - 0.02 * age)))  # Weaker logistic effect of age for \"Outside London\"\n)\n\nThe Fine-Grey model is a good multipurpose cumulative incidence model. We’ll give it a matrix of covariates\n\n# Create the Fine-Gray model\nfg_model &lt;- crr(\n  ftime = dummy_data$time_to_event,    # Time to event\n  fstatus = dummy_data$status,         # Status (event vs censoring)\n  cov1 = model.matrix(~ region + age, data = dummy_data)[, -1]  # Covariates (excluding intercept column)\n)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#summary-of-the-model",
    "href": "Cumulative_Incidence.html#summary-of-the-model",
    "title": "12  Cumulative Incidence",
    "section": "12.3 Summary of the model",
    "text": "12.3 Summary of the model\nCheck that both region and age are predictors of cumulative incidence\n\n# Summary of the model\nsummary(fg_model)\n\nCompeting Risks Regression\n\nCall:\ncrr(ftime = dummy_data$time_to_event, fstatus = dummy_data$status, \n    cov1 = model.matrix(~region + age, data = dummy_data)[, -1])\n\n                         coef exp(coef) se(coef)     z p-value\nregionOutside London  0.61488     1.849  0.06521  9.43  0.0000\nage                  -0.00947     0.991  0.00315 -3.01  0.0026\n\n                     exp(coef) exp(-coef)  2.5% 97.5%\nregionOutside London     1.849      0.541 1.628 2.102\nage                      0.991      1.010 0.984 0.997\n\nNum. cases = 4000\nPseudo Log-likelihood = -7527 \nPseudo likelihood ratio test = 101  on 2 df,",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#create-cumulative-incidence-data-using-cuminc-function-for-uncorrected-cumulative-incidence",
    "href": "Cumulative_Incidence.html#create-cumulative-incidence-data-using-cuminc-function-for-uncorrected-cumulative-incidence",
    "title": "12  Cumulative Incidence",
    "section": "12.4 Create cumulative incidence data using ‘cuminc’ function for uncorrected cumulative incidence",
    "text": "12.4 Create cumulative incidence data using ‘cuminc’ function for uncorrected cumulative incidence\n\nfit &lt;- cuminc(ftime = dummy_data$time_to_event, fstatus = dummy_data$status, group = dummy_data$region)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#wrangle-the-data-set",
    "href": "Cumulative_Incidence.html#wrangle-the-data-set",
    "title": "12  Cumulative Incidence",
    "section": "12.5 Wrangle the data set",
    "text": "12.5 Wrangle the data set\n\nuncorrected_ci_data &lt;- do.call(rbind, lapply(names(fit), function(region_name) {\n  if (\"est\" %in% names(fit[[region_name]])) {\n    data.frame(\n      time = fit[[region_name]]$time,\n      cuminc = fit[[region_name]]$est,\n      region = gsub(\" 1\", \"\", region_name)  # Clean the region name\n    )\n  }\n}))\n\n# Extract coefficients from the Fine-Gray model\ncoefficients &lt;- summary(fg_model)$coef\n\n# Extract coefficients for \"regionOutside London\" and \"age\"\noutside_london_coef &lt;- coefficients[\"regionOutside London\", \"coef\"]\nage_coef &lt;- coefficients[\"age\", \"coef\"]\n\n# Calculate the average age in the dataset\nmean_age &lt;- mean(dummy_data$age)\n\n# Extract the coefficients vector from the Fine-Gray model\ncoef_vector &lt;- coefficients[, \"coef\"]\n\n# Create the design matrix for the covariates (excluding intercept)\ncovariate_matrix &lt;- model.matrix(~ region + age, data = dummy_data)[, -1]  # Adjust as needed for additional covariates\n\n# Calculate the linear predictor for each observation\n# This is the dot product of the covariate values and the corresponding coefficients\nlinear_predictor &lt;- covariate_matrix %*% coef_vector",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#use-the-uncorrected-cumulative-incidence-as-a-baseline-and-adjust-for-the-effect-of-all-covariates",
    "href": "Cumulative_Incidence.html#use-the-uncorrected-cumulative-incidence-as-a-baseline-and-adjust-for-the-effect-of-all-covariates",
    "title": "12  Cumulative Incidence",
    "section": "12.6 Use the uncorrected cumulative incidence as a baseline and adjust for the effect of all covariates",
    "text": "12.6 Use the uncorrected cumulative incidence as a baseline and adjust for the effect of all covariates\n\n# \ncorrected_ci_data &lt;- uncorrected_ci_data %&gt;%\n  mutate(\n    # Apply the linear predictor to calculate the corrected cumulative incidence\n    corrected_cuminc = cuminc * exp(linear_predictor[match(region, dummy_data$region)]),\n    # Calculate standard errors for confidence intervals\n    se_cuminc = sqrt(diag(fg_model$var)[1])  # Simplified extraction of standard error for illustration purposes\n  ) %&gt;%\n  mutate(\n    ci_upper = corrected_cuminc + 1.96 * se_cuminc,\n    ci_lower = corrected_cuminc - 1.96 * se_cuminc,\n    type = \"Corrected\"\n  )\n\n# Add a new column to indicate whether the data is corrected or uncorrected\nuncorrected_ci_data &lt;- uncorrected_ci_data %&gt;%\n  mutate(type = \"Uncorrected\")\n\ncorrected_ci_data &lt;- corrected_ci_data %&gt;%\n  mutate(type = \"Corrected\")\n\n# Combine both corrected and uncorrected data\ncombined_ci_data &lt;- bind_rows(\n  uncorrected_ci_data %&gt;% rename(cif = cuminc),\n  corrected_ci_data %&gt;% rename(cif = corrected_cuminc)\n)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Cumulative_Incidence.html#plot-corrected-cumulative-incidence-curves-with-confidence-intervals",
    "href": "Cumulative_Incidence.html#plot-corrected-cumulative-incidence-curves-with-confidence-intervals",
    "title": "12  Cumulative Incidence",
    "section": "13.1 Plot corrected cumulative incidence curves with confidence intervals",
    "text": "13.1 Plot corrected cumulative incidence curves with confidence intervals\n\ncuminc_plot_corrected &lt;- ggplot(corrected_ci_data, aes(x = time, y = corrected_cuminc, color = region)) +\n  geom_ribbon(aes(x = time, ymin = ci_lower, ymax = ci_upper, fill = region), alpha = 0.2) +\n  geom_line(size = 1.2) +\n  # Labels and styling\n  labs(\n    title = \"Cumulative Incidence Function: Corrected (Fine-Gray)\",\n    x = \"Time to Event\",\n    y = \"Cumulative Incidence\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 14),\n    legend.title = element_text(size = 14),\n    legend.position = \"bottom\"\n  )\n\n# Calculate the number at risk for the unadjusted risk table\ntime_intervals &lt;- seq(0, max(dummy_data$time_to_event), by = 10)  # Set time intervals (e.g., every 10 units)\n\nrisk_data &lt;- lapply(time_intervals, function(t) {\n  dummy_data %&gt;%\n    filter(time_to_event &gt;= t) %&gt;%\n    group_by(region) %&gt;%\n    summarise(at_risk = n(), .groups = 'drop') %&gt;%\n    mutate(time = t)\n}) %&gt;%\n  bind_rows()\n\n# Plot the risk table with unadjusted number at risk\nrisk_table_plot &lt;- ggplot(risk_data, aes(x = time, y = region, label = at_risk)) +\n  geom_text(size = 3) +\n  labs(\n    x = \"Time to Event\",\n    y = \"Region\",\n    title = \"Number at Risk\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14),\n    axis.title.y = element_blank(),\n    axis.text.y = element_text(size = 10),\n    panel.grid.major = element_blank(),  # Remove grid lines for clarity\n    panel.grid.minor = element_blank()\n  )\n\n# Combine the corrected cumulative incidence plot and unadjusted risk table using patchwork\ncuminc_plot_corrected / risk_table_plot + plot_layout(heights = c(3, 1))\n\n\n\n\n\n\n\n\n\nTo Do\n\nAdd competing risks",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Cumulative Incidence</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html",
    "href": "Interrupted_Time_Series.html",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "",
    "text": "13.1 Introduction\nThis is a brief introduction to Interrupted Time Series analyses. It’s intended for use by people who have done some reading and understand about concepts like autocorrelation.\nI won’t be explaining all the technical stuff here but will be focussing on getting things done.\nThe general methods used in this set of examples are very well described in this paper on Segmented regression analysis of interrupted time series studies in medication use research, by Anita K. Wagner and colleagues and I recommend that you start by reading that paper at least twice.\nThe methods below will show you how to carry out an interrupted time series (ITS) using generalised least squares, accounting for autocorrelation via a corARMA model. We’ll start out with some fairly simple models, then build up to something a bit more complicated, before going on to add a control group.\nWe’re going to simulate a study where we are interested to know whether the values of a quantity (quantity.x) have changed significantly in response to some kind of intervention. If it helps you to think of a real-world situation, imagine we take weekly measurements of how many bacteria are found on the handles of public toilet doors. Our intervention might be placing signs about handwashing on all the doors. Can we say that putting up the signs correlated with a decrease in the number of bacteria counted each week?\nAt the core of this analysis is the concept of a counterfactual. We’ll be doing some modelling to estimate what did happen (the factual scenario) and what we expect would have happened (the counterfactual scenario) if we had never hung up the signs about handwashing.\nWe’ll be measuring whether the parameter quantity.x changes in response to the Intervention, but there are different kinds of change that we might expect.\nThe change could take two forms including\nDifferent real world scenarios may or may not be compatible with the assumptions that these types of change could occur, so use your knowledge to decide which ones to model.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#introduction",
    "href": "Interrupted_Time_Series.html#introduction",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "",
    "text": "A step change immediately after the intervention\n\nfor instance because the signs are so effective that everyone suddenly starts washing their hands much better\n\nA slope/trend change after the intervention\n\nbecause over time, the presence of the signs conditions more and more people to wash their hands. Alternatively, maybe after an initial step change down, it starts to creep back up because people start to ignore the signs\n\n\n\n\n13.1.1 This document provides examples of three main type of ITS analysis\n\nPart One - Uncontrolled ITS, one intervention\nPart Two - Uncontrolled ITS, two interventions\nPart Three - Controlled ITS, one intervention\n\nIn each case we will build a data set that shows each component of the model in a fairly longhand format. It’s totally possible to use fewer variables and to use interactions (shown further down this document) to model all the various components of the ITS, but this is harder to understand to the novice and harder to decode when it comes to reconciling the data frame against the model coefficients that will be used to interpret what effects the interruption had.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-one---uncontrolled-its-one-intervention",
    "href": "Interrupted_Time_Series.html#part-one---uncontrolled-its-one-intervention",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.2 Part One - Uncontrolled ITS, one intervention",
    "text": "13.2 Part One - Uncontrolled ITS, one intervention\nWe’ll create a dummy dataframe, where\n\nTime = A number, the time in study weeks (1 - 100 weeks)\nIntervention = A binary indication of whether the Intervention has taken place at Time x\nPost.intervention.time = A number, the time elapsed since the Intervention\nquantity.x = The thing we want to measure\n\n\n# Create a dummy dataframe\n\n# Here the interruption takes place at week 51\ndf &lt;- tibble(\n  Time = 1:100,\n  Intervention = c(rep(0, 50), rep(1, 50)),\n  Post.intervention.time = c(rep(0, 50), 1:50),\n  quantity.x = c(sort(sample(200:300, size = 50, replace = TRUE), decreasing = TRUE) + sample(-20:20, 50, replace = TRUE), \n                 sort(sample(20:170, size = 50, replace = TRUE), decreasing = TRUE) + sample(-40:40, 50, replace = TRUE))\n)\n\n# Fit an ARIMA model to generate moderate autocorrelation\nset.seed(123) # Set seed for reproducibility\n# ARIMA(2, 0, 0) model\narima_model &lt;- Arima(df$quantity.x, order = c(2, 0, 0))\n\n# Extract residuals and amplify them to introduce moderate effect\namplified_residuals &lt;- residuals(arima_model) * 1.2  # Amplify residuals by multiplying by 1.2\n\n# Add the amplified residuals back to the original values, leaving the first value intact\ndf &lt;- df %&gt;%\n  mutate(quantity.x = quantity.x + c(0, amplified_residuals[-1]))\n\n# Display the dataframe with moderate autocorrelation\ndatatable(df, options = list(pageLength = 100, scrollY = \"200px\"))\n\n\n\n\n\nIn the simplest form, the ITS model looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=“ML”)\nUsing the gls command from the nlme package, we can create a model that describes the change in quantity.x across time\n\nmodel.a = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=\"ML\")\n\n# Show a summary of the model\nsummary(model.a)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time + Intervention + Post.intervention.time \n  Data: df \n       AIC      BIC    logLik\n  1049.891 1062.917 -519.9454\n\nCoefficients:\n                           Value Std.Error   t-value p-value\n(Intercept)            295.79584 12.847369 23.023846  0.0000\nTime                    -2.01474  0.438474 -4.594891  0.0000\nIntervention           -34.38002 17.902424 -1.920411  0.0578\nPost.intervention.time  -0.93919  0.620096 -1.514583  0.1332\n\n Correlation: \n                       (Intr) Time   Intrvn\nTime                   -0.870              \nIntervention            0.348 -0.600       \nPost.intervention.time  0.615 -0.707 -0.017\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.4025774 -0.6727850  0.0287960  0.7829453  2.5718791 \n\nResidual standard error: 43.83865 \nDegrees of freedom: 100 total; 96 residual\n\n\nThese coefficients have real world meaning and are not there to be ignored.\nThis tells us that the modelled line passes the y axis at 295.6 units and that prior to the intervention, the average value of quantity.x was falling by 1.98 units per week. At the intervention, there was a step change of -24.8 units and subsequent to the intervention there was a trend in which the value of quantity.x fell by an additional 1.12 units per week. The last distinction is important because that means that after the intervention, the weekly decrease in quantity.x was 1.98 + 1.12 = 3.10\nThese statistics tell us whether the changes that happened at different timepoints were significant with respect to what the line was doing before the interruption. These values can be used to calculate the overall difference in quantity.x between times [i] and [ii] using this formula\nquantity.x[i] = 308.27918 + (Time[i]*-1.97565) + (Intervention[i]*-27.33565) + (Post.intervention.time[i]*-1.17575)\nquantity.x[ii] = 308.27918 + (Time[ii]*-1.97565) + (Intervention[ii]*-27.33565) + (Post.intervention.time[ii]*-1.17575)\nabsolute difference = difference(quantity.x[i] , quantity.x[ii])\nBut what would be nice would be to calculate the counterfactual scenario where the intervention didn’t happen and to estimate the difference between the factual values of quantity.x at time [i] and the counterfactuals at the same time [i]. This is coming up later.\nIt’s nice to add values from models to the df, so we will next copy the modelled values of quantity.x in to the df using the predictSE.gls command from the AICcmodavg package.\n\ndf&lt;-df %&gt;% mutate(\n  model.a.predictions = predictSE.gls (model.a, df, se.fit=T)$fit,\n  model.a.se = predictSE.gls (model.a, df, se.fit=T)$se\n  )\n\nNote that we captured both the predicted value of quantity.x and also the standard error on the estimate. We’ll need that to show the error margins and to draw confidence intervals on our charts.\nLet’s draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.a.predictions - (1.96*model.a.se), ymax = model.a.predictions + (1.96*model.a.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.a.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\n\n\n\n\nBefore we get too much further though, we need to look at autocorrelation in the data. gls allows us to add corARMA structures by specifying values for p (the autoregressive order) and q (the moving average order).\n\n13.2.1 Autocorrelation\nAdding residual diagnostics can be nice start point from which to explore the data and see if we can see evidence for autocorrelation\nFirst we’ll define a function to extract the residuals from an ITS dataset\n\n# Function to calculate ACF or PACF values\nget_acf_pacf &lt;- function(residuals, max_lag, type = c(\"acf\", \"pacf\"), model_name) {\n  type &lt;- match.arg(type)\n  if (type == \"acf\") {\n    values &lt;- acf(residuals, lag.max = max_lag, plot = FALSE)\n  } else {\n    values &lt;- pacf(residuals, lag.max = max_lag, plot = FALSE)\n  }\n  data.frame(\n    lag = values$lag,\n    acf = values$acf,\n    model = model_name,\n    type = type\n  )\n}\n\nWe’ll also need to think about a threshold value at which a residual is significant. Let’s set this to 95% of the distribution of values for quantity.x.\n\nn &lt;- length(df$quantity.x) # Number of observations\nthreshold &lt;- 1.96 / sqrt(n)\n\nNext we’ll extract the residuals from model.a\nThis is annual data, so the longest lag max_lag we could imagine would be a full year 52 week cycle\n\n# Extract normalized residuals from the models\nresiduals_a &lt;- residuals(model.a, type = 'normalized')\n\n# Set the maximum lag for the autocorrelation and partial autocorrelation functions\nmax_lag &lt;- 52\n\n# Get ACF and PACF data for model.a\nacf_a &lt;- get_acf_pacf(residuals_a, max_lag, \"acf\", \"Model A\")\npacf_a &lt;- get_acf_pacf(residuals_a, max_lag, \"pacf\", \"Model A\")\n\n\n13.2.1.1 Plot Autocorrelation Function (ACF) residuals\nThe ACF (Autocorrelation Function) is primarily used to determine the appropriate value for the Moving Average (MA)order q in an ARIMA model.\n\nggplotly(\nggplot(acf_a, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"ACF for Model A\",\n    x = \"Lag\",\n    y = \"ACF\"\n  )\n)\n\n\n\n\n\nThe big spike at lag 0 in the ACF of the residuals is entirely expected. This just shows how any data point is correlated to itself and can be ignored. If you have other significant spikes, then this might be addressed by including a moving average (MA) term. In a corARMA model, this means defining the value q. If you have spikes in higher lags, then maybe you need to increase the value of q to match. Those spikes imply that there is still autocorrelation in the data that has not been captured by the model. If you include q=2, then the moving average would include both of the lagged residuals up to 2 (i.e. lag =1, lag = 2)\nYou should look for where the ACF cuts off (i.e. where it drops to within the confidence bounds)\nIf the ACF shows significant correlations for the first few lags, but then cuts off after a specific lag (say lag 2, as seen here), it suggests that an MA(2) [i.e. q=2] model might be appropriate.\nThis is because the MA model accounts for how current values are related to past forecast errors.\n\n\n13.2.1.2 Plot Partial Autocorrelation Function PACF residuals\nPACF (Partial Autocorrelation Function) is used to determine the appropriate value for the Autoregressive (AR) order p in an ARIMA model.\n\nggplotly(\nggplot(pacf_a, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~ model) +\n  theme_minimal() +\n  labs(\n    title = \"PACF for Model A\",\n    x = \"Lag\",\n    y = \"PACF\"\n  )\n)\n\n\n\n\n\nThe PACF plot helps identify the lag order by indicating the direct effect of each lag while controlling for intermediate lags.\nYou should look for where the PACF plot cuts off. For instance, if the PACF has a significant spike at lag 2 but becomes insignificant after that, it suggests that an AR(2) [i.e. p=2] model might be appropriate.\nThis is because an AR model captures the relationship between a current value and its past values.\nIn this data set there’s a significant spike at ACF lag=2 and another in PACF at 2. This might suggest that we should use an MA(2) AR(2) function. This would relate to p=2, q=2 in the R code.\n\n\n\n13.2.2 A gls model with a corARMA correction\nThe corrected model looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,correlation= corARMA(p=1, q=1, form = ~ Time),method=“ML”)\nbut the critical issue is how to choose the correct values of p and q. There’s lots of information about this online, so read carefully because it is important. Whether you exactly understand what this is all about or not, you’ll still need to take steps to minimise the problem empirically.\nSo far we’ve looked at the ACF and PACF and have an idea that q=2, p=3 would be a good model for this data.\nWe’ll start by defining the basic model and then creating a basic R function that can apply different values of p and q to that model\n\nmod.1 = quantity.x ~ Time + Intervention + Post.intervention.time\n\nfx = function(pval,qval){summary(gls(mod.1, data = df, correlation= corARMA(p=pval,q=qval, form = ~ Time),method=\"ML\"))$AIC}\n\nNext, let’s test out the provisional values of p and q.\n\nmodel.b = gls(mod.1, data = df, correlation= corARMA(p=2,q=2, form = ~ Time),method=\"ML\")\n\nand extract the residuals\n\n# Extract normalized residuals from the models\nresiduals_b &lt;- residuals(model.b, type = 'normalized')\n\n\n# Get ACF and PACF data for model.a\nacf_b &lt;- get_acf_pacf(residuals_b, max_lag, \"acf\", \"Model B\")\npacf_b &lt;- get_acf_pacf(residuals_b, max_lag, \"pacf\", \"Model B\")\n\n\n\n13.2.3 Plot Autocorrelation Function (ACF) residuals\nThe ACF (Autocorrelation Function) is primarily used to determine the appropriate value for the Moving Average (MA)order q in an ARIMA model.\n\nggplotly(\nggplot(acf_b, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(\n    title = \"ACF for Model B\",\n    x = \"Lag\",\n    y = \"ACF\"\n  )\n)\n\n\n\n\n\n\n\n13.2.4 Plot Partial Autocorrelation Function PACF residuals\nPACF (Partial Autocorrelation Function) is used to determine the appropriate value for the Autoregressive (AR) order p in an ARIMA model.\n\nggplotly(\nggplot(pacf_b, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~ model) +\n  theme_minimal() +\n  labs(\n    title = \"PACF Model B\",\n    x = \"Lag\",\n    y = \"PACF\"\n  )\n)\n\n\n\n\n\n\n\n13.2.5 A More Exhaustive search for the best autocorrelation values\nUsing our empirically determined values of p and q seem to have helped a little, but there’s still a significant autocorrelation at lag 15 in the ACF plot and a significant spike at PACF lag 10.\nWhat I often do here is to apply an exhaustive search of all combinations of values of p and q to the gls model that might be sifnificant, capturing the value of the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for each model. Whichever combination of values of p and q returns the smallest AIC value is the best fit for our modelling.\nRealistically there’s some margin for human biases here. It’s hard to exactly define good numbers to put in to this analysis. I’d recommend a broad range of values because sometime autocorrelations can be quite lagged. In this case, it’s clear that something is going on as far as p=10 and q=10 because both ACF and PACF plots have a spike at lag 10.\nOur start point is the AIC value of the model we ran earlier where neither p or q were specified (i.e. no autocorrelation)\n\np = summary(gls(mod.1, data = df,method=\"ML\"))$AIC\nmessage(str_c (\"AIC Uncorrelated model = \",p))\n\nAIC Uncorrelated model = 1049.8908715391\n\n\nNext we can create a grid of combinations of values of p and q\nAbove we can see that sensible values of q might be 0,1,2,10,14,15 and 25. Meanwhile sensible values of p might be 0,1,2,10 and 15\n\nautocorrel = expand.grid(qval = c(0,1,2,10,14,15,25), pval = c(0,1,2,10,15))\n\nThen we apply the function to all possible combinations of p and q. Expect some not to work with this dummy data set because the models won’t all converge. You can fiddle with the settings to increase the number of iterations if you have issues with non-convergence. This may also take a while as there’s lots of computations happening. For the sake of illustration, I’m going to remove the ones which don’t converge from this example.\n\n# Increase the number of iterations for the optimization process\ncontrol &lt;- nlme::glsControl(maxIter = 1500, msMaxIter = 1500)  # Adjust max iterations as needed\n\nfor (i in 2:nrow(autocorrel)) {\n  try({\n    model &lt;- gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i], q = autocorrel$qval[i], form = ~ Time), method = \"ML\")\n    autocorrel$BIC[i] &lt;- AIC(model, k = log(nrow(df)))  # Calculate BIC using AIC function\n    autocorrel$AIC[i] &lt;- AIC(model)  # Calculate AIC directly\n  })\n}\n\nError in `$&lt;-.data.frame`(`*tmp*`, \"BIC\", value = c(NA, 1060.5897436766 : \n  replacement has 2 rows, data has 35\nError in `$&lt;-.data.frame`(`*tmp*`, \"BIC\", value = c(NA, NA, 1057.94350685086 : \n  replacement has 3 rows, data has 35\nError in `$&lt;-.data.frame`(`*tmp*`, \"BIC\", value = c(NA, NA, NA, 1081.84593352225 : \n  replacement has 4 rows, data has 35\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  function evaluation limit reached without convergence (9)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  false convergence (8)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  function evaluation limit reached without convergence (9)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in `coef&lt;-.corARMA`(`*tmp*`, value = value[parMap[, i]]) : \n  NA/NaN/Inf in foreign function call (arg 1)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  false convergence (8)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  function evaluation limit reached without convergence (9)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  function evaluation limit reached without convergence (9)\nError in gls(mod.1, data = df, correlation = corARMA(p = autocorrel$pval[i],  : \n  false convergence (8)\n\n# Remove rows with NA in BIC or AIC\n#autocorrel &lt;- autocorrel %&gt;% filter(!is.na(BIC), !is.na(AIC))\n\nautocorrel \n\n   qval pval      BIC      AIC\n1     0    0       NA       NA\n2     1    0       NA       NA\n3     2    0       NA       NA\n4    10    0       NA       NA\n5    14    0 1096.567 1047.069\n6    15    0 1100.903 1048.800\n7    25    0       NA       NA\n8     0    1 1064.656 1049.025\n9     1    1 1055.094 1036.858\n10    2    1 1062.361 1041.519\n11   10    1       NA       NA\n12   14    1       NA       NA\n13   15    1       NA       NA\n14   25    1       NA       NA\n15    0    2 1059.878 1041.642\n16    1    2 1062.179 1041.337\n17    2    2 1061.106 1037.660\n18   10    2       NA       NA\n19   14    2       NA       NA\n20   15    2 1096.567 1047.069\n21   25    2       NA       NA\n22    0   10 1087.249 1048.171\n23    1   10 1081.233 1039.551\n24    2   10 1085.379 1041.091\n25   10   10 1096.567 1047.069\n26   14   10       NA       NA\n27   15   10       NA       NA\n28   25   10       NA       NA\n29    0   15 1105.131 1053.028\n30    1   15 1095.635 1040.926\n31    2   15 1092.984 1035.670\n32   10   15       NA       NA\n33   14   15 1133.788 1045.212\n34   15   15       NA       NA\n35   25   15 1096.567 1047.069\n\n\n\n\n13.2.6 Show the AIC vs BIC\nChoose one which has low values for both dimensions\n\nggplotly(\n  ggplot(autocorrel, aes(AIC, BIC, label = paste(\"p=\", pval, \" q=\", qval))) +\n    geom_point()\n)\n\n\n\n\n\nHere, there’s models which are more predictive (i.e. AIC is lower) but which have relatively higher potential for overfitting (BIC is higher) and vice versa. I think I’m going to go with p=1, q=1 as this balances the two priorities and seems to be the best overall combination\nLet’s see what effect that has on our model by making a new model.c and comparing it to the original model.a and p=2,q=2 (model.b)\n\nmodel.c = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=\"ML\", correlation= corARMA(p=1,q=1, form = ~ Time))",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#check-the-residuals",
    "href": "Interrupted_Time_Series.html#check-the-residuals",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.3 Check the residuals",
    "text": "13.3 Check the residuals\nAdding residual diagnostics can help confirm that the model adequately captures the dynamics of the data. Let’s add residual checks to ensure our assumptions hold.\n\n# Extract normalized residuals from the models\nresiduals_a &lt;- residuals(model.a, type = 'normalized')\nresiduals_b &lt;- residuals(model.b, type = 'normalized')\nresiduals_c &lt;- residuals(model.c, type = 'normalized')\n\n\n# Set the maximum lag for the autocorrelation and partial autocorrelation functions\nmax_lag &lt;- 52\n\n# Get ACF and PACF data for both models\nacf_a &lt;- get_acf_pacf(residuals_a, max_lag, \"acf\", \"Model A\")\npacf_a &lt;- get_acf_pacf(residuals_a, max_lag, \"pacf\", \"Model A\")\n\nacf_b &lt;- get_acf_pacf(residuals_b, max_lag, \"acf\", \"Model B\")\npacf_b &lt;- get_acf_pacf(residuals_b, max_lag, \"pacf\", \"Model B\")\n\nacf_c &lt;- get_acf_pacf(residuals_c, max_lag, \"acf\", \"Model c\")\npacf_c &lt;- get_acf_pacf(residuals_c, max_lag, \"pacf\", \"Model c\")\n\n# Combine all data into a single dataframe\nacf_data &lt;- bind_rows(acf_a, acf_b,acf_c)\npacf_data &lt;- bind_rows(pacf_a, pacf_b,pacf_c)\n\nPlot Autocorrelation Function ACF residuals\n\nggplot(acf_data, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~ model) +\n  theme_minimal() +\n  labs(\n    title = \"ACF for Models A, B and C\",\n    x = \"Lag\",\n    y = \"ACF\"\n  )\n\n\n\n\n\n\n\n\nYou can see that there’s relatively little difference between model.b and model.c\n\n13.3.1 Plot Partial Autocorrelation Function PACF residuals\n\nggplotly(\nggplot(pacf_data, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"steelblue\") +\n  geom_hline(yintercept = c(-threshold, threshold), linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~ model) +\n  theme_minimal() +\n  labs(\n    title = \"PACF for Models A, B and C\",\n    x = \"Lag\",\n    y = \"ACF\"\n  )\n)\n\n\n\n\n\nIt even looks like model.c is worse than model.b in this respect. Really you could go back and forth all day on this!",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#look-how-maar-corrections-affect-results",
    "href": "Interrupted_Time_Series.html#look-how-maar-corrections-affect-results",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.4 Look how MA/AR corrections affect results",
    "text": "13.4 Look how MA/AR corrections affect results\n\ncoefficients(model.a)\n\n           (Intercept)                   Time           Intervention \n           295.7958393             -2.0147413            -34.3800207 \nPost.intervention.time \n            -0.9391869 \n\ncoefficients(model.b)\n\n           (Intercept)                   Time           Intervention \n            293.695634              -2.042827             -21.229331 \nPost.intervention.time \n             -1.269343 \n\ncoefficients(model.c)\n\n           (Intercept)                   Time           Intervention \n            293.710803              -2.041917             -21.456791 \nPost.intervention.time \n             -1.262984 \n\n\nYou can see that there’s some big changes to the values here.\nThe initial correction we did in model.b had a big effect, changing both the magnitude of the intervention (by ~13 units) and the post.intervention.time (by ~ 0.3 units per week. Post.intervention.time describes a weekly trend, so over long time periods, the error would creep up to make a huge difference in your results if you didn’t correct for autocorrelation.\nThe additional refinement we made by selecting model.c which had a higher value of p and q and which had less variance in the residuals plots, didn’t make as big a difference at all, which I suppose we should expect from looking at the ACF and PACF curves. Higher complexity models would likely overfit the data, make it hard to calculate models on counterfactuals and generally make your life bad, so I suggest just doing something relatively simple and accepting that there’s always some margin of error and a trade-off between predictive capacity and overfitting of the model.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#ljung-box-test",
    "href": "Interrupted_Time_Series.html#ljung-box-test",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.5 Ljung-Box Test",
    "text": "13.5 Ljung-Box Test\nThis simple test will tell you if there’s significant autocorrelation left in the residuals. If p&lt;0.05 there is.\n\nBox.test(residuals(model.a), type = \"Ljung-Box\", lag = 20)\n\n\n    Box-Ljung test\n\ndata:  residuals(model.a)\nX-squared = 23.272, df = 20, p-value = 0.2756\n\nBox.test(residuals(model.b), type = \"Ljung-Box\", lag = 20)\n\n\n    Box-Ljung test\n\ndata:  residuals(model.b)\nX-squared = 23.87, df = 20, p-value = 0.2481\n\nBox.test(residuals(model.c), type = \"Ljung-Box\", lag = 20)\n\n\n    Box-Ljung test\n\ndata:  residuals(model.c)\nX-squared = 23.878, df = 20, p-value = 0.2478\n\n\nIf we went on this alone, we’d accept the first model.a, but actually I prefer model.b for reasons described above. Obviously, your own data set will have its own set of peculiarities that you’ll need to investigate.\n\n13.5.1 Predicted values of model.b\nOK, so now we’ve dealt with the autocorrelation, let’s assign the predicted values of our preferred model.b on to the df\n\ndf&lt;- df %&gt;% \n  mutate(\n      model.b.predictions = predictSE.gls (model.b, df, se.fit=T)$fit,\n      model.b.se = predictSE.gls (model.b, df, se.fit=T)$se\n        )\n\nNext we need to ask what would have happened if there had been no intervention. This is the counterfactual model.\nThe gls model for the counterfactual looks like this…\ngls(quantity.x ~ Time, data = df,method=“ML”)\nThere’s nothing clever about this, it’s the same model as we had before, only we’ve taken out the intervention and post intervention arguments. Our aim here is to calculate the pre-intervention trend and simply to extrapolate out beyond the intervention time point. This can be done with the predictSE.gls function.\nTo create the counterfactual model, we have to make a new df which truncates the data at the time point immediately before the intervention. Then we run predict on the model, providiing the original df as ‘newdata’.\n\ndf2&lt;-filter(df,Time&lt;51)\nmodel.d = gls(quantity.x ~ Time, data = df2, correlation= corARMA(p=2,q=2, form = ~ Time),method=\"ML\")\n\nLet’s have a look at how the new counterfactual (model.d, overwriting the old one because I’m lazy) model compares to the factual model (model.a).\n\ncoefficients(model.a)\n\n           (Intercept)                   Time           Intervention \n           295.7958393             -2.0147413            -34.3800207 \nPost.intervention.time \n            -0.9391869 \n\n\n\ncoefficients(model.d)\n\n(Intercept)        Time \n 294.392508   -1.998109 \n\n\nAs you can see here, the intercept and slope of the factual and counterfactual models are almost identical, which is what we wanted. If you were a purist who cared about those little differences, you could use the actual values from model.a to calculate the counterfactuals manually by doing this\ny = 295.7958393 + (Time * -2.0147413)\nThat’s great in terms of accuracy of your y value estimates, but it is much harder to calculate the standard errors manually so your precision/confidence intervals becomes a real pain. This is especially true when the model gets more complicated (as we’ll see in part two).\nSo let’s accept a little error in the accuracy and use ‘predict’ because it gives us nice precise confidence estimates, let’s make a new variable with predictions of the counterfactual model, providing the full 100 week data frame as ‘newdata’\n\ndf&lt;-df %&gt;% mutate(\n  model.d.predictions = predictSE.gls (model.d, newdata = df, se.fit=T)$fit,\n  model.d.se = predictSE.gls (model.d, df, se.fit=T)$se\n)\n\nNext we can plot the chart\n\n  ggplot(df,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.d.predictions - (1.96*model.d.se), ymax = model.d.predictions + (1.96*model.d.se)), fill = \"pink\")+\n  geom_line(aes(Time,model.d.predictions),color=\"red\",lty=2)+\n  geom_ribbon(aes(ymin = model.b.predictions - (1.96*model.b.se), ymax = model.b.predictions + (1.96*model.b.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.b.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\n\n\n\n\nThe solid line with green ribbon is the factual data, the red dashed line with the pink ribbon is the counterfactual. Using the rule of thumb that if the confidence intervals don’t overlap, there’s something significant happening, we can conclude that the interruption preceded a significant step change in quantity.x. That the lines also diverge suggests that there could be a significant trend change.\nThe last remaining thing we need to do is to calculate those relative differences. This is easy because we’ve been adding the modelled values to the df as variables. To get a list of the relative differences at different timepoints, we really only have to do subtract the factual from the counterfactual.\nHere’ we can ask for the relative differences at weeks 1 (start), 50 (pre-intervention), 51 (immediate post-intervention) and 100 (end of the study)\n\nformat(df$model.b.predictions-df$model.d.predictions,scientific = F)[c(1,50,51,100)]\n\n            1            50            51           100 \n\" -0.7415923\" \" -2.9327748\" \"-25.4761670\" \"-89.8651631\" \n\n\nHere we can see that pre-intervention, the difference between factual and counterfactual is essentially zero, which is what we expect. At week 51 we see that the difference is 26 units, almost the same as the step change we saw in the coefficients for model.b above. At week 100 the factual data are 82 units lower than the counterfactual, which is the combined effect of the intervention step change and the intervention trend change.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-two---uncontrolled-its-two-interventions",
    "href": "Interrupted_Time_Series.html#part-two---uncontrolled-its-two-interventions",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.6 Part Two - Uncontrolled ITS, two interventions",
    "text": "13.6 Part Two - Uncontrolled ITS, two interventions\nSome designs may include multiple interventions and it is fairly simple to extend the model to account for this kind of thing We’ll make a new data set that includes a second intervention and a post-intervention-2 trend.\n\n# create a dummy dataframe\n# Here the interruption takes place at week 51\ndf3&lt;-tibble(\n  Time = 1:150,\n  Intervention = c(rep(0,50),rep(1,100)),\n  Post.intervention.time = c(rep(0,50),1:100),\n  Intervention.2 = c(rep(0,100),rep(1,50)),\n  Post.intervention.2.time = c(rep(0,100),1:50),\n  quantity.x = c(sort(sample(2000:2500,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(200:1700,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)),c(sort(sample(200:450,size = 50,replace = T),decreasing = F)+sample(-40:40,50,replace = T)))\n)\n\ndatatable(df3,options = list(pageLength = 100, scrollY = \"200px\"))\n\n\n\n\n\nThe new ITS model looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 + Post.intervention.2.time, data = df,method=“ML”, correlation= corARMA(p=2,q=2, form = ~ Time))\nRemember that you should probably deal with autocorrelation at this point. The method is the same as before, so I won’t reproduce it here. I’m just going to make up some values for p and q\n\nmodel.d = gls(quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 + Post.intervention.2.time, data = df3,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time))\n\n# Show a summary of the model\nsummary(model.d)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 +      Post.intervention.2.time \n  Data: df3 \n       AIC      BIC    logLik\n  1439.229 1472.346 -708.6146\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time \n Parameter estimate(s):\n      Phi1       Phi2     Theta1     Theta2 \n-0.1786700  0.7084663  0.5430931 -0.3129867 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)              2513.1264 18.369456 136.81006   0.000\nTime                      -10.1573  0.597890 -16.98864   0.000\nIntervention             -361.2113 20.878417 -17.30070   0.000\nPost.intervention.time    -18.6064  0.909986 -20.44690   0.000\nIntervention.2            -22.0933 20.889070  -1.05765   0.292\nPost.intervention.2.time   34.1683  0.909986  37.54819   0.000\n\n Correlation: \n                         (Intr) Time   Intrvn Pst.n. Intr.2\nTime                     -0.851                            \nIntervention              0.243 -0.488                     \nPost.intervention.time    0.618 -0.779 -0.010              \nIntervention.2           -0.025  0.051  0.146 -0.342       \nPost.intervention.2.time -0.066  0.135  0.368 -0.577 -0.033\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-3.2828539 -0.6002970  0.1169462  0.6345249  3.0034906 \n\nResidual standard error: 31.75995 \nDegrees of freedom: 150 total; 144 residual\n\n\nReferring back to the earlier, more simple example, you can probably see that these coefficients are easy to explain.\n\nThe intercept is still the initial average value of quantity.x\nTime is the pre-intervention slope\nIntervention describes the step change that occurs at the intervention timepoint\nPost.intervention.time describes the additional slope change that occurs at the intervention timepoint\nIntervention.2 describes the step change that occurs at the timepoint of the second intervention\nPost.intervention.2.time describes the additional slope change that occurs at the timepoint of the second intervention\n\nLet’s grab the estimated modelled values for the new two intervention study\n\ndf3&lt;-df3 %&gt;% mutate(\n  model.d.predictions = predictSE.gls (model.d, df3, se.fit=T)$fit,\n  model.d.se = predictSE.gls (model.d, df3, se.fit=T)$se\n  )\n\nLet’s draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df3,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.d.predictions - (1.96*model.d.se), ymax = model.d.predictions + (1.96*model.d.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.d.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\n\n\n\n\nlet’s calculate the first counterfactual, that there were no interventions\n\ndf4&lt;-filter(df3,Time&lt;51)\nmodel.e = gls(quantity.x ~ Time, data = df4, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf3&lt;-df3 %&gt;% mutate(\n  model.e.predictions = predictSE.gls (model.e, newdata = df3, se.fit=T)$fit,\n  model.e.se = predictSE.gls (model.e, df3, se.fit=T)$se\n)\n\nand then the second counterfactual, that only the first intervention happened\n\ndf5&lt;-filter(df3,Time&lt;101)\nmodel.f = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df5, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf3&lt;-df3 %&gt;% mutate(\n  model.f.predictions = predictSE.gls (model.f, newdata = df3, se.fit=T)$fit,\n  model.f.se = predictSE.gls (model.f, df3, se.fit=T)$se\n)\n\nFinally, let’s draw the chart that shows the factual data (black,green), the first (red, pink) and second (navy, turquoise) counterfactuals\n\n  ggplot(df3,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.f.predictions - (1.96*model.d.se), ymax = model.f.predictions + (1.96*model.e.se)), fill = \"lightblue\")+\n  geom_line(aes(Time,model.f.predictions),color=\"blue\",lty=2)+\n  geom_ribbon(aes(ymin = model.e.predictions - (1.96*model.d.se), ymax = model.e.predictions + (1.96*model.e.se)), fill = \"pink\")+\n  geom_line(aes(Time,model.e.predictions),color=\"red\",lty=2)+\n  geom_ribbon(aes(ymin = model.d.predictions - (1.96*model.d.se), ymax = model.d.predictions + (1.96*model.d.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.d.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\n\n\n\n\nYou can use the stored modelled values to do any calculations you want with regards to the relative differences. Remember that some things can’t ever go below zero and that your hypothesis probably isn’t that there’s a linear trend that continues forever. Think carefully about your counterfactuals in this context.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-three---controlled-its-one-intervention",
    "href": "Interrupted_Time_Series.html#part-three---controlled-its-one-intervention",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.7 Part Three - Controlled ITS, one intervention",
    "text": "13.7 Part Three - Controlled ITS, one intervention\nControlled ITS is about as good as it gets when it comes to time series. The control allows you to calibrate the ITS to account for independent secular and periodic changes. Let’s say that quantity.x and quantity.y are related. Both have exhibited some kind of long-range secular change (for instance there’s the pre-intervention trend in the examples above). Let’s say that both quantity.x and quantity.y are experiencing a parallel trend pre-intervention.\nThe intervention is intented to affect change in quantity.x, but not quantity.y, so post-intervention trends in quantity.y can be taken account for in our model, strenghtening our results.\nLet’s make a data set.\n\n# create a dummy dataframe\n# Here the interruption takes place at week 51\ndf.x&lt;-tibble(\n  x = 1,\n  Time = 1:100,\n  x.Time = x*Time,\n  Intervention = c(rep(0,50),rep(1,50)),\n  x.Intervention = x*Intervention,\n  Post.intervention.time = c(rep(0,50),1:50),\n  x.Post.intervention.time = x * Post.intervention.time,\n  quantity.x = c(sort(sample(200:300,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(20:170,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)))\n)\n\ndf.y&lt;-tibble(\n  x = 0,\n  Time = 1:100,\n  x.Time = x*Time,\n  Intervention = c(rep(0,50),rep(1,50)),\n  x.Intervention = x*Intervention,\n  Post.intervention.time = c(rep(0,50),1:50),\n  x.Post.intervention.time = x * Post.intervention.time,\n  quantity.x = c(sort(sample(500:600,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(280:500,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)))\n)\n\ndf6 = bind_rows(df.x,df.y) %&gt;% \n  arrange(Time,x)\n\ndatatable(df6,options = list(pageLength = 200, scrollY = \"200px\"))\n\n\n\n\n\nLook carefully at the dataset. I’ve introduced the variable x (which differentiates between the control (0) and test (1) groups, plus some new Interaction terms including x.Time, x.Intervention and x.Post.intervention.time. Each of these is just the value of the variable, multiplied by x (which indicates control variables [0, i.e. quantity.y] or variables of interest [1, i.e. quantity.x]). This makes those variables null for the controls and meaninful step or trend changes for the lines of the data relating to quantity.x\nThe ITS model is now a little more complicated. Again, I’ve skipped the step where we test different values of p and q. You should not skip that!\ngls(quantity.x ~ Time + x.Time + Intervention + x.Intervention + Post.intervention.time + x.Post.intervention.time, data = df6, correlation= corARMA(p=1, q=1, form = ~ Time|x),method=“ML”)\nNote that we’ve had to change the form = ~ Time|x) argument in the corARMA to account for the fact that it needs to correct for autocorrelation across time and across the two groups where x == 1 and x == 0\n\nmodel.g = gls(quantity.x ~ x + Time + x.Time + Intervention + x.Intervention + Post.intervention.time + x.Post.intervention.time, data = df6,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time|x))\n\n# Show a summary of the model\nsummary(model.g)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ x + Time + x.Time + Intervention + x.Intervention +      Post.intervention.time + x.Post.intervention.time \n  Data: df6 \n     AIC      BIC    logLik\n  1758.9 1801.778 -866.4499\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n       Phi1        Phi2      Theta1      Theta2 \n 0.10771949 -0.45234784 -0.09574448  0.64057848 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               601.6579  6.138936  98.00687  0.0000\nx                        -295.7286  8.681766 -34.06318  0.0000\nTime                       -1.9366  0.208916  -9.26991  0.0000\nx.Time                     -0.0924  0.295452  -0.31265  0.7549\nIntervention               -2.5040  8.459708  -0.29599  0.7676\nx.Intervention            -32.7129 11.963833  -2.73431  0.0068\nPost.intervention.time     -2.4599  0.297490  -8.26891  0.0000\nx.Post.intervention.time    1.8554  0.420714   4.41023  0.0000\n\n Correlation: \n                         (Intr) x      Time   x.Time Intrvn x.Intr Pst.n.\nx                        -0.707                                          \nTime                     -0.869  0.614                                   \nx.Time                    0.614 -0.869 -0.707                            \nIntervention              0.340 -0.241 -0.593  0.419                     \nx.Intervention           -0.241  0.340  0.419 -0.593 -0.707              \nPost.intervention.time    0.616 -0.435 -0.712  0.503 -0.018  0.012       \nx.Post.intervention.time -0.435  0.616  0.503 -0.712  0.012 -0.018 -0.707\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.5927345 -0.6294354 -0.1224230  0.7533547  2.2872558 \n\nResidual standard error: 18.81718 \nDegrees of freedom: 200 total; 192 residual\n\n\nIf you were a fancy Dan with these things, you could also use interaction terms in your model to achieve the same result\n\nmodel.h = gls(quantity.x ~ Time*x + Intervention*x + Post.intervention.time*x, data = df6,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time|x))\n\n# Show a summary of the model\nsummary(model.h)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time * x + Intervention * x + Post.intervention.time *      x \n  Data: df6 \n     AIC      BIC    logLik\n  1758.9 1801.778 -866.4499\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n       Phi1        Phi2      Theta1      Theta2 \n 0.10771949 -0.45234784 -0.09574448  0.64057848 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               601.6579  6.138936  98.00687  0.0000\nTime                       -1.9366  0.208916  -9.26991  0.0000\nx                        -295.7286  8.681766 -34.06318  0.0000\nIntervention               -2.5040  8.459708  -0.29599  0.7676\nPost.intervention.time     -2.4599  0.297490  -8.26891  0.0000\nTime:x                     -0.0924  0.295452  -0.31265  0.7549\nx:Intervention            -32.7129 11.963833  -2.73431  0.0068\nx:Post.intervention.time    1.8554  0.420714   4.41023  0.0000\n\n Correlation: \n                         (Intr) Time   x      Intrvn Pst.n. Time:x x:Intr\nTime                     -0.869                                          \nx                        -0.707  0.614                                   \nIntervention              0.340 -0.593 -0.241                            \nPost.intervention.time    0.616 -0.712 -0.435 -0.018                     \nTime:x                    0.614 -0.707 -0.869  0.419  0.503              \nx:Intervention           -0.241  0.419  0.340 -0.707  0.012 -0.593       \nx:Post.intervention.time -0.435  0.503  0.616  0.012 -0.707 -0.712 -0.018\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.5927345 -0.6294354 -0.1224230  0.7533547  2.2872558 \n\nResidual standard error: 18.81718 \nDegrees of freedom: 200 total; 192 residual\n\n\nSee, they’re identical! Personally I prefer to have the variables like x.Intervention written out in my datasets in full. Working with interactions confuses me and I also don’t like the way R presents the coefficients out of order.\nSo let’s look at the longhand version in detail\n\ncoefficients(model.g)\n\n             (Intercept)                        x                     Time \n            601.65788853            -295.72858910              -1.93663195 \n                  x.Time             Intervention           x.Intervention \n             -0.09237272              -2.50398019             -32.71285979 \n  Post.intervention.time x.Post.intervention.time \n             -2.45991389               1.85544370 \n\n\nThe interpretation of this is as follows\n\nIntercept is the average value of the control group at the start of the study\nx is the difference between Intercept and the value of quantity.x at the start of the study. To calculate the actual value on the y axis, you’d do 593.4-294.1 = 299.3. When we draw the chart below, you’ll see that the line for quantity.x starts at 299.3 units at week 1.\nTime is the pre-intervention slope for the control group\nx.Time is the difference between Time and the values of quantity.x (see how the control group influences our results!)\nIntervention describes the step change that occurs at the intervention timepoint in the control group\nx.Intervention describes the difference in the step changes that occurs at the intervention timepoint betweent the two groups\nPost.intervention.time describes the slope change that occurs at the intervention timepoint in the control group\nx.Post.intervention.time describes the difference in the slope changes that occurs at the intervention timepoint in the control group\n\nLet’s grab the estimated modelled values for the new controlled intervention study\n\ndf6&lt;-df6 %&gt;% mutate(\n  model.g.predictions = predictSE.gls (model.g, df6, se.fit=T)$fit,\n  model.g.se = predictSE.gls (model.g, df6, se.fit=T)$se\n  )\n\nThen draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df6,aes(Time,quantity.x))+geom_point(color=\"grey\")+\n  geom_ribbon(aes(ymin = model.g.predictions - (1.96*model.g.se), ymax = model.g.predictions + (1.96*model.g.se),fill=factor(x)),alpha=0.4)+\n  geom_line(aes(Time,model.g.predictions,color=factor(x)),lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\n\n\n\n\nSo let’s calculate the counterfactuals for each of these and add the predictions to the data set\n\ndf7&lt;-filter(df6,Time&lt;51)\nmodel.i = gls(quantity.x ~ x + Time + x.Time, data = df7, correlation= corARMA(p=1, q=1, form = ~ Time|x),method=\"ML\")\n\ndf6&lt;-df6 %&gt;% mutate(\n  model.i.predictions = predictSE.gls (model.i, newdata = df6, se.fit=T)$fit,\n  model.i.se = predictSE.gls (model.i, df6, se.fit=T)$se\n)\n\nThen plot the results\n\n  ggplot(df6,aes(Time,quantity.x))+geom_point(color=\"grey\")+\n  geom_ribbon(aes(ymin = model.g.predictions - (1.96*model.g.se), ymax = model.g.predictions + (1.96*model.g.se),fill=factor(x)),alpha=0.4)+\n  geom_ribbon(aes(ymin = model.i.predictions - (1.96*model.i.se), ymax = model.i.predictions + (1.96*model.i.se),fill=factor(x)),alpha=0.2)+\n  geom_line(aes(Time,model.g.predictions,color=factor(x)),lty=1)+\n  geom_line(aes(Time,model.i.predictions,color=factor(x)),lty=2)\n\n\n\n\n\n\n\n\nWe can see that in the group of interest, there’s been a big change in the amount of quantity.x since the intervention, but there’s also been a big change in quantity.y the control group. Is the effect we are seeing in the group of interest just happening because of the decline that is happening in both groups, which could indicate that some extrinsic factor influenced change in both groups. If this were the case (assuming that the control group is NOT affected by the intervention) then we’d be incorrectly attributing the result to the intervetion.\nLooking closely at the tables of coefficients, we can find some clues. Let’s look back at the fully controlled analysis\n\nsummary(model.g)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ x + Time + x.Time + Intervention + x.Intervention +      Post.intervention.time + x.Post.intervention.time \n  Data: df6 \n     AIC      BIC    logLik\n  1758.9 1801.778 -866.4499\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n       Phi1        Phi2      Theta1      Theta2 \n 0.10771949 -0.45234784 -0.09574448  0.64057848 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               601.6579  6.138936  98.00687  0.0000\nx                        -295.7286  8.681766 -34.06318  0.0000\nTime                       -1.9366  0.208916  -9.26991  0.0000\nx.Time                     -0.0924  0.295452  -0.31265  0.7549\nIntervention               -2.5040  8.459708  -0.29599  0.7676\nx.Intervention            -32.7129 11.963833  -2.73431  0.0068\nPost.intervention.time     -2.4599  0.297490  -8.26891  0.0000\nx.Post.intervention.time    1.8554  0.420714   4.41023  0.0000\n\n Correlation: \n                         (Intr) x      Time   x.Time Intrvn x.Intr Pst.n.\nx                        -0.707                                          \nTime                     -0.869  0.614                                   \nx.Time                    0.614 -0.869 -0.707                            \nIntervention              0.340 -0.241 -0.593  0.419                     \nx.Intervention           -0.241  0.340  0.419 -0.593 -0.707              \nPost.intervention.time    0.616 -0.435 -0.712  0.503 -0.018  0.012       \nx.Post.intervention.time -0.435  0.616  0.503 -0.712  0.012 -0.018 -0.707\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.5927345 -0.6294354 -0.1224230  0.7533547  2.2872558 \n\nResidual standard error: 18.81718 \nDegrees of freedom: 200 total; 192 residual\n\n\nWe need to look closely at the numbers here.\nIn this case, there was a pre-intervention decline in the control group (-1.98 units/week). The quantity.x group was also declining, at a very slightly lower rate (-1.94 units/week)\nAt the time of the intervention, the control group exhibited a step change of 12.20 units. The quantity.x group meanwhile went down 40.24 units relative to that, so overall it goes down 28.04 units.\nFinally, post-intervention, the rate of decline in the control group dipped (-2.94 units/week) whilst the quantity.x group actually went down less steeply (-2.94 + 1.92 = -1.02 units/week).\nWe might not care too much about these specific rates and numbers. Mostly we care about the big headline number of how much quantity.x has changed compared to the counterfactual, but do take one last look at the table, where the p-values tell you if each coefficient represents an independently significant change.\nIn this case, Time is significant, indicating that there was a significant change with time prior to the intervention. x.Time is not significant, which means that the pre-intervention trends for quantity.x and the control group are the same. I interpret this to mean that both groups were exhibiting similar trends prior to intervention\nMeanwhile, neither Intervention, nor x.Intervention are significant, so maybe we should interpret this to mean that the intervention did not have any immediate effect in the form of a step change.\nFinally, both Post.intervention.time and x.Post.intervention.time are significant, which suggests that something influenced the trends in both groups, but that the quantity.x group was affected differently to the control. That much is clear from the pictures, but how we interpret it is something to think about.\nIt’s possible that the Intervention affected both groups, which would suggest that this is a badly chosen control.\nIt’s also possible that some extrinsic factor affected both groups around the time that the intervention happened. That’s not good because it leaves us in a position where we can’t determine how much of the effect is due to the intervention and how much is due to the other factors.\nIn this dummy data, the controls are pretty bad because they change a lot. This is why a lot of the challenge of doing time series\nLet’s finally prove what the control is doing (and not doing) by looking at the numbers. At the end of the study, the fully controlled ITS model estimates that there’s a difference between the factual and counterfactual values of quantity.x\n\ndf6$model.g.predictions[200]-df6$model.i.predictions[200]\n\n      200 \n-65.23857 \n\n\nLet’s then compare that estimate to an estimate from the same data, but uncontrolled.\n\nmodel.j = gls(quantity.x ~ Time + Intervention  + Post.intervention.time , data = df.x,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time))\n\ndf.x &lt;- df.x %&gt;% \n  mutate(\n  model.j.predictions = predictSE.gls (model.j, newdata = df.x, se.fit=T)$fit,\n  model.j.se = predictSE.gls (model.j, df.x, se.fit=T)$se  \n  )\n\ndf8&lt;-filter(df.x,Time&lt;51)\n\nmodel.k = gls(quantity.x ~ Time, data = df8, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf.x&lt;-df.x %&gt;% mutate(\n  model.k.predictions = predictSE.gls (model.k, newdata = df.x, se.fit=T)$fit,\n  model.k.se = predictSE.gls (model.k, df.x, se.fit=T)$se\n  )\n\ndf.x$model.j.predictions[100]-df.x$model.k.predictions[100]\n\n      100 \n-66.60213 \n\n\nThere’s not much in it here. In both cases, we estimate a change of around 78 to 79 units in the value of quantity.x Really the controls are there to give you a subjective (eyeballing the charts) and statistical view (looking at the summary tables) of whether you should worry that extrinsic factors might have led you to incorrectly attributing the results to your intervention. In my opinion this is a bit of a silly game because you can’t really ever know whether a control group is being affected by the intervention, the extrinsic factor, or both. If you’ve defined your control group before you start, you might be shooting yourself in the foot by choosing the wrong thing. If you haven’t, you might be cherry-picking a control group simply because it doesn’t change when you, for instance, check it with an uncorrected time series analysis.\nIn the former case, you don’t know what to do with the data, nor know how to interpret the effects. In the latter case, there’s not really a lot of point, because a control group you’ve specifically cherry-picked for lack of any effect won’t contribute much to the model and so has little value in the statistics or analysis.\nControlled ITS is pretty much the gold standard, though personally I don’t think it often adds much to use the control. Some people like to use synthesised controls, which is probably more robust.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "Interrupted_Time_Series.html#final-word",
    "href": "Interrupted_Time_Series.html#final-word",
    "title": "13  Segmented Regression Analysis of Interrupted Time Series",
    "section": "13.8 Final word",
    "text": "13.8 Final word\nYou should now be able to do a fairly robust interrupted time series, using controls if you want and calculating counterfactuals for each part of the model. Why not try modifying the controlled ITS to have two interruptions? Or maybe add some periodic covariates, adding effects for seasons, or specific calendar events, or temperature, humidity, region…",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Segmented Regression Analysis of Interrupted Time Series</span>"
    ]
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html",
    "href": "new_lm_predict_left_predict_right.html",
    "title": "14  New lm Predict Left & Predict Right",
    "section": "",
    "text": "14.1 Make new functions\nFirst we create a pair of new functions which decorate an lm object to the right.\nlm_right &lt;- function(formula,data,...){\n                                      mod &lt;- lm(formula,data)\n                                      class(mod) &lt;- c('lm_right',class(mod))\n                                      mod\n                                      }\nand another that decorates an lm object to the left.\nlm_left &lt;- function(formula,data,...){\n                                        mod &lt;- lm(formula,data)\n                                        class(mod) &lt;- c('lm_left',class(mod))\n                                        mod\n                                        }\nThen create a new function which truncates the data of a model to the right of a defined point\npredictdf.lm_right &lt;- \n    function(model, xseq, se, level){\n                            init_range = range(model$model$x)\n                            xseq &lt;- xseq[xseq &gt;=init_range[1]]\n                            ggplot2:::predictdf.default(model, xseq[-length(xseq)], se, level)\n                                    }\nand a counterpart which does the same, but to the left\npredictdf.lm_left &lt;- \n    function(model, xseq, se, level){\n                            init_range = range(model$model$x)\n                            xseq &lt;- xseq[xseq &lt;=init_range[2]]\n                            ggplot2:::predictdf.default(model, xseq[-length(xseq)], se, level)\n                                    }\nNow we can apply the new functions to a dummy dataset to get a feel for what they do",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>New lm Predict Left & Predict Right</span>"
    ]
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#dummy-data-set",
    "href": "new_lm_predict_left_predict_right.html#dummy-data-set",
    "title": "14  New lm Predict Left & Predict Right",
    "section": "14.2 Dummy Data Set",
    "text": "14.2 Dummy Data Set\nThe dummy data set is a simulated time series where some kind of intervention took place at time point 85. The variable intv shows if the intervention has happened, whilst intv_trend counts the time elapsed since the intervention. Time is the study time from the first point. count is a measurable outcome.\n\nint = 85\nset.seed(42)\n\ndf &lt;- data.frame(\n                count = as.integer(rpois(132, 9) + rnorm(132, 1, 1)),\n                time = 1:132,  \n                at_risk = rep(\n                          c(4305, 4251, 4478, 4535, 4758, 4843, 4893, 4673, 4522, 4454, 4351),\n                          each  = 12\n                             )\n                ) %&gt;% \n  mutate(\n    month = rep(factor(month.name, levels = month.name),11),\n    intv = ifelse(time &gt;= int, 1, 0),\n    intv_trend = c(rep(0, (int - 1)),1:(length(unique(time)) - (int - 1))),\n    lag_count = dplyr::lag(count)\n  )\nhead(df)\n\n  count time at_risk    month intv intv_trend lag_count\n1    14    1    4305  January    0          0        NA\n2    16    2    4305 February    0          0        14\n3     8    3    4305    March    0          0        16\n4    13    4    4305    April    0          0         8\n5     9    5    4305      May    0          0        13\n6     9    6    4305     June    0          0         9",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>New lm Predict Left & Predict Right</span>"
    ]
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#apply-a-model-to-the-data",
    "href": "new_lm_predict_left_predict_right.html#apply-a-model-to-the-data",
    "title": "14  New lm Predict Left & Predict Right",
    "section": "14.3 Apply a model to the data",
    "text": "14.3 Apply a model to the data\n\nfit &lt;- glm(\n          count ~ month + time + intv + intv_trend + log(lag_count) + offset(log(at_risk)),\n          family = \"poisson\",\n          data = df\n          )\n\nsummary(fit)\n\n\nCall:\nglm(formula = count ~ month + time + intv + intv_trend + log(lag_count) + \n    offset(log(at_risk)), family = \"poisson\", data = df)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -5.819498   0.215788 -26.969  &lt; 2e-16 ***\nmonthFebruary  -0.041578   0.144861  -0.287 0.774095    \nmonthMarch     -0.012683   0.144400  -0.088 0.930007    \nmonthApril      0.177167   0.137702   1.287 0.198234    \nmonthMay       -0.008227   0.143182  -0.057 0.954180    \nmonthJune      -0.139175   0.148671  -0.936 0.349209    \nmonthJuly      -0.076264   0.147079  -0.519 0.604092    \nmonthAugust    -0.003828   0.144473  -0.026 0.978864    \nmonthSeptember -0.013810   0.145474  -0.095 0.924368    \nmonthOctober    0.101205   0.141138   0.717 0.473338    \nmonthNovember   0.071865   0.141538   0.508 0.611634    \nmonthDecember   0.166069   0.139051   1.194 0.232358    \ntime           -0.006706   0.001521  -4.409 1.04e-05 ***\nintv            0.123208   0.123167   1.000 0.317149    \nintv_trend      0.013809   0.003776   3.657 0.000255 ***\nlog(lag_count) -0.036098   0.067583  -0.534 0.593249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 175.48  on 130  degrees of freedom\nResidual deviance: 141.17  on 115  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 703.88\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>New lm Predict Left & Predict Right</span>"
    ]
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#predict-extrapolate-data",
    "href": "new_lm_predict_left_predict_right.html#predict-extrapolate-data",
    "title": "14  New lm Predict Left & Predict Right",
    "section": "14.4 Predict / Extrapolate data",
    "text": "14.4 Predict / Extrapolate data\nLet’s split the data in to three ‘phases’ including “Pre-intervention”,“Intervention” and “Post-Intervention”. We’ll then predict the direction of travel on the model using predict\n\ndf$group = rep(c(\"Control\",\"PreIntervention\",\"Intervention\",\"PostIntervention\"), c(30,32, 35,35))\n\n\ndf$predict = c(NA, predict(fit, type=\"response\"))\n\nkable(df[50:70,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\ntime\nat_risk\nmonth\nintv\nintv_trend\nlag_count\ngroup\npredict\n\n\n\n\n50\n10\n50\n4758\nFebruary\n0\n0\n14\nPreIntervention\n8.810524\n\n\n51\n8\n51\n4758\nMarch\n0\n0\n10\nPreIntervention\n9.118286\n\n\n52\n11\n52\n4758\nApril\n0\n0\n8\nPreIntervention\n11.039525\n\n\n53\n7\n53\n4758\nMay\n0\n0\n11\nPreIntervention\n9.005955\n\n\n54\n12\n54\n4758\nJune\n0\n0\n7\nPreIntervention\n7.976886\n\n\n55\n3\n55\n4758\nJuly\n0\n0\n12\nPreIntervention\n8.275475\n\n\n56\n10\n56\n4758\nAugust\n0\n0\n3\nPreIntervention\n9.291217\n\n\n57\n11\n57\n4758\nSeptember\n0\n0\n10\nPreIntervention\n8.748828\n\n\n58\n6\n58\n4758\nOctober\n0\n0\n11\nPreIntervention\n9.716138\n\n\n59\n7\n59\n4758\nNovember\n0\n0\n6\nPreIntervention\n9.579476\n\n\n60\n9\n60\n4758\nDecember\n0\n0\n7\nPreIntervention\n10.397414\n\n\n61\n10\n61\n4843\nJanuary\n0\n0\n9\nPreIntervention\n8.823483\n\n\n62\n14\n62\n4843\nFebruary\n0\n0\n10\nPreIntervention\n8.375655\n\n\n63\n10\n63\n4843\nMarch\n0\n0\n14\nIntervention\n8.460194\n\n\n64\n10\n64\n4843\nApril\n0\n0\n10\nIntervention\n10.284761\n\n\n65\n13\n65\n4843\nMay\n0\n0\n10\nIntervention\n8.487231\n\n\n66\n6\n66\n4843\nJune\n0\n0\n13\nIntervention\n7.326063\n\n\n67\n8\n67\n4843\nJuly\n0\n0\n6\nIntervention\n7.968960\n\n\n68\n14\n68\n4843\nAugust\n0\n0\n8\nIntervention\n8.422441\n\n\n69\n12\n69\n4843\nSeptember\n0\n0\n14\nIntervention\n8.117401\n\n\n70\n6\n70\n4843\nOctober\n0\n0\n12\nIntervention\n9.096444",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>New lm Predict Left & Predict Right</span>"
    ]
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#plot-models",
    "href": "new_lm_predict_left_predict_right.html#plot-models",
    "title": "14  New lm Predict Left & Predict Right",
    "section": "14.5 Plot models",
    "text": "14.5 Plot models\nA strength of this approach is that the new functions act like geoms in a ggplot.\nHere, we plot the pre-intervention trend, its extrapolation to the right (using method “lm_right”). We also show the post-intervention trend, extrapolating it backwards to the left (using method “lm_left”).\n\n  ggplot(data = df, aes(x = time, y = predict)) +\n\n    geom_line() +\n    geom_smooth(data=filter(df,group==\"Control\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"PreIntervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"Intervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"PostIntervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"Intervention\"),method=\"lm_left\", se=TRUE, aes(colour=group),fullrange=TRUE, linetype = \"dashed\",alpha=0.1)+\n    geom_smooth(data=filter(df,group==\"PreIntervention\"),method=\"lm_right\", se=TRUE, aes(colour=group),fullrange=TRUE, linetype = \"dashed\",alpha=0.1)\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nThis is a neat trick, but in most circumstances you can probably make good use of an extrapolation from the truncated data as described in the section on Interrupted Time Series Analysis",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>New lm Predict Left & Predict Right</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html",
    "href": "Diagnostics_Power_Calculation.html",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "",
    "text": "15.1 Background\nFor the findings of a proposed study of diagnostic accuracy to be robust, we require a minimum number of infected and uninfected cases. The total number of samples required to accurately evaluate the diagnostic devices can be calculated using the formulas below.\nThe number \\(\\LARGE n\\) of specimens needed to obtain precision in diagnostic performance estimates is calculated using the formula\n\\(\\LARGE n = \\frac{(1.96+1.28)^2 * (p*(1-p))}{(p-po)^2/m}\\)\nwhere\n\\(\\LARGE p\\) is the expected sensitivity of the novel diagnostic\n\\(\\LARGE p_{0}\\) is the minimum acceptable sensitivity of the novel diagnostic\n\\(\\LARGE m\\) is the estimated prevalence of infection/disease/condition/state in the population\nWhen \\(\\LARGE n\\) is chosen this way, you can design the test to ensure that the lower limit of the confidence interval for the estimate of sensitivity/specificity is not likely to exceed \\(\\LARGE p0\\).\nThis is based on Banoo, S. et al. Evaluation of diagnostic tests for infectious diseases: general principles. Nature Reviews Microbiology 4, S20–S32 (2006).",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#libraries",
    "href": "Diagnostics_Power_Calculation.html#libraries",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "15.2 Libraries",
    "text": "15.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#define-a-function-that-applies-the-formula",
    "href": "Diagnostics_Power_Calculation.html#define-a-function-that-applies-the-formula",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "15.3 Define a function that applies the formula",
    "text": "15.3 Define a function that applies the formula\n\nrequired_specimens_min_acceptable&lt;-function(p,po,m)(((1.96+1.28)^2)*(p*(1-p)))/((p-po)^2)/m",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#create-a-data-set-with-likely-values",
    "href": "Diagnostics_Power_Calculation.html#create-a-data-set-with-likely-values",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "15.4 Create a data set with likely values",
    "text": "15.4 Create a data set with likely values\n\ndf&lt;- expand_grid(\n                p = seq(0.80,0.99,0.01),\n                m = seq(0.02,0.16,0.02)\n                )",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#apply-the-function",
    "href": "Diagnostics_Power_Calculation.html#apply-the-function",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "15.5 Apply the function",
    "text": "15.5 Apply the function\n\n15.5.1 At 1%, 5%, 10% and 15% tolerance in lower limit precision in estimate\n\ndf.5pc&lt;- df %&gt;% \n  mutate(\n        po = p-0.05,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"five_percent\"\n        )\n\ndf.10pc&lt;- df %&gt;% \n  mutate(\n        po = p-0.10,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"ten_percent\"\n        )\n\ndf.15pc&lt;- df %&gt;% \n  mutate(\n        po = p-0.15,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"fifteen_percent\"\n        )\n\ndf&lt;-bind_rows(df.5pc,df.10pc,df.15pc) %&gt;% \n  mutate(lower = factor(lower,levels=c(\"five_percent\",\"ten_percent\",\"fifteen_percent\")))\nrm(df.5pc,df.10pc,df.15pc)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#draw-chart",
    "href": "Diagnostics_Power_Calculation.html#draw-chart",
    "title": "15  Diagnostic Evaluation Power Calculation",
    "section": "15.6 Draw chart",
    "text": "15.6 Draw chart\nThis shows the values of \\(\\LARGE n\\) (y axis) for various values of \\(\\LARGE p\\)\nColoured lines show different underlying prevalence values and facets show different acceptable levels of precision in the estimate, here 5%, 10% and 15%, indicating that for a given value of \\(\\LARGE p\\) such as 0.8, a precision as low as 0.75, 0.7 or 0.65 would be minimally acceptable.\nThe ggplotly view allows you to explore results visually.\n\nggplotly(ggplot(df,aes(p,n,color=factor(m)))+geom_line()+facet_wrap(.~lower))",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Diagnostic Evaluation Power Calculation</span>"
    ]
  },
  {
    "objectID": "CI95_point_estimate.html",
    "href": "CI95_point_estimate.html",
    "title": "16  95% Confidence Intervals on a point estimate",
    "section": "",
    "text": "16.1 Find the 95% CI on a point estimate\nThis simple approach is based on the formula\n\\(\\LARGE p ± Z * \\sqrt\\frac{p(1-p)}{n}\\)\nWhere\n\\(\\LARGE p\\) = point estimate\n\\(\\LARGE Z\\) = Zcrit value for 95% confidence level (i.e. 1.96 for a 95% confidence interval)\n\\(\\LARGE n\\) = sample size",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>95% Confidence Intervals on a point estimate</span>"
    ]
  },
  {
    "objectID": "CI95_point_estimate.html#libraries",
    "href": "CI95_point_estimate.html#libraries",
    "title": "16  95% Confidence Intervals on a point estimate",
    "section": "16.2 Libraries",
    "text": "16.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\nDefine Some data\n\ndf = tibble(\n            month = 1:10,\n            prevalence = c(0.72,0.62,0.44,0.22,0.17,0.12,0.13,0.09,0.04,0.02)\n            )\n\nkable(df)\n\n\n\n\nmonth\nprevalence\n\n\n\n\n1\n0.72\n\n\n2\n0.62\n\n\n3\n0.44\n\n\n4\n0.22\n\n\n5\n0.17\n\n\n6\n0.12\n\n\n7\n0.13\n\n\n8\n0.09\n\n\n9\n0.04\n\n\n10\n0.02",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>95% Confidence Intervals on a point estimate</span>"
    ]
  },
  {
    "objectID": "CI95_point_estimate.html#define-a-function-to-calculate-upper-and-lower-confidence-interval",
    "href": "CI95_point_estimate.html#define-a-function-to-calculate-upper-and-lower-confidence-interval",
    "title": "16  95% Confidence Intervals on a point estimate",
    "section": "16.3 Define a function to calculate upper and lower confidence interval",
    "text": "16.3 Define a function to calculate upper and lower confidence interval\n\npoint.estimate.CI &lt;- function(p,z=1.96,n){z * sqrt((p*(1-p))/n)}\n\n\n16.3.1 Capture the upper and lower limit for a given value of n\n\ndf&lt;-df %&gt;% \n  mutate(\n        upper10 = prevalence + point.estimate.CI(prevalence,n = 10),\n        lower10 = prevalence - point.estimate.CI(prevalence,n = 10),\n        upper50 = prevalence + point.estimate.CI(prevalence,n = 50),\n        lower50 = prevalence - point.estimate.CI(prevalence,n = 50),\n        upper1000 = prevalence + point.estimate.CI(prevalence,n = 1000),\n        lower1000 = prevalence - point.estimate.CI(prevalence,n = 1000)\n        ) \n\n\n\n16.3.2 Draw the confindence intervals\nThis chart shows the point estimates (black dots) as well as the 95% confidence intervals obtained when n was 10 (green ribbon), 50 (red ribbon) or 1000 (blue ribbon)\n\nggplot(df,aes(month,prevalence))+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower10,ymax=upper10),alpha=0.4,fill=\"green\")+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower50,ymax=upper50),alpha=0.6,fill=\"red\")+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower1000,ymax=upper1000),alpha=0.6,fill=\"blue\")+\n  geom_point()+\n  geom_line()",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>95% Confidence Intervals on a point estimate</span>"
    ]
  },
  {
    "objectID": "hazratio_chart.html",
    "href": "hazratio_chart.html",
    "title": "17  Hazard Ratio Chart from Summary Data",
    "section": "",
    "text": "17.1 Background\nOften a user just wants to draw a quick chart of study results using only summary data. For instance, you may be looking at a table of hazard ratios and their confidence intervals and would like to visualise the results instead of working with the table.\nThis short script can take a vector of estimates, along with 95% CIs and will draw a chart.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hazard Ratio Chart from Summary Data</span>"
    ]
  },
  {
    "objectID": "hazratio_chart.html#libraries",
    "href": "hazratio_chart.html#libraries",
    "title": "17  Hazard Ratio Chart from Summary Data",
    "section": "17.2 Libraries",
    "text": "17.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hazard Ratio Chart from Summary Data</span>"
    ]
  },
  {
    "objectID": "hazratio_chart.html#dummy-data",
    "href": "hazratio_chart.html#dummy-data",
    "title": "17  Hazard Ratio Chart from Summary Data",
    "section": "17.3 Dummy Data",
    "text": "17.3 Dummy Data\nStart by building a data frame using tibble. You should provide some labels (here IMD), along with an estimate (ABSOLUTE.CHANGE) and both lower (LCI) and upper (UCL) confidence limits\n\ndf &lt;- tibble( \n            IMD = c(\"IMD1\",\"IMD2\",\"IMD3\",\"IMD4\",\"IMD5\"),\n            ABSOLUTE.CHANGE  = c(-3.1,-6.6,-1.6,-2.0,-0.9),\n            LCI = c(-0.4,-4.4,0.49,-1.06,-0.2),\n            UCI = c(-5.7,-8.8,-3.62,-2.9,-1.5)\n)\n\ndf\n\n# A tibble: 5 × 4\n  IMD   ABSOLUTE.CHANGE   LCI   UCI\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 IMD1             -3.1 -0.4  -5.7 \n2 IMD2             -6.6 -4.4  -8.8 \n3 IMD3             -1.6  0.49 -3.62\n4 IMD4             -2   -1.06 -2.9 \n5 IMD5             -0.9 -0.2  -1.5",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hazard Ratio Chart from Summary Data</span>"
    ]
  },
  {
    "objectID": "hazratio_chart.html#chart",
    "href": "hazratio_chart.html#chart",
    "title": "17  Hazard Ratio Chart from Summary Data",
    "section": "17.4 Chart",
    "text": "17.4 Chart\nThere’s nothing clever about this chart. It uses geom_point to draw the estimates, then uses geom_errorbar to add the upper and lower confidence limits.\n\nggplot(df,aes(x=IMD,y=ABSOLUTE.CHANGE,colour=IMD))+\n       geom_errorbar( mapping=aes(x=IMD, y=ABSOLUTE.CHANGE, ymin=LCI, ymax=UCI), width=0.1, linewidth=1)+\n       geom_point(size=5)+\n       geom_hline(yintercept = 0,lty=2)  +\n       ylim (-10,10)\n\n\n\n\n\n\n\n\nIf you have some point estimates and know the sample size, but don’t have confidence intervals, then you can calculate the confidence intervals using [this method](../examples/95CI_point_estimate.qmd)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Hazard Ratio Chart from Summary Data</span>"
    ]
  },
  {
    "objectID": "power_calc_gen_case_control.html",
    "href": "power_calc_gen_case_control.html",
    "title": "18  Genetic Power Calculator",
    "section": "",
    "text": "18.1 Background\nThese power calculations are useful for genetic case control studies They use the gap package and pbsize2 command\ncitation Jing Hua Zhao (2013). gap: Genetic Analysis Package. R package version 1.1-10",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Genetic Power Calculator</span>"
    ]
  },
  {
    "objectID": "power_calc_gen_case_control.html#libraries",
    "href": "power_calc_gen_case_control.html#libraries",
    "title": "18  Genetic Power Calculator",
    "section": "18.2 Libraries",
    "text": "18.2 Libraries\n\nlibrary(gap)\n\nLoading required package: gap.datasets\n\n\ngap version 1.6\n\nlibrary(knitr)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Genetic Power Calculator</span>"
    ]
  },
  {
    "objectID": "power_calc_gen_case_control.html#usage",
    "href": "power_calc_gen_case_control.html#usage",
    "title": "18  Genetic Power Calculator",
    "section": "18.3 Usage",
    "text": "18.3 Usage\n\n18.3.1 Define study characteristics\n\nprevalence=0.1\nalpha=0.05\nnum.cases=2000\nnum.controls=2000\nnum.total=num.cases+num.controls\nprop.cases=num.cases/num.total\n\n\n\n18.3.2 Define ranges of minor allele frequency and effect sizes\n\nmaf_values&lt;-seq(0.01,0.49,by=0.01)\nrisk_values&lt;-seq(1,1.6,by=0.01)\n\n\n\n18.3.3 Calculate power at all possible combinations\n\nx&lt;-matrix(nrow=length(risk_values),ncol=length(maf_values))\n\nfor (i in 1:length(risk_values)){risk = risk_values[i];for (j in 1:length(maf_values)){maf = maf_values[j];try(x[i,j]&lt;-pbsize2(N=num.total,fc=prop.cases,alpha=alpha,gamma=risk,p=maf,kp=prevalence,model=\"additive\"))}}\n\n\n\n18.3.4 Define a matrix which contains the results\n\nxx&lt;-as.data.frame(x,row.names=as.character(risk_values));names(xx)&lt;-as.character(maf_values)\n\nkable(xx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\n0.38\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n\n\n\n\n1\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n\n\n1.01\n0.0501394\n0.0502758\n0.0504094\n0.0505402\n0.0506680\n0.0507930\n0.0509151\n0.0510343\n0.0511507\n0.0512642\n0.0513748\n0.0514826\n0.0515875\n0.0516896\n0.0517887\n0.0518851\n0.0519786\n0.0520692\n0.0521569\n0.0522418\n0.0523239\n0.0524031\n0.0524795\n0.0525530\n0.0526237\n0.0526916\n0.0527566\n0.0528187\n0.0528781\n0.0529346\n0.0529882\n0.0530391\n0.0530871\n0.0531323\n0.0531747\n0.0532142\n0.0532509\n0.0532849\n0.0533160\n0.0533443\n0.0533698\n0.0533924\n0.0534123\n0.0534294\n0.0534437\n0.0534551\n0.0534638\n0.0534697\n0.0534728\n\n\n1.02\n0.0505549\n0.0510983\n0.0516302\n0.0521506\n0.0526594\n0.0531566\n0.0536424\n0.0541166\n0.0545792\n0.0550304\n0.0554700\n0.0558980\n0.0563146\n0.0567196\n0.0571131\n0.0574951\n0.0578655\n0.0582245\n0.0585720\n0.0589080\n0.0592325\n0.0595455\n0.0598471\n0.0601372\n0.0604159\n0.0606832\n0.0609390\n0.0611834\n0.0614165\n0.0616381\n0.0618483\n0.0620472\n0.0622348\n0.0624110\n0.0625759\n0.0627295\n0.0628718\n0.0630028\n0.0631226\n0.0632312\n0.0633285\n0.0634146\n0.0634895\n0.0635532\n0.0636058\n0.0636473\n0.0636776\n0.0636968\n0.0637050\n\n\n1.03\n0.0512433\n0.0524609\n0.0536530\n0.0548193\n0.0559599\n0.0570746\n0.0581635\n0.0592265\n0.0602636\n0.0612747\n0.0622599\n0.0632190\n0.0641521\n0.0650591\n0.0659401\n0.0667951\n0.0676240\n0.0684268\n0.0692036\n0.0699543\n0.0706789\n0.0713776\n0.0720502\n0.0726969\n0.0733176\n0.0739123\n0.0744811\n0.0750240\n0.0755410\n0.0760322\n0.0764976\n0.0769372\n0.0773511\n0.0777394\n0.0781020\n0.0784390\n0.0787504\n0.0790364\n0.0792969\n0.0795320\n0.0797417\n0.0799262\n0.0800854\n0.0802195\n0.0803285\n0.0804124\n0.0804713\n0.0805052\n0.0805144\n\n\n1.04\n0.0522012\n0.0543582\n0.0564707\n0.0585383\n0.0605609\n0.0625382\n0.0644700\n0.0663560\n0.0681962\n0.0699903\n0.0717382\n0.0734398\n0.0750949\n0.0767036\n0.0782656\n0.0797810\n0.0812497\n0.0826717\n0.0840469\n0.0853752\n0.0866568\n0.0878917\n0.0890797\n0.0902210\n0.0913156\n0.0923635\n0.0933648\n0.0943196\n0.0952279\n0.0960897\n0.0969052\n0.0976744\n0.0983974\n0.0990744\n0.0997053\n0.1002904\n0.1008298\n0.1013234\n0.1017716\n0.1021743\n0.1025317\n0.1028440\n0.1031113\n0.1033337\n0.1035114\n0.1036444\n0.1037331\n0.1037774\n0.1037776\n\n\n1.05\n0.0534259\n0.0567856\n0.0600782\n0.0633027\n0.0664583\n0.0695444\n0.0725602\n0.0755051\n0.0783785\n0.0811802\n0.0839095\n0.0865661\n0.0891497\n0.0916600\n0.0940969\n0.0964600\n0.0987492\n0.1009645\n0.1031057\n0.1051728\n0.1071657\n0.1090844\n0.1109290\n0.1126995\n0.1143960\n0.1160185\n0.1175672\n0.1190422\n0.1204436\n0.1217716\n0.1230263\n0.1242080\n0.1253168\n0.1263529\n0.1273165\n0.1282079\n0.1290273\n0.1297749\n0.1304510\n0.1310558\n0.1315896\n0.1320527\n0.1324453\n0.1327677\n0.1330203\n0.1332032\n0.1333168\n0.1333614\n0.1333372\n\n\n1.06\n0.0549148\n0.0597397\n0.0644721\n0.0691099\n0.0736510\n0.0780937\n0.0824362\n0.0866773\n0.0908155\n0.0948499\n0.0987794\n0.1026031\n0.1063204\n0.1099307\n0.1134334\n0.1168282\n0.1201146\n0.1232926\n0.1263620\n0.1293225\n0.1321743\n0.1349173\n0.1375517\n0.1400775\n0.1424949\n0.1448042\n0.1470055\n0.1490993\n0.1510857\n0.1529652\n0.1547380\n0.1564046\n0.1579654\n0.1594208\n0.1607712\n0.1620171\n0.1631589\n0.1641971\n0.1651322\n0.1659646\n0.1666949\n0.1673235\n0.1678510\n0.1682778\n0.1686045\n0.1688316\n0.1689595\n0.1689889\n0.1689201\n\n\n1.07\n0.0566656\n0.0632174\n0.0696502\n0.0759589\n0.0821393\n0.0881877\n0.0941006\n0.0998752\n0.1055088\n0.1109993\n0.1163447\n0.1215436\n0.1265945\n0.1314963\n0.1362483\n0.1408497\n0.1453001\n0.1495991\n0.1537466\n0.1577426\n0.1615871\n0.1652803\n0.1688226\n0.1722142\n0.1754558\n0.1785477\n0.1814906\n0.1842851\n0.1869319\n0.1894316\n0.1917852\n0.1939932\n0.1960566\n0.1979761\n0.1997525\n0.2013868\n0.2028797\n0.2042321\n0.2054449\n0.2065189\n0.2074550\n0.2082540\n0.2089168\n0.2094442\n0.2098370\n0.2100962\n0.2102224\n0.2102166\n0.2100794\n\n\n1.08\n0.0586762\n0.0672167\n0.0756109\n0.0838493\n0.0919236\n0.0998267\n0.1075524\n0.1150951\n0.1224505\n0.1296145\n0.1365840\n0.1433562\n0.1499292\n0.1563012\n0.1624709\n0.1684376\n0.1742007\n0.1797600\n0.1851154\n0.1902674\n0.1952163\n0.1999629\n0.2045079\n0.2088523\n0.2129971\n0.2169435\n0.2206928\n0.2242462\n0.2276052\n0.2307711\n0.2337453\n0.2365294\n0.2391248\n0.2415330\n0.2437555\n0.2457938\n0.2476493\n0.2493235\n0.2508178\n0.2521337\n0.2532725\n0.2542357\n0.2550244\n0.2556402\n0.2560842\n0.2563577\n0.2564618\n0.2563979\n0.2561670\n\n\n1.09\n0.0609447\n0.0717356\n0.0823531\n0.0927803\n0.1030025\n0.1130072\n0.1227836\n0.1323228\n0.1416172\n0.1506607\n0.1594481\n0.1679757\n0.1762403\n0.1842398\n0.1919728\n0.1994385\n0.2066365\n0.2135673\n0.2202314\n0.2266299\n0.2327642\n0.2386358\n0.2442467\n0.2495988\n0.2546944\n0.2595358\n0.2641253\n0.2684655\n0.2725589\n0.2764080\n0.2800153\n0.2833835\n0.2865152\n0.2894127\n0.2920786\n0.2945154\n0.2967254\n0.2987109\n0.3004743\n0.3020178\n0.3033434\n0.3044532\n0.3053493\n0.3060334\n0.3065075\n0.3067734\n0.3068326\n0.3066869\n0.3063377\n\n\n1.1\n0.0634695\n0.0767725\n0.0898755\n0.1027499\n0.1153713\n0.1277191\n0.1397763\n0.1515287\n0.1629649\n0.1740757\n0.1848540\n0.1952944\n0.2053934\n0.2151485\n0.2245584\n0.2336231\n0.2423432\n0.2507201\n0.2587559\n0.2664532\n0.2738149\n0.2808446\n0.2875457\n0.2939223\n0.2999783\n0.3057180\n0.3111456\n0.3162653\n0.3210815\n0.3255985\n0.3298204\n0.3337516\n0.3373960\n0.3407576\n0.3438405\n0.3466482\n0.3491845\n0.3514529\n0.3534568\n0.3551993\n0.3566836\n0.3579125\n0.3588890\n0.3596155\n0.3600946\n0.3603287\n0.3603198\n0.3600702\n0.3595817\n\n\n1.11\n0.0662489\n0.0823259\n0.0981762\n0.1137539\n0.1290205\n0.1439443\n0.1584994\n0.1726649\n0.1864244\n0.1997654\n0.2126787\n0.2251578\n0.2371987\n0.2487996\n0.2599605\n0.2706827\n0.2809689\n0.2908228\n0.3002490\n0.3092526\n0.3178395\n0.3260159\n0.3337882\n0.3411633\n0.3481479\n0.3547491\n0.3609738\n0.3668288\n0.3723210\n0.3774572\n0.3822438\n0.3866873\n0.3907937\n0.3945692\n0.3980193\n0.4011496\n0.4039654\n0.4064715\n0.4086729\n0.4105738\n0.4121786\n0.4134912\n0.4145154\n0.4152544\n0.4157116\n0.4158900\n0.4157921\n0.4154206\n0.4147777\n\n\n1.12\n0.0692815\n0.0883940\n0.1072521\n0.1257848\n0.1439344\n0.1616540\n0.1789061\n0.1956617\n0.2118987\n0.2276008\n0.2427572\n0.2573610\n0.2714092\n0.2849018\n0.2978410\n0.3102315\n0.3220793\n0.3333921\n0.3441784\n0.3544476\n0.3642096\n0.3734749\n0.3822541\n0.3905581\n0.3983976\n0.4057835\n0.4127264\n0.4192366\n0.4253244\n0.4309996\n0.4362718\n0.4411501\n0.4456434\n0.4497600\n0.4535079\n0.4568947\n0.4599275\n0.4626132\n0.4649579\n0.4669677\n0.4686480\n0.4700038\n0.4710400\n0.4717607\n0.4721700\n0.4722713\n0.4720679\n0.4715625\n0.4707578\n\n\n1.13\n0.0725659\n0.0949751\n0.1170986\n0.1388317\n0.1600902\n0.1808077\n0.2009328\n0.2204265\n0.2392608\n0.2574164\n0.2748815\n0.2916506\n0.3077232\n0.3231030\n0.3377969\n0.3518148\n0.3651682\n0.3778706\n0.3899366\n0.4013815\n0.4122212\n0.4224721\n0.4321504\n0.4412723\n0.4498540\n0.4579112\n0.4654592\n0.4725128\n0.4790865\n0.4851940\n0.4908484\n0.4960624\n0.5008476\n0.5052156\n0.5091767\n0.5127411\n0.5159179\n0.5187159\n0.5211431\n0.5232071\n0.5249145\n0.5262717\n0.5272844\n0.5279576\n0.5282961\n0.5283039\n0.5279845\n0.5273410\n0.5263761\n\n\n1.14\n0.0761009\n0.1020668\n0.1277094\n0.1528786\n0.1774564\n0.2013520\n0.2244972\n0.2468425\n0.2683541\n0.2890110\n0.3088028\n0.3277274\n0.3457894\n0.3629992\n0.3793711\n0.3949226\n0.4096738\n0.4236464\n0.4368632\n0.4493479\n0.4611242\n0.4722160\n0.4826469\n0.4924401\n0.5016182\n0.5102030\n0.5182156\n0.5256761\n0.5326038\n0.5390170\n0.5449330\n0.5503682\n0.5553378\n0.5598564\n0.5639374\n0.5675933\n0.5708357\n0.5736753\n0.5761219\n0.5781846\n0.5798715\n0.5811900\n0.5821467\n0.5827475\n0.5829976\n0.5829014\n0.5824628\n0.5816849\n0.5805704\n\n\n1.15\n0.0798850\n0.1096662\n0.1390758\n0.1679049\n0.1959929\n0.2232197\n0.2494984\n0.2747688\n0.2989930\n0.3221507\n0.3442357\n0.3652529\n0.3852160\n0.4041452\n0.4220659\n0.4390068\n0.4549994\n0.4700767\n0.4842723\n0.4976204\n0.5101549\n0.5219092\n0.5329156\n0.5432058\n0.5528100\n0.5617572\n0.5700749\n0.5777895\n0.5849256\n0.5915066\n0.5975542\n0.6030890\n0.6081299\n0.6126948\n0.6167999\n0.6204604\n0.6236902\n0.6265019\n0.6289072\n0.6309164\n0.6325391\n0.6337835\n0.6346571\n0.6351663\n0.6353169\n0.6351134\n0.6345598\n0.6336592\n0.6324138\n\n\n1.16\n0.0839170\n0.1177696\n0.1511866\n0.1838840\n0.2156497\n0.2463297\n0.2758169\n0.3040421\n0.3309658\n0.3565724\n0.3808643\n0.4038582\n0.4255813\n0.4460685\n0.4653603\n0.4835005\n0.5005357\n0.5165130\n0.5314800\n0.5454837\n0.5585701\n0.5707838\n0.5821678\n0.5927630\n0.6026084\n0.6117410\n0.6201956\n0.6280048\n0.6351992\n0.6418074\n0.6478559\n0.6533693\n0.6583704\n0.6628801\n0.6669176\n0.6705005\n0.6736448\n0.6763652\n0.6786745\n0.6805847\n0.6821061\n0.6832479\n0.6840182\n0.6844238\n0.6844706\n0.6841634\n0.6835058\n0.6825008\n0.6811502\n\n\n1.17\n0.0881956\n0.1263727\n0.1640275\n0.2007837\n0.2363675\n0.2705871\n0.3033160\n0.3344783\n0.3640380\n0.3919894\n0.4183498\n0.4431534\n0.4664467\n0.4882846\n0.5087274\n0.5278387\n0.5456834\n0.5623265\n0.5778320\n0.5922622\n0.6056771\n0.6181340\n0.6296874\n0.6403889\n0.6502868\n0.6594265\n0.6678502\n0.6755974\n0.6827046\n0.6892054\n0.6951310\n0.7005100\n0.7053685\n0.7097306\n0.7136180\n0.7170506\n0.7200462\n0.7226208\n0.7247889\n0.7265632\n0.7279550\n0.7289738\n0.7296283\n0.7299253\n0.7298709\n0.7294695\n0.7287246\n0.7276388\n0.7262133\n\n\n1.18\n0.0927193\n0.1354700\n0.1775816\n0.2185655\n0.2580770\n0.2958842\n0.3318428\n0.3658757\n0.3979570\n0.4280984\n0.4563395\n0.4827392\n0.5073700\n0.5303127\n0.5516528\n0.5714780\n0.5898757\n0.6069315\n0.6227285\n0.6373463\n0.6508603\n0.6633418\n0.6748578\n0.6854707\n0.6952386\n0.7042154\n0.7124508\n0.7199908\n0.7268774\n0.7331492\n0.7388416\n0.7439867\n0.7486139\n0.7527496\n0.7564178\n0.7596402\n0.7624360\n0.7648225\n0.7668149\n0.7684268\n0.7696697\n0.7705537\n0.7710873\n0.7712775\n0.7711300\n0.7706491\n0.7698378\n0.7686979\n0.7672302\n\n\n1.19\n0.0974867\n0.1450549\n0.1918283\n0.2371848\n0.2806999\n0.3221009\n0.3612308\n0.3980188\n0.4324578\n0.4645866\n0.4944759\n0.5222179\n0.5479185\n0.5716908\n0.5936510\n0.6139152\n0.6325967\n0.6498049\n0.6656439\n0.6802120\n0.6936012\n0.7058975\n0.7171804\n0.7275233\n0.7369940\n0.7456546\n0.7535617\n0.7607673\n0.7673185\n0.7732584\n0.7786256\n0.7834556\n0.7877799\n0.7916271\n0.7950228\n0.7979900\n0.8005488\n0.8027173\n0.8045113\n0.8059444\n0.8070286\n0.8077738\n0.8081884\n0.8082792\n0.8080515\n0.8075089\n0.8066542\n0.8054883\n0.8040112\n\n\n1.2\n0.1024962\n0.1551198\n0.2067440\n0.2565910\n0.3041490\n0.3491070\n0.3913026\n0.4306820\n0.4672685\n0.5011391\n0.5324063\n0.5612050\n0.5876821\n0.6119899\n0.6342803\n0.6547020\n0.6733974\n0.6905018\n0.7061417\n0.7204351\n0.7334911\n0.7454102\n0.7562842\n0.7661973\n0.7752260\n0.7834399\n0.7909020\n0.7976692\n0.8037931\n0.8093199\n0.8142914\n0.8187450\n0.8227142\n0.8262289\n0.8293155\n0.8319978\n0.8342965\n0.8362299\n0.8378138\n0.8390618\n0.8399857\n0.8405952\n0.8408984\n0.8409014\n0.8406090\n0.8400245\n0.8391498\n0.8379851\n0.8365296\n\n\n1.21\n0.1077461\n0.1656557\n0.2223016\n0.2767274\n0.3283292\n0.3767630\n0.4218725\n0.4636342\n0.5021162\n0.5374468\n0.5697920\n0.5993391\n0.6262843\n0.6508252\n0.6731546\n0.6934570\n0.7119065\n0.7286653\n0.7438835\n0.7576987\n0.7702367\n0.7816119\n0.7919279\n0.8012783\n0.8097476\n0.8174115\n0.8243382\n0.8305889\n0.8362181\n0.8412746\n0.8458020\n0.8498391\n0.8534204\n0.8565762\n0.8593335\n0.8617161\n0.8637447\n0.8654373\n0.8668095\n0.8678745\n0.8686437\n0.8691261\n0.8693293\n0.8692590\n0.8689192\n0.8683125\n0.8674400\n0.8663015\n0.8648952\n\n\n1.22\n0.1132344\n0.1766524\n0.2384707\n0.2975320\n0.3531385\n0.4049228\n0.4527499\n0.4966438\n0.5367335\n0.5732136\n0.6063161\n0.6362908\n0.6633912\n0.6878658\n0.7099522\n0.7298735\n0.7478367\n0.7640318\n0.7786318\n0.7917937\n0.8036587\n0.8143541\n0.8239936\n0.8326791\n0.8405012\n0.8475407\n0.8538697\n0.8595519\n0.8646440\n0.8691962\n0.8732529\n0.8768532\n0.8800318\n0.8828191\n0.8852416\n0.8873228\n0.8890827\n0.8905388\n0.8917061\n0.8925971\n0.8932224\n0.8935904\n0.8937079\n0.8935799\n0.8932098\n0.8925994\n0.8917492\n0.8906583\n0.8893241\n\n\n1.23\n0.1189591\n0.1880984\n0.2552176\n0.3189377\n0.3784692\n0.4334357\n0.4837429\n0.5294827\n0.5708637\n0.6081628\n0.6416910\n0.6717702\n0.6987185\n0.7228402\n0.7444206\n0.7637228\n0.7809870\n0.7964304\n0.8102481\n0.8226146\n0.8336853\n0.8435981\n0.8524754\n0.8604251\n0.8675426\n0.8739122\n0.8796080\n0.8846951\n0.8892312\n0.8932664\n0.8968452\n0.9000063\n0.9027836\n0.9052069\n0.9073017\n0.9090906\n0.9105927\n0.9118245\n0.9128000\n0.9135309\n0.9140268\n0.9142954\n0.9143426\n0.9141726\n0.9137882\n0.9131906\n0.9123795\n0.9113532\n0.9101089\n\n\n1.24\n0.1249179\n0.1999805\n0.2725053\n0.3408728\n0.4042090\n0.4621484\n0.5146613\n0.5619309\n0.6042662\n0.6420430\n0.6756638\n0.7055322\n0.7320364\n0.7555404\n0.7763786\n0.7948543\n0.8112401\n0.8257788\n0.8386856\n0.8501504\n0.8603402\n0.8694016\n0.8774631\n0.8846369\n0.8910213\n0.8967020\n0.9017537\n0.9062419\n0.9102234\n0.9137479\n0.9168585\n0.9195927\n0.9219833\n0.9240584\n0.9258426\n0.9273567\n0.9286187\n0.9296440\n0.9304453\n0.9310332\n0.9314163\n0.9316015\n0.9315938\n0.9313967\n0.9310121\n0.9304407\n0.9296816\n0.9287327\n0.9275907\n\n\n1.25\n0.1311085\n0.2122844\n0.2902939\n0.3632618\n0.4302427\n0.4909075\n0.5453199\n0.5937806\n0.6367209\n0.6746322\n0.7080207\n0.7373793\n0.7631712\n0.7858216\n0.8057144\n0.8231919\n0.8385568\n0.8520750\n0.8639791\n0.8744715\n0.8837282\n0.8919017\n0.8991242\n0.9055101\n0.9111585\n0.9161548\n0.9205732\n0.9244776\n0.9279233\n0.9309581\n0.9336235\n0.9359549\n0.9379831\n0.9397347\n0.9412321\n0.9424949\n0.9435393\n0.9443794\n0.9450265\n0.9454901\n0.9457779\n0.9458955\n0.9458473\n0.9456360\n0.9452629\n0.9447281\n0.9440303\n0.9431669\n0.9421342\n\n\n1.26\n0.1375280\n0.2249943\n0.3085409\n0.3860260\n0.4564533\n0.5195620\n0.5755418\n0.6248398\n0.6680318\n0.7057415\n0.7385893\n0.7671636\n0.7920051\n0.8136006\n0.8323818\n0.8487274\n0.8629672\n0.8753868\n0.8862325\n0.8957159\n0.9040190\n0.9112974\n0.9176846\n0.9232949\n0.9282262\n0.9325623\n0.9363750\n0.9397259\n0.9426676\n0.9452455\n0.9474982\n0.9494590\n0.9511563\n0.9526143\n0.9538536\n0.9548919\n0.9557438\n0.9564216\n0.9569356\n0.9572940\n0.9575031\n0.9575681\n0.9574922\n0.9572776\n0.9569250\n0.9564341\n0.9558031\n0.9550292\n0.9541084\n\n\n1.27\n0.1441736\n0.2380932\n0.3272010\n0.4090844\n0.4827238\n0.5479651\n0.6051609\n0.6549347\n0.6980299\n0.7352170\n0.7672397\n0.7947857\n0.8184740\n0.8388513\n0.8563938\n0.8715128\n0.8845611\n0.8958396\n0.9056046\n0.9140734\n0.9214304\n0.9278317\n0.9334098\n0.9382766\n0.9425272\n0.9462421\n0.9494899\n0.9523285\n0.9548074\n0.9569685\n0.9588477\n0.9604751\n0.9618768\n0.9630743\n0.9640864\n0.9649285\n0.9656136\n0.9661525\n0.9665541\n0.9668252\n0.9669716\n0.9669970\n0.9669044\n0.9666951\n0.9663694\n0.9659266\n0.9653646\n0.9646806\n0.9638705\n\n\n1.28\n0.1510420\n0.2515624\n0.3462269\n0.4323544\n0.5089386\n0.5759765\n0.6340241\n0.6839128\n0.7265748\n0.7629407\n0.7938838\n0.8201929\n0.8425636\n0.8615987\n0.8778146\n0.8916500\n0.9034757\n0.9136035\n0.9222953\n0.9297703\n0.9362123\n0.9417753\n0.9465879\n0.9507584\n0.9543773\n0.9575209\n0.9602531\n0.9626278\n0.9646906\n0.9664797\n0.9680276\n0.9693614\n0.9705042\n0.9714754\n0.9722912\n0.9729652\n0.9735088\n0.9739311\n0.9742396\n0.9744403\n0.9745378\n0.9745351\n0.9744344\n0.9742367\n0.9739419\n0.9735489\n0.9730556\n0.9724590\n0.9717550\n\n\n1.29\n0.1581299\n0.2653824\n0.3655692\n0.4557525\n0.5349846\n0.6034643\n0.6619938\n0.7116440\n0.7535556\n0.7888301\n0.8184737\n0.8433755\n0.8643038\n0.8819121\n0.8967510\n0.9092810\n0.9198850\n0.9288810\n0.9365320\n0.9430558\n0.9486324\n0.9534110\n0.9575150\n0.9610468\n0.9640915\n0.9667199\n0.9689909\n0.9709538\n0.9726497\n0.9741130\n0.9753726\n0.9764525\n0.9773730\n0.9781509\n0.9788004\n0.9793331\n0.9797587\n0.9800849\n0.9803181\n0.9804631\n0.9805236\n0.9805021\n0.9804002\n0.9802184\n0.9799563\n0.9796126\n0.9791853\n0.9786713\n0.9780667\n\n\n1.3\n0.1654335\n0.2795320\n0.3851773\n0.4791957\n0.5607531\n0.6303063\n0.6889491\n0.7380216\n0.7788910\n0.8128367\n0.8409987\n0.8643624\n0.8837627\n0.8998971\n0.9133434\n0.9245771\n0.9339879\n0.9418945\n0.9485573\n0.9541890\n0.9589633\n0.9630224\n0.9664827\n0.9694397\n0.9719721\n0.9741445\n0.9760105\n0.9776142\n0.9789923\n0.9801753\n0.9811883\n0.9820525\n0.9827852\n0.9834011\n0.9839120\n0.9843279\n0.9846568\n0.9849053\n0.9850785\n0.9851805\n0.9852140\n0.9851811\n0.9850828\n0.9849194\n0.9846902\n0.9843939\n0.9840283\n0.9835906\n0.9830771\n\n\n1.31\n0.1729487\n0.2939891\n0.4049990\n0.5026015\n0.5861403\n0.6563918\n0.7147866\n0.7629630\n0.8025286\n0.8349444\n0.8614816\n0.8832155\n0.9010398\n0.9156877\n0.9277565\n0.9377297\n0.9459981\n0.9528765\n0.9586186\n0.9634289\n0.9674727\n0.9708834\n0.9737692\n0.9762178\n0.9783008\n0.9800766\n0.9815927\n0.9828884\n0.9839958\n0.9849414\n0.9857471\n0.9864309\n0.9870076\n0.9874896\n0.9878869\n0.9882077\n0.9884588\n0.9886455\n0.9887719\n0.9888413\n0.9888559\n0.9888172\n0.9887258\n0.9885818\n0.9883843\n0.9881321\n0.9878230\n0.9874544\n0.9870227\n\n\n1.32\n0.1806715\n0.3087306\n0.4249817\n0.5258894\n0.6110491\n0.6816222\n0.7394214\n0.7864091\n0.8244428\n0.8551658\n0.8799741\n0.9000244\n0.9162592\n0.9294386\n0.9401709\n0.9489413\n0.9561355\n0.9620601\n0.9669588\n0.9710257\n0.9744153\n0.9772512\n0.9796325\n0.9816387\n0.9833339\n0.9847698\n0.9859885\n0.9870241\n0.9879044\n0.9886522\n0.9892861\n0.9898213\n0.9902703\n0.9906434\n0.9909489\n0.9911936\n0.9913830\n0.9915213\n0.9916119\n0.9916573\n0.9916592\n0.9916187\n0.9915361\n0.9914114\n0.9912436\n0.9910315\n0.9907731\n0.9904658\n0.9901065\n\n\n1.33\n0.1885971\n0.3237322\n0.4450723\n0.5489813\n0.6353896\n0.7059119\n0.7627873\n0.8083232\n0.8446337\n0.8735395\n0.8965527\n0.9149002\n0.9295629\n0.9413177\n0.9507759\n0.9584174\n0.9646179\n0.9696719\n0.9738103\n0.9772145\n0.9800273\n0.9823616\n0.9843066\n0.9859334\n0.9872987\n0.9884479\n0.9894174\n0.9902365\n0.9909291\n0.9915143\n0.9920078\n0.9924224\n0.9927684\n0.9930542\n0.9932866\n0.9934711\n0.9936122\n0.9937132\n0.9937768\n0.9938050\n0.9937989\n0.9937594\n0.9936867\n0.9935803\n0.9934396\n0.9932632\n0.9930493\n0.9927957\n0.9924994\n\n\n1.34\n0.1967209\n0.3389691\n0.4652177\n0.5718023\n0.6590799\n0.7291891\n0.7848360\n0.8286905\n0.8631235\n0.8901257\n0.9113131\n0.9279703\n0.9411046\n0.9514998\n0.9597623\n0.9663604\n0.9716556\n0.9759267\n0.9793896\n0.9822118\n0.9845233\n0.9864257\n0.9879987\n0.9893048\n0.9903935\n0.9913041\n0.9920676\n0.9927090\n0.9932484\n0.9937019\n0.9940823\n0.9944002\n0.9946641\n0.9948808\n0.9950558\n0.9951934\n0.9952973\n0.9953700\n0.9954137\n0.9954298\n0.9954193\n0.9953826\n0.9953197\n0.9952303\n0.9951137\n0.9949685\n0.9947933\n0.9945858\n0.9943437\n\n\n1.35\n0.2050376\n0.3544154\n0.4853651\n0.5942813\n0.6820471\n0.7513955\n0.8055371\n0.8475154\n0.8799545\n0.9050030\n0.9243663\n0.9393726\n0.9510438\n0.9601607\n0.9673172\n0.9729648\n0.9774465\n0.9810232\n0.9838943\n0.9862120\n0.9880937\n0.9896295\n0.9908894\n0.9919280\n0.9927878\n0.9935023\n0.9940978\n0.9945953\n0.9950114\n0.9953593\n0.9956498\n0.9958912\n0.9960905\n0.9962532\n0.9963836\n0.9964852\n0.9965608\n0.9966124\n0.9966417\n0.9966497\n0.9966370\n0.9966041\n0.9965507\n0.9964766\n0.9963810\n0.9962627\n0.9961204\n0.9959522\n0.9957559\n\n\n1.36\n0.2135418\n0.3700448\n0.5054628\n0.6163516\n0.7042271\n0.7724864\n0.8248765\n0.8648203\n0.8951858\n0.9182640\n0.9358334\n0.9492512\n0.9595415\n0.9674728\n0.9736200\n0.9784129\n0.9821732\n0.9851420\n0.9875010\n0.9893874\n0.9909051\n0.9921335\n0.9931334\n0.9939516\n0.9946243\n0.9951797\n0.9956399\n0.9960221\n0.9963401\n0.9966046\n0.9968243\n0.9970060\n0.9971551\n0.9972761\n0.9973723\n0.9974466\n0.9975009\n0.9975370\n0.9975560\n0.9975587\n0.9975456\n0.9975168\n0.9974723\n0.9974115\n0.9973339\n0.9972385\n0.9971239\n0.9969887\n0.9968308\n\n\n1.37\n0.2222280\n0.3858305\n0.5254599\n0.6379513\n0.7255654\n0.7924306\n0.8428559\n0.8806431\n0.9088901\n0.9300117\n0.9458420\n0.9577515\n0.9667553\n0.9736011\n0.9788387\n0.9828727\n0.9860010\n0.9884442\n0.9903656\n0.9918873\n0.9931006\n0.9940744\n0.9948608\n0.9954996\n0.9960212\n0.9964491\n0.9968015\n0.9970925\n0.9973333\n0.9975326\n0.9976973\n0.9978328\n0.9979434\n0.9980325\n0.9981029\n0.9981566\n0.9981952\n0.9982200\n0.9982319\n0.9982314\n0.9982188\n0.9981943\n0.9981576\n0.9981083\n0.9980460\n0.9979696\n0.9978782\n0.9977703\n0.9976442\n\n\n1.38\n0.2310901\n0.4017452\n0.5453072\n0.6590240\n0.7460171\n0.8112093\n0.8594908\n0.8950349\n0.9211507\n0.9403563\n0.9545220\n0.9650174\n0.9728371\n0.9787006\n0.9831281\n0.9864959\n0.9890772\n0.9910710\n0.9926228\n0.9938399\n0.9948015\n0.9955668\n0.9961799\n0.9966743\n0.9970752\n0.9974020\n0.9976695\n0.9978892\n0.9980700\n0.9982188\n0.9983412\n0.9984413\n0.9985226\n0.9985877\n0.9986387\n0.9986771\n0.9987043\n0.9987210\n0.9987281\n0.9987258\n0.9987143\n0.9986938\n0.9986640\n0.9986245\n0.9985748\n0.9985142\n0.9984418\n0.9983563\n0.9982564\n\n\n1.39\n0.2401221\n0.4177614\n0.5649573\n0.6795186\n0.7655466\n0.8288160\n0.8748092\n0.9080577\n0.9320589\n0.9494114\n0.9620030\n0.9711876\n0.9779298\n0.9829145\n0.9866284\n0.9894181\n0.9915309\n0.9931448\n0.9943878\n0.9953532\n0.9961090\n0.9967053\n0.9971793\n0.9975587\n0.9978642\n0.9981117\n0.9983130\n0.9984775\n0.9986121\n0.9987224\n0.9988126\n0.9988860\n0.9989452\n0.9989924\n0.9990290\n0.9990562\n0.9990751\n0.9990862\n0.9990900\n0.9990868\n0.9990768\n0.9990599\n0.9990359\n0.9990046\n0.9989653\n0.9989176\n0.9988607\n0.9987934\n0.9987148\n\n\n1.4\n0.2493174\n0.4338516\n0.5843648\n0.6993899\n0.7841278\n0.8452551\n0.8888500\n0.9197819\n0.9417110\n0.9572919\n0.9684112\n0.9763940\n0.9821660\n0.9863727\n0.9894648\n0.9917579\n0.9934739\n0.9947699\n0.9957576\n0.9965171\n0.9971064\n0.9975673\n0.9979308\n0.9982195\n0.9984505\n0.9986363\n0.9987867\n0.9989088\n0.9990082\n0.9990892\n0.9991552\n0.9992086\n0.9992514\n0.9992852\n0.9993113\n0.9993304\n0.9993433\n0.9993505\n0.9993523\n0.9993489\n0.9993404\n0.9993267\n0.9993076\n0.9992829\n0.9992522\n0.9992149\n0.9991704\n0.9991178\n0.9990563\n\n\n1.41\n0.2586693\n0.4499883\n0.6034868\n0.7185989\n0.8017432\n0.8605412\n0.9016609\n0.9302846\n0.9502058\n0.9641112\n0.9738677\n0.9807597\n0.9856670\n0.9891919\n0.9917475\n0.9936183\n0.9950015\n0.9960342\n0.9968130\n0.9974059\n0.9978617\n0.9982151\n0.9984916\n0.9987096\n0.9988829\n0.9990214\n0.9991328\n0.9992227\n0.9992956\n0.9993546\n0.9994025\n0.9994410\n0.9994717\n0.9994958\n0.9995142\n0.9995275\n0.9995362\n0.9995408\n0.9995413\n0.9995380\n0.9995309\n0.9995199\n0.9995049\n0.9994856\n0.9994617\n0.9994328\n0.9993983\n0.9993575\n0.9993096\n\n\n1.42\n0.2681711\n0.4661442\n0.6222829\n0.7371124\n0.8183843\n0.8746979\n0.9132973\n0.9396469\n0.9576429\n0.9699792\n0.9784864\n0.9843980\n0.9885420\n0.9914751\n0.9935723\n0.9950876\n0.9961942\n0.9970110\n0.9976204\n0.9980797\n0.9984295\n0.9986985\n0.9989072\n0.9990706\n0.9991995\n0.9993019\n0.9993838\n0.9994496\n0.9995026\n0.9995453\n0.9995798\n0.9996074\n0.9996292\n0.9996463\n0.9996591\n0.9996683\n0.9996741\n0.9996769\n0.9996767\n0.9996737\n0.9996679\n0.9996592\n0.9996475\n0.9996326\n0.9996141\n0.9995918\n0.9995652\n0.9995337\n0.9994966\n\n\n1.43\n0.2778154\n0.4822921\n0.6407157\n0.7549031\n0.8340500\n0.8877564\n0.9238203\n0.9479525\n0.9641203\n0.9750009\n0.9823736\n0.9874119\n0.9908884\n0.9933125\n0.9950215\n0.9962401\n0.9971191\n0.9977604\n0.9982338\n0.9985870\n0.9988535\n0.9990567\n0.9992131\n0.9993346\n0.9994298\n0.9995050\n0.9995648\n0.9996125\n0.9996508\n0.9996815\n0.9997061\n0.9997257\n0.9997412\n0.9997531\n0.9997620\n0.9997683\n0.9997721\n0.9997737\n0.9997732\n0.9997706\n0.9997659\n0.9997591\n0.9997500\n0.9997385\n0.9997244\n0.9997073\n0.9996869\n0.9996627\n0.9996342\n\n\n1.44\n0.2875951\n0.4984053\n0.6587505\n0.7719499\n0.8487472\n0.8997550\n0.9332949\n0.9552859\n0.9697332\n0.9792754\n0.9856267\n0.9898940\n0.9927917\n0.9947818\n0.9961651\n0.9971383\n0.9978316\n0.9983317\n0.9986967\n0.9989664\n0.9991679\n0.9993203\n0.9994366\n0.9995263\n0.9995962\n0.9996510\n0.9996943\n0.9997287\n0.9997561\n0.9997781\n0.9997955\n0.9998094\n0.9998202\n0.9998285\n0.9998347\n0.9998389\n0.9998414\n0.9998422\n0.9998416\n0.9998394\n0.9998357\n0.9998304\n0.9998234\n0.9998146\n0.9998039\n0.9997908\n0.9997753\n0.9997568\n0.9997350\n\n\n1.45\n0.2975025\n0.5144578\n0.6763558\n0.7882372\n0.8624890\n0.9107371\n0.9417893\n0.9617307\n0.9745727\n0.9828945\n0.9883339\n0.9919263\n0.9943263\n0.9959496\n0.9970618\n0.9978339\n0.9983770\n0.9987642\n0.9990437\n0.9992482\n0.9993996\n0.9995130\n0.9995989\n0.9996648\n0.9997156\n0.9997553\n0.9997865\n0.9998111\n0.9998306\n0.9998462\n0.9998585\n0.9998682\n0.9998757\n0.9998815\n0.9998857\n0.9998885\n0.9998901\n0.9998905\n0.9998898\n0.9998880\n0.9998851\n0.9998810\n0.9998757\n0.9998691\n0.9998609\n0.9998510\n0.9998392\n0.9998252\n0.9998086\n\n\n1.46\n0.3075301\n0.5304240\n0.6935034\n0.8037552\n0.8752948\n0.9207509\n0.9493728\n0.9673688\n0.9787251\n0.9859429\n0.9905747\n0.9935809\n0.9955565\n0.9968723\n0.9977608\n0.9983693\n0.9987919\n0.9990896\n0.9993023\n0.9994562\n0.9995691\n0.9996530\n0.9997161\n0.9997640\n0.9998008\n0.9998294\n0.9998516\n0.9998691\n0.9998830\n0.9998939\n0.9999025\n0.9999093\n0.9999145\n0.9999185\n0.9999213\n0.9999232\n0.9999241\n0.9999243\n0.9999236\n0.9999222\n0.9999199\n0.9999168\n0.9999128\n0.9999078\n0.9999017\n0.9998942\n0.9998853\n0.9998747\n0.9998621\n\n\n1.47\n0.3176698\n0.5462791\n0.7101681\n0.8184990\n0.8871892\n0.9298475\n0.9561150\n0.9722789\n0.9822707\n0.9884976\n0.9924196\n0.9949206\n0.9965371\n0.9975970\n0.9983023\n0.9987788\n0.9991055\n0.9993330\n0.9994937\n0.9996088\n0.9996925\n0.9997541\n0.9998001\n0.9998348\n0.9998612\n0.9998816\n0.9998974\n0.9999098\n0.9999195\n0.9999272\n0.9999332\n0.9999379\n0.9999415\n0.9999441\n0.9999461\n0.9999473\n0.9999479\n0.9999479\n0.9999473\n0.9999461\n0.9999444\n0.9999421\n0.9999391\n0.9999353\n0.9999307\n0.9999251\n0.9999184\n0.9999104\n0.9999009\n\n\n1.48\n0.3279138\n0.5619993\n0.7263283\n0.8324690\n0.8982014\n0.9380806\n0.9620850\n0.9765361\n0.9852839\n0.9906278\n0.9939306\n0.9959994\n0.9973143\n0.9981630\n0.9987194\n0.9990902\n0.9993412\n0.9995139\n0.9996345\n0.9997201\n0.9997817\n0.9998267\n0.9998600\n0.9998849\n0.9999038\n0.9999183\n0.9999295\n0.9999381\n0.9999449\n0.9999503\n0.9999544\n0.9999576\n0.9999601\n0.9999619\n0.9999632\n0.9999640\n0.9999643\n0.9999642\n0.9999637\n0.9999629\n0.9999615\n0.9999598\n0.9999575\n0.9999547\n0.9999513\n0.9999471\n0.9999421\n0.9999361\n0.9999289\n\n\n1.49\n0.3382540\n0.5775616\n0.7419655\n0.8456701\n0.9083642\n0.9455052\n0.9673499\n0.9802113\n0.9878327\n0.9923953\n0.9951617\n0.9968635\n0.9979270\n0.9986025\n0.9990388\n0.9993256\n0.9995172\n0.9996475\n0.9997375\n0.9998007\n0.9998458\n0.9998785\n0.9999024\n0.9999202\n0.9999337\n0.9999439\n0.9999517\n0.9999578\n0.9999625\n0.9999662\n0.9999690\n0.9999712\n0.9999729\n0.9999741\n0.9999750\n0.9999755\n0.9999756\n0.9999755\n0.9999752\n0.9999745\n0.9999735\n0.9999722\n0.9999705\n0.9999684\n0.9999659\n0.9999628\n0.9999590\n0.9999546\n0.9999492\n\n\n1.5\n0.3486820\n0.5929440\n0.7570643\n0.8581112\n0.9177137\n0.9521768\n0.9719748\n0.9833703\n0.9899789\n0.9938549\n0.9961598\n0.9975521\n0.9984073\n0.9989419\n0.9992821\n0.9995026\n0.9996480\n0.9997457\n0.9998125\n0.9998589\n0.9998917\n0.9999152\n0.9999323\n0.9999450\n0.9999545\n0.9999616\n0.9999671\n0.9999713\n0.9999746\n0.9999771\n0.9999791\n0.9999805\n0.9999817\n0.9999825\n0.9999830\n0.9999834\n0.9999834\n0.9999833\n0.9999830\n0.9999825\n0.9999818\n0.9999808\n0.9999795\n0.9999780\n0.9999761\n0.9999738\n0.9999711\n0.9999677\n0.9999637\n\n\n1.51\n0.3591896\n0.6081256\n0.7716127\n0.8698055\n0.9262883\n0.9581507\n0.9760217\n0.9860744\n0.9917780\n0.9950544\n0.9969650\n0.9980980\n0.9987820\n0.9992027\n0.9994664\n0.9996349\n0.9997446\n0.9998175\n0.9998667\n0.9999005\n0.9999243\n0.9999411\n0.9999533\n0.9999623\n0.9999689\n0.9999739\n0.9999777\n0.9999806\n0.9999828\n0.9999846\n0.9999859\n0.9999869\n0.9999877\n0.9999882\n0.9999886\n0.9999887\n0.9999888\n0.9999887\n0.9999884\n0.9999880\n0.9999875\n0.9999868\n0.9999859\n0.9999847\n0.9999833\n0.9999817\n0.9999796\n0.9999771\n0.9999741\n\n\n1.52\n0.3697684\n0.6230866\n0.7856017\n0.8807691\n0.9341283\n0.9634813\n0.9795492\n0.9883795\n0.9932795\n0.9960358\n0.9976115\n0.9985286\n0.9990727\n0.9994020\n0.9996052\n0.9997333\n0.9998156\n0.9998696\n0.9999057\n0.9999302\n0.9999473\n0.9999593\n0.9999679\n0.9999742\n0.9999788\n0.9999823\n0.9999849\n0.9999869\n0.9999885\n0.9999896\n0.9999905\n0.9999912\n0.9999917\n0.9999921\n0.9999923\n0.9999924\n0.9999924\n0.9999923\n0.9999921\n0.9999919\n0.9999915\n0.9999909\n0.9999903\n0.9999894\n0.9999884\n0.9999872\n0.9999857\n0.9999838\n0.9999816\n\n\n1.53\n0.3804099\n0.6378087\n0.7990252\n0.8910214\n0.9412750\n0.9682217\n0.9826126\n0.9903364\n0.9945272\n0.9968349\n0.9981280\n0.9988666\n0.9992972\n0.9995535\n0.9997093\n0.9998061\n0.9998675\n0.9999073\n0.9999336\n0.9999513\n0.9999635\n0.9999720\n0.9999781\n0.9999825\n0.9999857\n0.9999881\n0.9999899\n0.9999912\n0.9999923\n0.9999931\n0.9999937\n0.9999941\n0.9999945\n0.9999947\n0.9999948\n0.9999949\n0.9999949\n0.9999948\n0.9999947\n0.9999945\n0.9999942\n0.9999938\n0.9999933\n0.9999927\n0.9999920\n0.9999910\n0.9999899\n0.9999886\n0.9999869\n\n\n1.54\n0.3911058\n0.6522744\n0.8118800\n0.9005844\n0.9477703\n0.9724231\n0.9852629\n0.9919911\n0.9955595\n0.9974827\n0.9985388\n0.9991307\n0.9994696\n0.9996681\n0.9997869\n0.9998597\n0.9999052\n0.9999344\n0.9999534\n0.9999662\n0.9999748\n0.9999808\n0.9999851\n0.9999881\n0.9999903\n0.9999920\n0.9999932\n0.9999941\n0.9999948\n0.9999954\n0.9999958\n0.9999961\n0.9999963\n0.9999965\n0.9999966\n0.9999966\n0.9999966\n0.9999965\n0.9999964\n0.9999962\n0.9999960\n0.9999958\n0.9999954\n0.9999950\n0.9999944\n0.9999938\n0.9999929\n0.9999920\n0.9999907\n\n\n1.55\n0.4018474\n0.6664678\n0.8241657\n0.9094823\n0.9536565\n0.9761345\n0.9875476\n0.9933848\n0.9964102\n0.9980056\n0.9988640\n0.9993360\n0.9996014\n0.9997543\n0.9998445\n0.9998989\n0.9999325\n0.9999538\n0.9999675\n0.9999766\n0.9999827\n0.9999869\n0.9999899\n0.9999920\n0.9999935\n0.9999946\n0.9999955\n0.9999961\n0.9999966\n0.9999969\n0.9999972\n0.9999974\n0.9999975\n0.9999976\n0.9999977\n0.9999977\n0.9999977\n0.9999977\n0.9999976\n0.9999975\n0.9999973\n0.9999971\n0.9999968\n0.9999965\n0.9999961\n0.9999957\n0.9999951\n0.9999943\n0.9999934\n\n\n1.56\n0.4126264\n0.6803742\n0.8358843\n0.9177408\n0.9589751\n0.9794022\n0.9895101\n0.9945541\n0.9971082\n0.9984259\n0.9991203\n0.9994948\n0.9997018\n0.9998189\n0.9998870\n0.9999275\n0.9999521\n0.9999676\n0.9999774\n0.9999839\n0.9999882\n0.9999911\n0.9999932\n0.9999946\n0.9999957\n0.9999964\n0.9999970\n0.9999974\n0.9999977\n0.9999980\n0.9999981\n0.9999983\n0.9999984\n0.9999984\n0.9999985\n0.9999985\n0.9999985\n0.9999984\n0.9999984\n0.9999983\n0.9999982\n0.9999980\n0.9999978\n0.9999976\n0.9999973\n0.9999970\n0.9999966\n0.9999960\n0.9999954\n\n\n1.57\n0.4234342\n0.6939803\n0.8470401\n0.9253874\n0.9637673\n0.9822702\n0.9911898\n0.9955314\n0.9976787\n0.9987622\n0.9993214\n0.9996173\n0.9997778\n0.9998671\n0.9999182\n0.9999482\n0.9999662\n0.9999773\n0.9999844\n0.9999889\n0.9999920\n0.9999940\n0.9999954\n0.9999964\n0.9999971\n0.9999976\n0.9999980\n0.9999983\n0.9999985\n0.9999987\n0.9999988\n0.9999989\n0.9999989\n0.9999990\n0.9999990\n0.9999990\n0.9999990\n0.9999990\n0.9999989\n0.9999988\n0.9999988\n0.9999987\n0.9999985\n0.9999984\n0.9999982\n0.9999979\n0.9999976\n0.9999972\n0.9999967\n\n\n1.58\n0.4342624\n0.7072742\n0.8576399\n0.9324505\n0.9680730\n0.9847793\n0.9926225\n0.9963452\n0.9981431\n0.9990302\n0.9994786\n0.9997111\n0.9998351\n0.9999029\n0.9999411\n0.9999631\n0.9999762\n0.9999842\n0.9999892\n0.9999924\n0.9999945\n0.9999960\n0.9999969\n0.9999976\n0.9999981\n0.9999984\n0.9999987\n0.9999989\n0.9999990\n0.9999991\n0.9999992\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999992\n0.9999992\n0.9999991\n0.9999990\n0.9999989\n0.9999987\n0.9999986\n0.9999983\n0.9999980\n0.9999977\n\n\n1.59\n0.4451027\n0.7202450\n0.8676921\n0.9389591\n0.9719310\n0.9869677\n0.9938403\n0.9970203\n0.9985197\n0.9992429\n0.9996008\n0.9997828\n0.9998781\n0.9999293\n0.9999577\n0.9999739\n0.9999834\n0.9999891\n0.9999926\n0.9999949\n0.9999963\n0.9999973\n0.9999979\n0.9999984\n0.9999987\n0.9999990\n0.9999991\n0.9999993\n0.9999994\n0.9999994\n0.9999995\n0.9999995\n0.9999995\n0.9999996\n0.9999996\n0.9999996\n0.9999996\n0.9999995\n0.9999995\n0.9999995\n0.9999994\n0.9999994\n0.9999993\n0.9999992\n0.9999991\n0.9999990\n0.9999988\n0.9999986\n0.9999984\n\n\n1.6\n0.4559468\n0.7328836\n0.8772072\n0.9449428\n0.9753783\n0.9888704\n0.9948720\n0.9975784\n0.9988238\n0.9994111\n0.9996955\n0.9998374\n0.9999102\n0.9999487\n0.9999698\n0.9999816\n0.9999884\n0.9999925\n0.9999950\n0.9999965\n0.9999975\n0.9999982\n0.9999986\n0.9999989\n0.9999992\n0.9999993\n0.9999994\n0.9999995\n0.9999996\n0.9999996\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999996\n0.9999996\n0.9999995\n0.9999995\n0.9999994\n0.9999993\n0.9999992\n0.9999990\n0.9999989\n\n\n\n\n\n\n\n18.3.5 Define a palette for the results\n\n#set the colours used for the plot\nwut.colors&lt;-rev(c(\"red\",\"magenta\",\"pink\",\"orange\",\"yellow\",\"green\",\"dark green\", \"blue\",\"purple\",\"dark blue\"))\n\n\n#print plot of power\n\nimage(risk_values,maf_values,as.matrix(xx),zlim=c(0,1),col= wut.colors,breaks=seq(0,1,by=0.1),useRaster=T,main=paste(\"Power @ Prevalence :\",prevalence,\"| Cases :\",num.cases,\"| Controls :\",num.controls,\"| alpha :\",alpha),xlab=\"Relative Risk\",ylab=\"Minor allele frequency\",cex.main=0.8);legend(x=1,y=0.4,cex=0.5,legend=c(\"0.00-0.09\",\"0.10-0.19\",\"0.20-0.29\",\"0.30-0.39\",\"0.40-0.49\",\"0.50-0.59\",\"0.60-0.69\",\"0.70-0.79\",\"0.80-0.89\",\"0.90-1.00\"),fill=wut.colors,title=\"Power\")",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Genetic Power Calculator</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html",
    "href": "Principal_Components_Analysis.html",
    "title": "19  Principal Components Analysis",
    "section": "",
    "text": "19.1 Libraries\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#set-a-random-seed",
    "href": "Principal_Components_Analysis.html#set-a-random-seed",
    "title": "19  Principal Components Analysis",
    "section": "19.2 Set a random seed",
    "text": "19.2 Set a random seed\n\nset.seed(8882321)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#load-iris-data",
    "href": "Principal_Components_Analysis.html#load-iris-data",
    "title": "19  Principal Components Analysis",
    "section": "19.3 Load iris data",
    "text": "19.3 Load iris data\nUse the R Iris data\n\ndf&lt;-as.tibble(iris)\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\nkable(df)\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n5.0\n3.4\n1.5\n0.2\nsetosa\n\n\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.1\nsetosa\n\n\n5.4\n3.7\n1.5\n0.2\nsetosa\n\n\n4.8\n3.4\n1.6\n0.2\nsetosa\n\n\n4.8\n3.0\n1.4\n0.1\nsetosa\n\n\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n5.8\n4.0\n1.2\n0.2\nsetosa\n\n\n5.7\n4.4\n1.5\n0.4\nsetosa\n\n\n5.4\n3.9\n1.3\n0.4\nsetosa\n\n\n5.1\n3.5\n1.4\n0.3\nsetosa\n\n\n5.7\n3.8\n1.7\n0.3\nsetosa\n\n\n5.1\n3.8\n1.5\n0.3\nsetosa\n\n\n5.4\n3.4\n1.7\n0.2\nsetosa\n\n\n5.1\n3.7\n1.5\n0.4\nsetosa\n\n\n4.6\n3.6\n1.0\n0.2\nsetosa\n\n\n5.1\n3.3\n1.7\n0.5\nsetosa\n\n\n4.8\n3.4\n1.9\n0.2\nsetosa\n\n\n5.0\n3.0\n1.6\n0.2\nsetosa\n\n\n5.0\n3.4\n1.6\n0.4\nsetosa\n\n\n5.2\n3.5\n1.5\n0.2\nsetosa\n\n\n5.2\n3.4\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.6\n0.2\nsetosa\n\n\n4.8\n3.1\n1.6\n0.2\nsetosa\n\n\n5.4\n3.4\n1.5\n0.4\nsetosa\n\n\n5.2\n4.1\n1.5\n0.1\nsetosa\n\n\n5.5\n4.2\n1.4\n0.2\nsetosa\n\n\n4.9\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.2\n1.2\n0.2\nsetosa\n\n\n5.5\n3.5\n1.3\n0.2\nsetosa\n\n\n4.9\n3.6\n1.4\n0.1\nsetosa\n\n\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n5.1\n3.4\n1.5\n0.2\nsetosa\n\n\n5.0\n3.5\n1.3\n0.3\nsetosa\n\n\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n5.0\n3.5\n1.6\n0.6\nsetosa\n\n\n5.1\n3.8\n1.9\n0.4\nsetosa\n\n\n4.8\n3.0\n1.4\n0.3\nsetosa\n\n\n5.1\n3.8\n1.6\n0.2\nsetosa\n\n\n4.6\n3.2\n1.4\n0.2\nsetosa\n\n\n5.3\n3.7\n1.5\n0.2\nsetosa\n\n\n5.0\n3.3\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.9\n3.1\n4.9\n1.5\nversicolor\n\n\n5.5\n2.3\n4.0\n1.3\nversicolor\n\n\n6.5\n2.8\n4.6\n1.5\nversicolor\n\n\n5.7\n2.8\n4.5\n1.3\nversicolor\n\n\n6.3\n3.3\n4.7\n1.6\nversicolor\n\n\n4.9\n2.4\n3.3\n1.0\nversicolor\n\n\n6.6\n2.9\n4.6\n1.3\nversicolor\n\n\n5.2\n2.7\n3.9\n1.4\nversicolor\n\n\n5.0\n2.0\n3.5\n1.0\nversicolor\n\n\n5.9\n3.0\n4.2\n1.5\nversicolor\n\n\n6.0\n2.2\n4.0\n1.0\nversicolor\n\n\n6.1\n2.9\n4.7\n1.4\nversicolor\n\n\n5.6\n2.9\n3.6\n1.3\nversicolor\n\n\n6.7\n3.1\n4.4\n1.4\nversicolor\n\n\n5.6\n3.0\n4.5\n1.5\nversicolor\n\n\n5.8\n2.7\n4.1\n1.0\nversicolor\n\n\n6.2\n2.2\n4.5\n1.5\nversicolor\n\n\n5.6\n2.5\n3.9\n1.1\nversicolor\n\n\n5.9\n3.2\n4.8\n1.8\nversicolor\n\n\n6.1\n2.8\n4.0\n1.3\nversicolor\n\n\n6.3\n2.5\n4.9\n1.5\nversicolor\n\n\n6.1\n2.8\n4.7\n1.2\nversicolor\n\n\n6.4\n2.9\n4.3\n1.3\nversicolor\n\n\n6.6\n3.0\n4.4\n1.4\nversicolor\n\n\n6.8\n2.8\n4.8\n1.4\nversicolor\n\n\n6.7\n3.0\n5.0\n1.7\nversicolor\n\n\n6.0\n2.9\n4.5\n1.5\nversicolor\n\n\n5.7\n2.6\n3.5\n1.0\nversicolor\n\n\n5.5\n2.4\n3.8\n1.1\nversicolor\n\n\n5.5\n2.4\n3.7\n1.0\nversicolor\n\n\n5.8\n2.7\n3.9\n1.2\nversicolor\n\n\n6.0\n2.7\n5.1\n1.6\nversicolor\n\n\n5.4\n3.0\n4.5\n1.5\nversicolor\n\n\n6.0\n3.4\n4.5\n1.6\nversicolor\n\n\n6.7\n3.1\n4.7\n1.5\nversicolor\n\n\n6.3\n2.3\n4.4\n1.3\nversicolor\n\n\n5.6\n3.0\n4.1\n1.3\nversicolor\n\n\n5.5\n2.5\n4.0\n1.3\nversicolor\n\n\n5.5\n2.6\n4.4\n1.2\nversicolor\n\n\n6.1\n3.0\n4.6\n1.4\nversicolor\n\n\n5.8\n2.6\n4.0\n1.2\nversicolor\n\n\n5.0\n2.3\n3.3\n1.0\nversicolor\n\n\n5.6\n2.7\n4.2\n1.3\nversicolor\n\n\n5.7\n3.0\n4.2\n1.2\nversicolor\n\n\n5.7\n2.9\n4.2\n1.3\nversicolor\n\n\n6.2\n2.9\n4.3\n1.3\nversicolor\n\n\n5.1\n2.5\n3.0\n1.1\nversicolor\n\n\n5.7\n2.8\n4.1\n1.3\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n7.1\n3.0\n5.9\n2.1\nvirginica\n\n\n6.3\n2.9\n5.6\n1.8\nvirginica\n\n\n6.5\n3.0\n5.8\n2.2\nvirginica\n\n\n7.6\n3.0\n6.6\n2.1\nvirginica\n\n\n4.9\n2.5\n4.5\n1.7\nvirginica\n\n\n7.3\n2.9\n6.3\n1.8\nvirginica\n\n\n6.7\n2.5\n5.8\n1.8\nvirginica\n\n\n7.2\n3.6\n6.1\n2.5\nvirginica\n\n\n6.5\n3.2\n5.1\n2.0\nvirginica\n\n\n6.4\n2.7\n5.3\n1.9\nvirginica\n\n\n6.8\n3.0\n5.5\n2.1\nvirginica\n\n\n5.7\n2.5\n5.0\n2.0\nvirginica\n\n\n5.8\n2.8\n5.1\n2.4\nvirginica\n\n\n6.4\n3.2\n5.3\n2.3\nvirginica\n\n\n6.5\n3.0\n5.5\n1.8\nvirginica\n\n\n7.7\n3.8\n6.7\n2.2\nvirginica\n\n\n7.7\n2.6\n6.9\n2.3\nvirginica\n\n\n6.0\n2.2\n5.0\n1.5\nvirginica\n\n\n6.9\n3.2\n5.7\n2.3\nvirginica\n\n\n5.6\n2.8\n4.9\n2.0\nvirginica\n\n\n7.7\n2.8\n6.7\n2.0\nvirginica\n\n\n6.3\n2.7\n4.9\n1.8\nvirginica\n\n\n6.7\n3.3\n5.7\n2.1\nvirginica\n\n\n7.2\n3.2\n6.0\n1.8\nvirginica\n\n\n6.2\n2.8\n4.8\n1.8\nvirginica\n\n\n6.1\n3.0\n4.9\n1.8\nvirginica\n\n\n6.4\n2.8\n5.6\n2.1\nvirginica\n\n\n7.2\n3.0\n5.8\n1.6\nvirginica\n\n\n7.4\n2.8\n6.1\n1.9\nvirginica\n\n\n7.9\n3.8\n6.4\n2.0\nvirginica\n\n\n6.4\n2.8\n5.6\n2.2\nvirginica\n\n\n6.3\n2.8\n5.1\n1.5\nvirginica\n\n\n6.1\n2.6\n5.6\n1.4\nvirginica\n\n\n7.7\n3.0\n6.1\n2.3\nvirginica\n\n\n6.3\n3.4\n5.6\n2.4\nvirginica\n\n\n6.4\n3.1\n5.5\n1.8\nvirginica\n\n\n6.0\n3.0\n4.8\n1.8\nvirginica\n\n\n6.9\n3.1\n5.4\n2.1\nvirginica\n\n\n6.7\n3.1\n5.6\n2.4\nvirginica\n\n\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n\nThe first step in PCA is to run the prcomp command on the columns of interest. This is necessarily a complete case table with numeric values, so missing values should be imputed. The mice package is good for imputation in R, but isn’t used here as the data are mostly complete. There’s just one missing value which we’ll drop to keep this tutorial focussed.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#prcomp",
    "href": "Principal_Components_Analysis.html#prcomp",
    "title": "19  Principal Components Analysis",
    "section": "19.4 Prcomp",
    "text": "19.4 Prcomp\n\ndf.pca &lt;- prcomp(select(iris,Sepal.Length:Petal.Width))\n(df.pca.summary&lt;-summary(df.pca))\n\nImportance of components:\n                          PC1     PC2    PC3     PC4\nStandard deviation     2.0563 0.49262 0.2797 0.15439\nProportion of Variance 0.9246 0.05307 0.0171 0.00521\nCumulative Proportion  0.9246 0.97769 0.9948 1.00000",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#extract-importance-data-used-for-scree-plots",
    "href": "Principal_Components_Analysis.html#extract-importance-data-used-for-scree-plots",
    "title": "19  Principal Components Analysis",
    "section": "19.5 Extract importance data used for scree plots",
    "text": "19.5 Extract importance data used for scree plots\n\n(\n  df.importance&lt;-tibble(pc = 1:length(df.pca.summary$importance[1,]),\n                      standard.deviation = df.pca.summary$importance[1,],\n                      prop.of.variance = df.pca.summary$importance[2,],\n                      cumulative.variance = df.pca.summary$importance[3,]\n                        )\n)\n\n# A tibble: 4 × 4\n     pc standard.deviation prop.of.variance cumulative.variance\n  &lt;int&gt;              &lt;dbl&gt;            &lt;dbl&gt;               &lt;dbl&gt;\n1     1              2.06           0.925                 0.925\n2     2              0.493          0.0531                0.978\n3     3              0.280          0.0171                0.995\n4     4              0.154          0.00521               1    \n\n\nWhenever you work with PCA, it is useful to start by looking at the scree plots, which show (a) the proportion of all variance in the data that are explained by each principal component and (b) the cumulative proportion of variance explained by PCs 1-n.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#scree-plot",
    "href": "Principal_Components_Analysis.html#scree-plot",
    "title": "19  Principal Components Analysis",
    "section": "19.6 Scree plot",
    "text": "19.6 Scree plot\n\nggplot(df.importance,aes(pc,prop.of.variance))+ geom_bar(stat=\"identity\") +\nggplot(df.importance,aes(pc,cumulative.variance))+geom_bar(stat=\"identity\")\n\n\n\n\n\n\n\n\nIt looks like most of the variance is explained by the first principal component.\nThe prcomp data outputs are not compatible with ggplot, so we’ll grab the PC values and add them to our data frame. This is possible because the prcomp data is embedded in an object x which maps to the original df on a row-by-row basis. This allows merging via the bind_cols function.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#add-pc-variables-to-data-frame",
    "href": "Principal_Components_Analysis.html#add-pc-variables-to-data-frame",
    "title": "19  Principal Components Analysis",
    "section": "19.7 Add PC variables to data frame",
    "text": "19.7 Add PC variables to data frame\n\ndf&lt;- bind_cols(df,as.data.frame(df.pca$x))",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Principal_Components_Analysis.html#biplot-pc1pc2",
    "href": "Principal_Components_Analysis.html#biplot-pc1pc2",
    "title": "19  Principal Components Analysis",
    "section": "19.8 Biplot PC1/PC2",
    "text": "19.8 Biplot PC1/PC2\nBiplots are used to explore PCA data visually. We’ll start by looking at PC1 and PC2. Our chart will be scaled on the x and y axes [using the aspect ratio controls within theme()]so that the dimensions of the chart don’t misrepresent the proportion of variance explained by each PC. The ellipses show the 95% confidence interval for the various groups.\n\nggplot(df,aes(PC1,PC2,col=Species))+\n  geom_point() +\n  stat_ellipse(geom=\"polygon\", aes(fill = Species), alpha = 0.2, show.legend = FALSE, level = 0.95)+\n  xlab(label = str_c(\"PC1 (\",round(100*df.importance$prop.of.variance[1],2),\" %)\",sep = \"\"))+\n  ylab(label = str_c(\"PC2 (\",round(100*df.importance$prop.of.variance[2],2),\" %)\",sep = \"\"))+\n  theme(aspect.ratio=df.importance$prop.of.variance[2]/df.importance$prop.of.variance[1])+\n  ggtitle(\"Iris data PCA\")\n\n\n\n\n\n\n\n\n\n19.8.1 Loadings plot PC1/PC2\nIt is hard to interpret this in the context of various features without understanding which variables contributed to the observed variance in the PCA. A loadings plot is really useful for this. We can get the Loadings from the prcomp object.\n\ndf.pca %&gt;%\n    tidy(matrix = \"rotation\") %&gt;% \n    filter(PC==\"1\" | PC==\"2\") %&gt;% \n    ggplot(aes(column,value)) +\n  geom_bar(stat=\"identity\")+\n    facet_grid(PC~.)+\n  coord_flip()",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Principal Components Analysis</span>"
    ]
  },
  {
    "objectID": "Consent_Forms_with_QR.html",
    "href": "Consent_Forms_with_QR.html",
    "title": "20  Consent Forms with Unique IDs and QR Codes",
    "section": "",
    "text": "20.1 Background\nThis is a very basic example of how R markdown can be scripted to create a set of uniquely barcoded consent forms. By using the QRcode from the consent form to assign unique IDs to participants, the process of assignment and ascertainment is made more robust. There is no need to use stickers or complex systems to assign IDs.\nThe code is an R script that acts as a controller and an RMD file which creates the final PDF/Word forms. The controller assigns IDs, generates QRCodes and sends these to the RMD file, which binds them with the information/consent sheet and creates either PDF or Word files that can be printed.\nThe code in the R file includes a simple system for assigning non-overlapping unique ID (UID) codes.",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Consent Forms with Unique IDs and QR Codes</span>"
    ]
  },
  {
    "objectID": "Consent_Forms_with_QR.html#project-link",
    "href": "Consent_Forms_with_QR.html#project-link",
    "title": "20  Consent Forms with Unique IDs and QR Codes",
    "section": "20.2 Project Link",
    "text": "20.2 Project Link\nThis project is hosted on a standalone Github Repo - Unique Consent Forms",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Consent Forms with Unique IDs and QR Codes</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html",
    "href": "Digital_Luminex.html",
    "title": "21  Digital Luminex",
    "section": "",
    "text": "21.0.1 Background\nThis script demonstrates how to read data from a Luminex .LXB file and to apply a ‘digital’ luminex approach. This counts the number of beads that fluoresce above a threshold, rather than looking at the mean fluorescence intensity for a marker on all beads. This is analogous to a ‘digital PCR’ approach.\nIn theory this might be a method that requires less inter-plate normalisation as the method is less dependent on average signal strength and more dependent on a within-plate EM model that simply asks what fraction of beads have a positive signal for each marker. Specifically, the actual fluorescence intensity of the markers on the beads is actually very variable from plate to plate, but whatever the fluorescence intensity is, it should still be possible to define a positive and negative population.\nThe work uses an approach to reading the LXB files which was originally implemented in the lxb package. This is no longer maintained, so a copy of the tar file is included in the packages directory of this repo.",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html#libraries",
    "href": "Digital_Luminex.html#libraries",
    "title": "21  Digital Luminex",
    "section": "21.1 Libraries",
    "text": "21.1 Libraries\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(data.table)\nlibrary(mclust)\nlibrary(mixtools)\ninstall.packages(\"packages/lxb-1.5.tar.gz\", repos = NULL, type=\"source\")\nlibrary(lxb)",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html#define-functions",
    "href": "Digital_Luminex.html#define-functions",
    "title": "21  Digital Luminex",
    "section": "21.2 Define Functions",
    "text": "21.2 Define Functions\n\n21.2.1 Convert LXB Files to CSV\nCreate function to import an lxb file, add the well name, convert to a data frame and export a CSV with similar name to original file.\n\nlxb.parse&lt;-function(lxbfile)\n{\n#get the file\noutputfile&lt;-gsub(x = lxbfile,pattern = \".lxb\",replacement = \".csv\")\nwell&lt;-unlist(strsplit(lxbfile,split = \"_\"))\nwell&lt;-well[length(well)]\nwell&lt;-gsub(well,pattern = \".lxb\",replacement = \"\")\nlxbdata &lt;- readLxb(lxbfile)  \nlxbdata &lt;- as.data.frame(lxbdata) \nnames(lxbdata)&lt;-gsub(names(lxbdata),pattern = \" \",replacement = \".\")\nlxbdata$well&lt;-well\n#remove invalid beads\nlxbdata&lt;-lxbdata[which(lxbdata$Bead.Valid==1),]\nwrite.csv(x = lxbdata,file = outputfile,row.names = F)\n}\n\n\n\n21.2.2 Aggregate the csv files in a single df\n\n#define a function to \nlxb.aggregate.data&lt;-function(path)\n{\n  #list only the csv files\n  lxbs&lt;-list.files(path = path,pattern = \"*.csv\",full.names = T)\n  tables &lt;- lapply(lxbs, fread)\n  data.table::rbindlist(tables)\n}\n\n\n\n21.2.3 Apply mixed EM to find thresholds on each bead\n\nlxb.digital.thresholds&lt;-function(df,pdf.out=FALSE,sd.threshold=5)\n{\n  df2&lt;-df %&gt;% group_by(Bead.ID) %&gt;% summarise(n=n())\n  #get bead values from DF\n  df2$EM.threshold&lt;-NA\n  \n  for(i in 1:length(df2$Bead.ID))\n  {\n    fit&lt;-normalmixEM(df$RP1L.Peak[which(df$Bead.ID==df2$Bead.ID[i])])\n    #plot(fit,which=2,breaks=40)\n    \n    df2$EM.threshold[i]&lt;-fit$mu[which(fit$mu==min(fit$mu))]+(sd.threshold*fit$sigma[which(fit$mu==min(fit$mu))])\n    if(pdf.out==TRUE){\n                      pdf(file = paste(\"data/Luminex_LXB_Files/\",df2$Bead.ID[i],\".pdf\",sep=\"\"))\n                      par(mfrow=c(1,2))\n                      plot(sort(df$RP1L.Peak[which(df$Bead.ID==df2$Bead.ID[i])]))\n                      abline(h=df2$EM.threshold[i],lty=2,col=\"red\")\n                      plot(fit,whichplots = 2)\n                      abline(v=df2$EM.threshold[i],lty=2,col=\"red\")\n                                        dev.off()\n                      }\n  }\n  df&lt;-merge(df,df2,by=\"Bead.ID\")\n  print(df2)\n  df$classification&lt;-as.numeric(df$RP1L.Peak&gt;df$EM.threshold)  \n  return(df)\n}\n\n\n\n21.2.4 Manually select threshold values\n\nlxb.digital.thresholds.locator&lt;-function(df,pdf.out=FALSE,sd.threshold=4)\n{\n  df2&lt;-df %&gt;% group_by(Bead.ID) %&gt;% summarise(n=n())\n  #get bead values from DF\n  df2$EM.threshold&lt;-NA\n  df2$EM.threshold&lt;-as.numeric(df2$EM.threshold)\n  \n  for(i in 1:length(df2$Bead.ID))\n  {\n    fit&lt;-normalmixEM(df$RP1L.Peak[which(df$Bead.ID==df2$Bead.ID[i])])\n    #plot(fit,which=2,breaks=40)\n    \n    \n      plot(fit,whichplots = 2)\n      EM.threshold&lt;-locator(n = 1)\n      df2$EM.threshold[i]&lt;-EM.threshold$x\n      abline(v=df2$EM.threshold[i],lty=2,col=\"red\")\n  }  \n    \n  \n  df&lt;-merge(df,df2,by=\"Bead.ID\")\n  print(df2)\n  df$classification&lt;-as.numeric(df$RP1L.Peak&gt;df$EM.threshold)  \n  return(df)\n}",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html#apply-the-functions-to-real-data",
    "href": "Digital_Luminex.html#apply-the-functions-to-real-data",
    "title": "21  Digital Luminex",
    "section": "21.3 Apply the functions to real data",
    "text": "21.3 Apply the functions to real data\n\n21.3.1 List the LXB files\nLuminex machines spit out one LXB file for each well\n\nlxbs&lt;-list.files(path = \"data/Luminex_LXB_Files/\",pattern = \"*.lxb\",full.names = T)\nlxbs\n\n [1] \"data/Luminex_LXB_Files//A1.lxb\"  \"data/Luminex_LXB_Files//A10.lxb\"\n [3] \"data/Luminex_LXB_Files//A11.lxb\" \"data/Luminex_LXB_Files//A12.lxb\"\n [5] \"data/Luminex_LXB_Files//A2.lxb\"  \"data/Luminex_LXB_Files//A3.lxb\" \n [7] \"data/Luminex_LXB_Files//A4.lxb\"  \"data/Luminex_LXB_Files//A5.lxb\" \n [9] \"data/Luminex_LXB_Files//A6.lxb\"  \"data/Luminex_LXB_Files//A7.lxb\" \n[11] \"data/Luminex_LXB_Files//A8.lxb\"  \"data/Luminex_LXB_Files//A9.lxb\" \n\n\n\n\n21.3.2 Apply the lxb.parse function to all lxb files in directory\n\nlapply(lxbs,lxb.parse)\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\n[[6]]\nNULL\n\n[[7]]\nNULL\n\n[[8]]\nNULL\n\n[[9]]\nNULL\n\n[[10]]\nNULL\n\n[[11]]\nNULL\n\n[[12]]\nNULL\n\n\n\n\n21.3.3 Aggregate the data\n\na&lt;-lxb.aggregate.data(path = \"data/Luminex_LXB_Files/\")\na$col&lt;-substr(a$well,start = 1,stop = 1)\na$row&lt;-substr(a$well,start = 2,stop = 3)\nkable(head(a,20))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCL1.X\nCL1.Y\nCL1.Peak\nCL1.Ch.Value\nCL1.Area\nCL1.Valid\nCL1.SR\nCL1.MBR\nCL1.Pass.Mini.Ratio\nCL1.Pass.Max.Ratio\nCL1.Pass.Nearby\nCL1.Pass.Edge\nCL2.X\nCL2.Y\nCL2.Peak\nCL2.Ch.Value\nCL2.Area\nCL2.Valid\nCL2.SR\nCL2.MBR\nCL2.Pass.Mini.Ratio\nCL2.Pass.Max.Ratio\nCL2.Pass.Nearby\nCL2.Pass.Edge\nRP1L.X\nRP1L.Y\nRP1L.Peak\nRP1L.Ch.Value\nRP1L.Area\nRP1L.Valid\nRP1L.SR\nRP1L.MBR\nRP1L.Pass.Mini.Ratio\nRP1L.Pass.Max.Ratio\nRP1L.Pass.Nearby\nRP1L.Pass.Edge\nRP1S.X\nRP1S.Y\nRP1S.Peak\nRP1S.Ch.Value\nRP1S.Area\nRP1S.Valid\nRP1S.SR\nRP1S.MBR\nRP1S.Pass.Mini.Ratio\nRP1S.Pass.Max.Ratio\nRP1S.Pass.Nearby\nRP1S.Pass.Edge\nBead.Valid\nCL2.Offset.X\nCL2.Offset.Y\nCL1_CL2.Dist\nRP1.Offset.X\nRP1.Offset.Y\nCL1.Value\nCL2.Value\nRP1.Value\nBead.ID\nwell\ncol\nrow\n\n\n\n\n1149234516\n1092558165\n1343\n0\n16947\n1\n-1560336802\n-462012473\n1\n1\n1\n1\n1149223297\n1092479444\n1296\n0\n16974\n1\n1181464885\n-1991868891\n1\n1\n1\n1\n1149236985\n1092988697\n59632\n0\n1135332\n1\n0\n0\n1\n1\n1\n1\n1149236985\n1092988697\n59632\n0\n52185\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-812668335\n0\n0\n118\n118\n6103\n12\nFiles//A1\nF\nil\n\n\n1155884337\n1093973789\n2535\n0\n32248\n1\n-1461695296\n-1174163721\n1\n1\n1\n1\n1155877959\n1094254143\n3861\n0\n49562\n1\n1419346258\n-463449520\n1\n1\n1\n1\n1155884214\n1094404032\n49395\n0\n777699\n1\n0\n0\n1\n1\n1\n1\n1155884214\n1094404032\n49395\n0\n27587\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1717978224\n0\n0\n224\n345\n3080\n26\nFiles//A1\nF\nil\n\n\n1141831731\n1094397512\n7971\n0\n100698\n1\n681421247\n-1790273464\n1\n1\n1\n1\n1141820439\n1093943886\n2157\n0\n26850\n1\n-402982476\n-53427154\n1\n1\n1\n1\n1141835711\n1094827668\n61485\n0\n1217522\n1\n0\n0\n1\n1\n1\n1\n1141835711\n1094827668\n61485\n0\n63810\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1367863707\n0\n0\n701\n187\n7463\n21\nFiles//A1\nF\nil\n\n\n1150917202\n1097198649\n4966\n0\n60897\n1\n271482799\n-1191083431\n1\n1\n1\n1\n1150911874\n1097454201\n4016\n0\n50462\n1\n1850697676\n-113029935\n1\n1\n1\n1\n1150918093\n1097628233\n61767\n0\n1249945\n1\n0\n0\n1\n1\n1\n1\n1150918093\n1097628233\n61767\n0\n80109\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n459537696\n0\n0\n424\n352\n9369\n27\nFiles//A1\nF\nil\n\n\n1152361617\n1097643907\n16662\n0\n208370\n1\n-351809365\n-2047823587\n1\n1\n1\n1\n1152359059\n1097576007\n2136\n0\n28456\n1\n2107587046\n76070548\n1\n1\n1\n1\n1152362214\n1098073401\n59204\n0\n848208\n1\n0\n0\n1\n1\n1\n1\n1152362214\n1098073401\n59204\n0\n33077\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1127313627\n0\n0\n1450\n198\n3868\n22\nFiles//A1\nF\nil\n\n\n1151775855\n1098727795\n16030\n0\n203329\n1\n870860878\n1513437910\n1\n1\n1\n1\n1151770676\n1098681510\n3822\n0\n47775\n1\n-758130026\n1252485282\n1\n1\n1\n1\n1151776571\n1099032358\n14198\n0\n166583\n1\n0\n0\n1\n1\n1\n1\n1151776571\n1099032358\n14198\n0\n5589\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1774791597\n0\n0\n1415\n333\n660\n29\nFiles//A1\nF\nil\n\n\n1155033097\n1100340701\n4580\n0\n59419\n1\n-1852063166\n-1009139138\n1\n1\n1\n1\n1155030331\n1100114914\n3966\n0\n51219\n1\n114335912\n-500949543\n1\n1\n1\n1\n1155033148\n1100555026\n61894\n0\n1325814\n1\n0\n0\n1\n1\n1\n1\n1155033148\n1100555026\n61894\n0\n81886\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-730158204\n0\n0\n414\n357\n9577\n27\nFiles//A1\nF\nil\n\n\n1156429977\n1102179833\n15608\n0\n198756\n1\n1505756983\n1583439109\n1\n1\n1\n1\n1156424231\n1102098349\n2208\n0\n28248\n1\n-1090034562\n391667932\n1\n1\n1\n1\n1156429743\n1102393783\n59594\n0\n1228318\n1\n0\n0\n1\n1\n1\n1\n1156429743\n1102393783\n59594\n0\n53059\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n493372241\n0\n0\n1383\n197\n6205\n22\nFiles//A1\nF\nil\n\n\n1133053433\n1102435101\n2559\n0\n32062\n1\n-1841237300\n-413394956\n1\n1\n1\n1\n1133042566\n1102219953\n1325\n0\n16486\n1\n926199853\n962368627\n1\n1\n1\n1\n1133063386\n1102648999\n40484\n0\n566360\n1\n0\n0\n1\n1\n1\n1\n1133063386\n1102648999\n40484\n0\n20078\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n2138917700\n0\n0\n223\n115\n2243\n13\nFiles//A1\nF\nil\n\n\n1146917648\n1102574438\n15451\n0\n199489\n1\n9330036\n-2058791451\n1\n1\n1\n1\n1146911794\n1102523540\n2114\n0\n26610\n1\n1844585360\n-208857109\n1\n1\n1\n1\n1146920590\n1102788307\n59663\n0\n1092730\n1\n0\n0\n1\n1\n1\n1\n1146920590\n1102788307\n59663\n0\n45526\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1874985181\n0\n0\n1388\n185\n5324\n22\nFiles//A1\nF\nil\n\n\n1132034886\n1104184846\n15495\n0\n196309\n1\n-614810869\n-1382596637\n1\n1\n1\n1\n1132011269\n1104132015\n2002\n0\n27293\n1\n-1153969394\n-422053358\n1\n1\n1\n1\n1132055122\n1104398386\n60985\n0\n1203448\n1\n0\n0\n1\n1\n1\n1\n1132055122\n1104398386\n60985\n0\n51501\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-199422534\n0\n0\n1366\n190\n6023\n22\nFiles//A1\nF\nil\n\n\n1148738278\n1104800157\n2225\n0\n30448\n1\n336334840\n-1722049420\n1\n1\n1\n1\n1148727247\n1104737582\n2088\n0\n27703\n1\n-1264850604\n711461030\n1\n1\n1\n1\n1148740848\n1105013572\n4811\n0\n52665\n1\n0\n0\n1\n1\n1\n1\n1148740848\n1105013572\n4811\n0\n1513\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1664716139\n0\n0\n212\n193\n209\n19\nFiles//A1\nF\nil\n\n\n1154128511\n1104521200\n16433\n0\n201733\n1\n-1224775220\n-434514866\n1\n1\n1\n1\n1154122887\n1104494251\n3611\n0\n47364\n1\n317673209\n429097738\n1\n1\n1\n1\n1154128747\n1104734672\n14749\n0\n226132\n1\n0\n0\n1\n1\n1\n1\n1154128747\n1104734672\n14749\n0\n7379\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-510452285\n0\n0\n1404\n330\n895\n29\nFiles//A1\nF\nil\n\n\n1121352736\n1105125885\n4568\n0\n58170\n1\n1999822347\n898421215\n1\n1\n1\n1\n1121307392\n1105088368\n1294\n0\n16234\n1\n-983333522\n-1739258039\n1\n1\n1\n1\n1121397014\n1105339233\n6723\n0\n93542\n1\n0\n0\n1\n1\n1\n1\n1121397014\n1105339233\n6723\n0\n2880\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1329047872\n0\n0\n405\n113\n370\n14\nFiles//A1\nF\nil\n\n\n1155909558\n1106777421\n1199\n0\n16460\n1\n821266378\n-1246218943\n1\n1\n1\n1\n1155906397\n1106722413\n2283\n0\n29468\n1\n1183943161\n-426756626\n1\n1\n1\n1\n1155909430\n1106990432\n17653\n0\n332506\n1\n0\n0\n1\n1\n1\n1\n1155909430\n1106990432\n17653\n0\n11300\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-250784161\n0\n0\n115\n205\n1317\n18\nFiles//A1\nF\nil\n\n\n1155824328\n1107578705\n4538\n0\n59342\n1\n1559411272\n1151799898\n1\n1\n1\n1\n1155818790\n1107546496\n4086\n0\n51988\n1\n-919791236\n351277236\n1\n1\n1\n1\n1155824218\n1107685100\n60610\n0\n1258483\n1\n0\n0\n1\n1\n1\n1\n1155824218\n1107685100\n60610\n0\n58400\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n1128053269\n0\n0\n413\n362\n6830\n27\nFiles//A1\nF\nil\n\n\n1155536851\n1108324761\n15984\n0\n204814\n1\n27751286\n468010390\n1\n1\n1\n1\n1155531489\n1108306151\n3837\n0\n49674\n1\n-124177506\n392542407\n1\n1\n1\n1\n1155536799\n1108431004\n27421\n0\n513624\n1\n0\n0\n1\n1\n1\n1\n1155536799\n1108431004\n27421\n0\n17913\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n583326885\n0\n0\n1426\n346\n2034\n29\nFiles//A1\nF\nil\n\n\n1155406742\n1108697607\n2641\n0\n33191\n1\n-1232077645\n-149200003\n1\n1\n1\n1\n1155400871\n1108654290\n2163\n0\n29448\n1\n0\n2050056381\n1\n1\n1\n1\n1155406717\n1108803773\n3061\n0\n42222\n1\n0\n0\n1\n1\n1\n1\n1155406717\n1108803773\n3061\n0\n1392\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1202766431\n0\n0\n231\n205\n167\n19\nFiles//A1\nF\nil\n\n\n1156984613\n1108597201\n1279\n0\n16629\n1\n-1691806559\n-659651601\n1\n1\n1\n1\n1156981428\n1108475694\n2288\n0\n29547\n1\n-763044262\n583186408\n1\n1\n1\n1\n1156984266\n1108703388\n47656\n0\n1013692\n1\n0\n0\n1\n1\n1\n1\n1156984266\n1108703388\n47656\n0\n36156\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1178951951\n0\n0\n116\n206\n4014\n18\nFiles//A1\nF\nil\n\n\n1142762190\n1109171906\n4698\n0\n58567\n1\n1518863941\n-1049339851\n1\n1\n1\n1\n1142752128\n1109138271\n2183\n0\n28897\n1\n1038826580\n-1612340562\n1\n1\n1\n1\n1142765981\n1109277976\n39473\n0\n517176\n1\n0\n0\n1\n1\n1\n1\n1142765981\n1109277976\n39473\n0\n18003\n1\n0\n0\n1\n1\n1\n1\n1\n0\n0\n-1169600992\n0\n0\n408\n201\n2048\n20\nFiles//A1\nF\nil\n\n\n\n\n\n\n\n21.3.4 Trim data to beads we actually used\nThe data may include beads that weren’t used in the assay. Here we used a subset of beads and so need to filter the data\n\na&lt;-subset(a, subset = Bead.ID %in% c(12:15,18,19,20,21,22,25:30))\n\n\n\n21.3.5 Plot data for all beads\nCL1 and CL2 are the bead identification lasers.\n\nggplot(data = a,aes(x=CL1.Peak,y=CL2.Peak,color=factor(Bead.ID)))+geom_point()+\n  scale_x_continuous(trans = \"log\")\n\n\n\n\n\n\n\n\n\n\n21.3.6 Show table of bead counts\n\ntable(factor(a$Bead.ID),a$Bead.Valid)\n\n    \n        1\n  12 1554\n  13 1183\n  14 1482\n  15 1283\n  18 1711\n  19 1607\n  20 1375\n  21 1536\n  22 1567\n  25 1684\n  26 1347\n  27 1775\n  28 1490\n  29 1634\n  30 1150\n\n\n\n\n21.3.7 Threshold classify everything based on automated EM algorithm\nThis also saves a PDF in the directory data/Luminex_LXB_Files/\n\na&lt;-lxb.digital.thresholds(a,pdf.out = T,sd.threshold = 3)\n\nnumber of iterations= 14 \n\n\nnumber of iterations= 63 \n\n\nnumber of iterations= 36 \n\n\nnumber of iterations= 33 \n\n\nnumber of iterations= 55 \n\n\nnumber of iterations= 36 \n\n\nnumber of iterations= 21 \n\n\nnumber of iterations= 34 \n\n\nnumber of iterations= 39 \n\n\nnumber of iterations= 52 \n\n\nnumber of iterations= 36 \n\n\nnumber of iterations= 20 \n\n\nnumber of iterations= 498 \n\n\nnumber of iterations= 104 \n\n\nnumber of iterations= 19 \n\n\n# A tibble: 15 × 3\n   Bead.ID     n EM.threshold\n     &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;\n 1      12  1554       82664.\n 2      13  1183       27482.\n 3      14  1482       18306.\n 4      15  1283       13884.\n 5      18  1711       49013.\n 6      19  1607       11781.\n 7      20  1375        8491.\n 8      21  1536       10624.\n 9      22  1567       22790.\n10      25  1684       21194.\n11      26  1347       14887.\n12      27  1775       95263.\n13      28  1490        8799.\n14      29  1634       12463.\n15      30  1150       90940.",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html#summarise-based-on-bead-counts",
    "href": "Digital_Luminex.html#summarise-based-on-bead-counts",
    "title": "21  Digital Luminex",
    "section": "21.4 Summarise based on bead counts",
    "text": "21.4 Summarise based on bead counts\n\naa&lt;-a %&gt;% group_by(Bead.ID,well) %&gt;% summarise(count=n(),bead.pos=sum(classification==1),bead.neg=sum(classification==0),bead.fraction=(bead.pos=sum(classification==1)/n()))\naa$col&lt;-substr(aa$well,start = 1,stop = 1)\naa$row&lt;-substr(aa$well,start = 2,stop = 3)\n\n\nkable(head(aa,24))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBead.ID\nwell\ncount\nbead.pos\nbead.neg\nbead.fraction\ncol\nrow\n\n\n\n\n12\nFiles//A1\n70\n0\n70\n0.0000000\nF\nil\n\n\n12\nFiles//A10\n161\n0\n161\n0.0000000\nF\nil\n\n\n12\nFiles//A11\n139\n0\n139\n0.0000000\nF\nil\n\n\n12\nFiles//A12\n134\n0\n134\n0.0000000\nF\nil\n\n\n12\nFiles//A2\n119\n0\n119\n0.0000000\nF\nil\n\n\n12\nFiles//A3\n139\n0\n139\n0.0000000\nF\nil\n\n\n12\nFiles//A4\n119\n0\n119\n0.0000000\nF\nil\n\n\n12\nFiles//A5\n135\n0\n135\n0.0000000\nF\nil\n\n\n12\nFiles//A6\n151\n0\n151\n0.0000000\nF\nil\n\n\n12\nFiles//A7\n117\n0\n117\n0.0000000\nF\nil\n\n\n12\nFiles//A8\n141\n0\n141\n0.0000000\nF\nil\n\n\n12\nFiles//A9\n129\n0\n129\n0.0000000\nF\nil\n\n\n13\nFiles//A1\n57\n25\n32\n0.4385965\nF\nil\n\n\n13\nFiles//A10\n119\n0\n119\n0.0000000\nF\nil\n\n\n13\nFiles//A11\n88\n4\n84\n0.0454545\nF\nil\n\n\n13\nFiles//A12\n114\n3\n111\n0.0263158\nF\nil\n\n\n13\nFiles//A2\n102\n35\n67\n0.3431373\nF\nil\n\n\n13\nFiles//A3\n106\n3\n103\n0.0283019\nF\nil\n\n\n13\nFiles//A4\n88\n6\n82\n0.0681818\nF\nil\n\n\n13\nFiles//A5\n87\n4\n83\n0.0459770\nF\nil\n\n\n13\nFiles//A6\n107\n8\n99\n0.0747664\nF\nil\n\n\n13\nFiles//A7\n88\n4\n84\n0.0454545\nF\nil\n\n\n13\nFiles//A8\n114\n76\n38\n0.6666667\nF\nil\n\n\n13\nFiles//A9\n113\n5\n108\n0.0442478\nF\nil",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Digital_Luminex.html#interpretation",
    "href": "Digital_Luminex.html#interpretation",
    "title": "21  Digital Luminex",
    "section": "21.5 Interpretation",
    "text": "21.5 Interpretation\nThe bead.fraction is the fraction of all beads in the well which were above the n SD threshold for positivity. This is a digital count converted in to a fraction, which allows a continuous measure to be gained for each\n\nggplot(aa,aes(well,bead.fraction))+\n  geom_bar(stat=\"identity\")+\n  facet_wrap(.~Bead.ID)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Digital Luminex</span>"
    ]
  },
  {
    "objectID": "Generic_Research_Data_Folder_Design.html",
    "href": "Generic_Research_Data_Folder_Design.html",
    "title": "22  Research study folder template",
    "section": "",
    "text": "22.0.1 Research study\nfs::dir_tree(path = \"data/Generic_Research_Folders/\" , recurse = TRUE)\n\ndata/Generic_Research_Folders/\n├── 01 Study Management\n│   ├── Governance\n│   └── Roles & Responsibilities\n├── 02 Protocol\n├── 03 ICF & Participant info\n│   ├── Flyers and Information\n│   └── Informed Consent Form and Informed Assent Form\n├── 04 Agreements and Contracts\n│   ├── Contracts\n│   ├── Data Transfer Agreements\n│   └── Material Transfer Agreements\n├── 05 Ethics Applications and Letters\n├── 06 Data Management\n│   ├── Case Reporting Forms\n│   ├── Data Management Plan\n│   ├── Data compliance plan\n│   └── Scripts and code\n├── 07 Field Operations\n├── 08 QC, Monitoring, SOPs\n│   ├── Monitoring\n│   ├── Quality Assurance\n│   └── SOPs\n├── 09 Meetings\n├── 10 Steering committee\n├── 11 Reports\n└── 12 Publications",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Research study folder template</span>"
    ]
  },
  {
    "objectID": "Generic_Research_Data_Folder_Design.html#clinical-trial",
    "href": "Generic_Research_Data_Folder_Design.html#clinical-trial",
    "title": "22  Research study folder template",
    "section": "22.1 Clinical trial",
    "text": "22.1 Clinical trial\n\nfs::dir_tree(path = \"data/Generic_Trial_Folders/\" , recurse = TRUE)\n\ndata/Generic_Trial_Folders/\n├── 01 Study Management\n│   ├── Governance\n│   └── Roles & Responsibilities\n├── 02 Protocol\n├── 03 ICF & Participant info\n│   ├── Flyers and Information\n│   └── Informed Consent Form and Informed Assent Form\n├── 04 Agreements and Contracts\n│   ├── Contracts\n│   ├── Data Transfer Agreements\n│   └── Material Transfer Agreements\n├── 05 Ethics Applications and Letters\n├── 06 Data Management\n│   ├── Case Reporting Forms\n│   ├── Data Management Plan\n│   ├── Data compliance plan\n│   └── Scripts and code\n├── 06 Importation\n│   ├── Importation\n│   └── Technical Release\n├── 07 Field Operations\n├── 08 Pharmacovigilance\n├── 09 QC, Monitoring, SOPs\n│   ├── Monitoring\n│   ├── Quality Assurance\n│   └── SOPs\n├── 10 Meetings\n├── 11 External Monitoring\n├── 12 DSMB\n│   ├── DSMB Meeting Minutes\n│   └── Membership\n├── 13 Steering committee\n├── 14 Reports\n└── 15 Publications",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Research study folder template</span>"
    ]
  },
  {
    "objectID": "FPC_Grader.html",
    "href": "FPC_Grader.html",
    "title": "23  FPC Grader",
    "section": "",
    "text": "23.1 Background",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>FPC Grader</span>"
    ]
  },
  {
    "objectID": "FPC_Grader.html#background",
    "href": "FPC_Grader.html#background",
    "title": "23  FPC Grader",
    "section": "",
    "text": "23.1.1 A tool for rapid grading of trachoma using the FPC system\nIntroduction Trachoma is characterised by clinical assessment of conjunctival disease signs. Some researchers collect photographic evidence of clinical disease at the conjunctiva, but grading photographs can become burdensome when there are many hundreds or thousands to process.\nThis tool is designed to simplify the task of grading trachoma photos.\n\n\n23.1.2 FPC Grading\nThe platform implements Dawson’s 1981 system (referred to as the FPC system), which involves assessment of follicles (F), papillae (P) and cicatricae (C) on the conjunctivae.\n\n\n23.1.3 For more details see\nDawson CR, Jones BR, Tarizzo ML, World Health Organization. Guide to trachoma control in programmes for the prevention of blindness. Geneva, Switzerland: World Health Organization; 1981.\nBenefits It provides a system for viewing and recording photo grades without the need to remove the hands from the keyboard. There are no mouse clicks, no need to open photo files, no need to record the results in a file. You simply put the photos to be graded in a folder, run the script and off you go.\nThe system also records which files have previously been graded and won’t allow you to accidentally grade the same file twice. It also keeps track of where you got to, so if you turn it off for a while, when you start it up again, it picks up where you left off.\nOur experienced photograders have informally benchmarked this at around 10 photographs per minute in populations with a low trachoma prevalence.\nThe end result is a tidy data file saved on your computer hard drive. ## Project Link",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>FPC Grader</span>"
    ]
  },
  {
    "objectID": "FPC_Grader.html#project-link",
    "href": "FPC_Grader.html#project-link",
    "title": "23  FPC Grader",
    "section": "23.2 Project Link",
    "text": "23.2 Project Link\nThis project is hosted on a standalone Github Repo - FPC Grader",
    "crumbs": [
      "Laboratory | Field Methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>FPC Grader</span>"
    ]
  },
  {
    "objectID": "ODK_Biometrics.html",
    "href": "ODK_Biometrics.html",
    "title": "24  ODK Biometrics",
    "section": "",
    "text": "24.1 Biometrics Solutions for ODK Ecosystem Tools\nThis project provides an Android app Keppel that interfaces with mobile data collection software of the ODK ecosystem and which allows ISO 19794-2 fingerprint templates to be scanned and/or validated as part of an XLSForm. We also provide a second app, the Keppel CLI, a command line interface which is designed to be run on a computer workstation. Keppel CLI can compare two fingerprint templates and return a matching score and is primarily useful for post-hoc quality assurance and audit.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ODK Biometrics</span>"
    ]
  },
  {
    "objectID": "ODK_Biometrics.html#validation",
    "href": "ODK_Biometrics.html#validation",
    "title": "24  ODK Biometrics",
    "section": "24.2 Validation",
    "text": "24.2 Validation\nThis software has been evaluated in a formal scientific context and the results are published in the following study.\nBiometric linkage of longitudinally collected electronic case report forms and confirmation of subject identity: an open framework for ODK and related tools\n~Chrissy h Roberts, Callum Stott, Marianne Shawe-Taylor, Zain Chaudhry, Sham Lal 1 & Michael Marks\nFront Digit Health. 2023 Aug 4:5:1072331. eCollection 2023.~\nPMID: 37600479 PMCID: PMC10436742 DOI: 10.3389/fdgth.2023.1072331",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ODK Biometrics</span>"
    ]
  },
  {
    "objectID": "ODK_Biometrics.html#features",
    "href": "ODK_Biometrics.html#features",
    "title": "24  ODK Biometrics",
    "section": "24.3 Features",
    "text": "24.3 Features\n\nCapture to XLSForm one or more enrollment templatesA in an XLSForm\nCapture to XLSForm a National Institute of Standards and Technology Fingerprint Image Quality (NFIQ) value, a measure of fingerprint quality\nScan a verification templateB and compare template to one or more previously saved templates for a known individual\nReturn a match probability score indicating likelihood of their being a match between a verification template and one or more enrollment template.\nUsing XLSform design, constrain form progress, content or actions on basis of both NFIQ and match probability.\nDepending on form design, capture an identification templateC to scan a modestly sized database of people, in order to identify the current person.\nCapture to XLSForm the verification or identification templates and verification NFIQ scores\n\nA Enrollment Templates: Created when a user is initially registered in the system. These templates need to be of high quality to ensure reliable future matching. B Verification Templates: Generated when a user attempts to verify their identity. These are compared against enrollment templates for a match. C Identification Templates: Used in systems where one-to-many matching is required, such as law enforcement databases.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ODK Biometrics</span>"
    ]
  },
  {
    "objectID": "ODK_Biometrics.html#project-link",
    "href": "ODK_Biometrics.html#project-link",
    "title": "24  ODK Biometrics",
    "section": "24.4 Project Link",
    "text": "24.4 Project Link\nThis project is hosted on a standalone Github Repo - ODK Biometrics",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>ODK Biometrics</span>"
    ]
  },
  {
    "objectID": "ruODK_basics.html",
    "href": "ruODK_basics.html",
    "title": "25  ruODK Setup and basic functions",
    "section": "",
    "text": "25.1 Background\nruODK is a great package for interacting with ODK Central via R. Full details of how to use it are here.\nTo connect your ODK Central account to R, you’ll need to set up your passwords in the r.environ file so they’re not exposed in scripts. Your passwords will be stored in plain text on your r.environ so be cautious about which machines you work with this way. There’s also options to add you password manually each time, but we’ll assume you’re on a secure machine when you do this.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>ruODK Setup and basic functions</span>"
    ]
  },
  {
    "objectID": "ruODK_basics.html#add-your-credentials-to-r.environ",
    "href": "ruODK_basics.html#add-your-credentials-to-r.environ",
    "title": "25  ruODK Setup and basic functions",
    "section": "25.2 Add your credentials to r.environ",
    "text": "25.2 Add your credentials to r.environ\nRun this command to open the r.environ file\n\nusethis::edit_r_environ(scope = “user”)\n\nInside renviron file, be careful not to delete anything, but add these lines (substituting your ODK central URL and email address in the appropriate places.\n\nODKC_URL=“https://central.xxx.com”\nODKC_UN=“mustafa.orbach@xxx.com”\n\nOn a new line, add the password you use to log in to the system. Substitute xxx for your password\n\nODKC_PW=“xxx”\n\nOn a new line, add your password for decryption (if you use project level encryption). Substitute xxx for your password\n\nODKC_PP=“xxx”\n\nFinally, save the r.environ file and restart R to load it in to R.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>ruODK Setup and basic functions</span>"
    ]
  },
  {
    "objectID": "ruODK_basics.html#using-ruodk-to-download-a-data-set",
    "href": "ruODK_basics.html#using-ruodk-to-download-a-data-set",
    "title": "25  ruODK Setup and basic functions",
    "section": "25.3 Using ruODK to download a data set",
    "text": "25.3 Using ruODK to download a data set\n\n25.3.1 Libraries\n\nlibrary(ruODK)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\n\n\n\n25.3.2 Connect to server\nMy servers are at central.lshtm.ac.uk. You need to change this to yours. Sys.getenv() pulls the passwords from the r.environ file in a way that is more secure than just adding to the script.\n\n#connect to server\nruODK::ru_setup(\n  url = \"https://central.lshtm.ac.uk\",\n  un = Sys.getenv(\"ODKC_UN\"),\n  pw = Sys.getenv(\"ODKC_PW\"),\n  tz = \"Europe/London\",\n  verbose = TRUE\n)\n\n&lt;ruODK settings&gt;\n  Default ODK Central Project ID:  \n  Default ODK Central Form ID:  \n  Default ODK Central URL: https://central.lshtm.ac.uk \n  Default ODK Central Username: chrissy.roberts@lshtm.ac.uk \n  Default ODK Central Password: run ruODK::get_default_pw() to show \n  Default ODK Central Passphrase: run ruODK::get_default_pp() to show \n  Default Time Zone: Europe/London \n  Default ODK Central Version: 2023.4.0 \n  Default HTTP GET retries: 3 \n  Verbose messages: TRUE \n  Test ODK Central Project ID:  \n  Test ODK Central Form ID:  \n  Test ODK Central Form ID (ZIP tests):  \n  Test ODK Central Form ID (Attachment tests):  \n  Test ODK Central Form ID (Parsing tests):  \n  Test ODK Central Form ID (WKT tests):  \n  Test ODK Central URL:  \n  Test ODK Central Username:  \n  Test ODK Central Password: run ruODK::get_test_pw() to show \n  Test ODK Central Passphrase: run ruODK::get_test_pp() to show \n  Test ODK Central Version: 2023.4.0 \n\n\n\n\n25.3.3 Show a list of projects\nYou’ll want to know the pid number, given here as id\n\nkable(ruODK::project_list()[1:2,-9])\n\n\n\n\nid\nname\ndescription\narchived\nkey_id\ncreated_at\nupdated_at\ndeleted_at\nforms\napp_users\ndatasets\nlast_submission\nlast_entity\n\n\n\n\n138\n000_entities_chrissy_roberts\n2024-01-05 : Please review our updated terms of use here\nFALSE\nNA\n2023-10-16T12:20:44.601Z\n2024-01-05T12:46:43.358Z\nNA\n11\n1\n3\n2024-07-15T22:28:02.151Z\n2024-06-14T13:14:04.972Z\n\n\n37\n000_Scratch_Project__Chrissy\nNA\nFALSE\nNA\n2021-06-10T11:43:13.735Z\n2024-11-08T16:09:46.023Z\nNA\n38\n2\n1\n2024-11-08T16:24:25.350Z\n2024-11-08T11:35:50.408Z\n\n\n\n\n\n\n\n25.3.4 Show a list of forms that are included in the project\n\nkable(ruODK::form_list(pid = 37))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproject_id\nxml_form_id\nstate\nenketo_id\nenketo_once_id\ncreated_at\nupdated_at\nkey_id\nversion\nhash\nsha\nsha256\ndraft_token\npublished_at\nname\nsubmissions\nentity_related\nreview_states_received\nreview_states_has_issues\nreview_states_edited\nlast_submission\nexcel_content_type\ncreated_by_id\ncreated_by_type\ncreated_by_display_name\ncreated_by_created_at\ncreated_by_updated_at\ncreated_by_deleted_at\nfid\n\n\n\n\n37\nall-widgets\nopen\nr91ysjaAF4UeES0pS4egTnle70pUKtQ\n3d3e0c6f4dedc4417b2aa86d24074ca957055d55bd58b0fc9d6b452dc2b7e812\n2024-04-17 16:18:08\n2024-11-08 16:09:46\nNA\n2023030101\nf45e83d847b8f82ce494b7961b41c42b\n7a2c9ff9bfbd3598ed4a98391fc79255ab70a046\n3c55c304922dbe8ac48ac1985a9907ded7d3bb5df5d6922bf4d880ff36c6bb54\nNA\n2024-04-17 16:18:55\nAll widgets\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nall-widgets\n\n\n37\nAN_Monitor\nopen\ngGwfHzOgg3dMejAgIZHOU1FcuzyRobK\n11c1c76d466f72e471d12b11c9f5d223a5987a23dade79a42d9ed3db663c51a8\n2023-07-21 15:14:40\n2024-11-08 16:09:46\nNA\n\nbfb550086d060c85ca5c98bd31575537\n42b9d1742e0387b387bebd8ab3f2542f262d92a7\nd3d6f0abb2c8c28af73b75bbc815bc20545056feda7a88526ae7ce7c15d01f99\nNA\n2023-07-21 15:18:26\nAN_Monitor\n33\nFALSE\n32\n0\n1\n2024-04-23T09:53:54.265Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nAN_Monitor\n\n\n37\napplied_epi_ch_21\nopen\ncNtRuf9DyfK0HMAePwCMjX46iqGwZBJ\n608b7ae1bc81c9a479f048b4aef68e83b2c5d31cc80ced9a673de56603021096\n2024-06-19 14:17:30\n2024-11-08 16:09:46\nNA\n20240619153255\n989a84ab642563d0ec8d8ca60f37363f\n80fd7f67f1a0fee937c6648cbabac89dfb38debf\n6b07be42103e49eaf3581bafb4034758bd3d0288f8e44a9bb95e7b50f685ee8e\nNA\n2024-06-19 15:33:17\napplied_epidemiology_chapter_21\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n15\nuser\nChrissy h. Roberts (User)\n2020-10-09 17:44:09\n2020-10-16 12:26:08\nNA\napplied_epi_ch_21\n\n\n37\nBadger-2040-Test-Acquire\nopen\nXO8gzUmvHtUl8DchjYiLbzltj2EaljN\n8b4cbaad7ef0d1c35041f326a23748e41bcd8fcce320f86ce23e765152f5191e\n2023-07-17 13:46:34\n2024-11-08 16:09:46\nNA\n1688347654\naad5b860f0a9b0802746814931600ca3\n2fd1201f2064b45e515c713d422ab5702188120f\n87e283cf98c4b9fb70caf491d7ab955c3d45991cd748227c8bc5a5e1477914ca\nNA\n2023-07-17 13:47:22\nBadger 2040 Test Acquire\n1\nFALSE\n1\n0\n0\n2023-07-17T12:49:07.408Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nBadger-2040-Test-Acquire\n\n\n37\nbarcode\nopen\nJhyeDvlWAsCHf9QHEGBMTHhCxav9Sas\nffb06698afb54d556ad46662c2d2525ba75e998703f5d67dba452f1a70bc652a\n2023-07-17 13:31:00\n2024-11-08 16:09:46\nNA\n\nbf0a782e17e8b5b49118ed29f96016a4\n55988cd4a0cfc3b72f321d507df721d21dabf5e5\n9898f5db30d7bf585f414bf3f0076220cb3ca9b34134d1665cf4a0d494e11f0e\nNA\n2023-07-17 13:31:07\nbarcode\n2\nFALSE\n2\n0\n0\n2023-07-17T12:36:05.736Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nbarcode\n\n\n37\nBook1\nopen\niEJdbcIkiiy4EJRbbrSXZLT5UV3H5Dd\na12692041174d95b2bf77c641c4cc59b06a6118b9f132ff897441acdabf429a7\n2023-10-03 10:45:21\n2024-11-08 16:09:46\nNA\n6\n98a93fbbc315d701ec2a1c00fea9f3e1\n51645db17b4ab4d1b65c1e0802aa184be88415a5\n73363d42cbe6a93b9eab810a172bf60369c588adff289fc484c13acd2aaaadb1\nNA\n2023-10-03 10:45:30\nBook1\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nBook1\n\n\n37\nBook2\nopen\niMFxucJ5QoVlX8Fk58DJ2oa1MDbw1sV\n4904830bafa8a7cb0e4d28fe0311d5e272a9aa0bdac6d32bac3f0c9ec95ca1ff\n2023-10-03 11:21:25\n2024-11-08 16:09:46\nNA\n2\n254cd7fe492ee632cfa3f09f337c8641\nf1807ff6ae156a1f9ac1e178367fc576e9602158\n3267505c477fb1b76a567e1f7958d6b2afadc7eb870d19b2a702bb5c2cfbe4b9\nNA\n2023-10-03 11:22:13\nBook2\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nBook2\n\n\n37\nbsc4\nopen\nujsgQE0BigNyuMsWcfCwZQJo3soXbcH\n758767be4f73ee6f7e5224d71c04b38b199c7f0d6c6da4d173a8ce4870062821\n2023-11-03 09:45:40\n2024-11-08 16:09:46\nNA\n1\n093d3ca75cc80d42bd35a0bb60f30832\n3fe5b79e18603dfe9baa40b3745fb6ae2d393504\n6eafbe8e38f0f70408bf4b003e507939ac8e093979d540261734dc00cb33c4e1\nNA\n2023-11-03 09:45:44\nbotswana spectacle compliance questionnaire\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nbsc4\n\n\n37\ndeja_vu\nopen\nmfAXNBr1eZMZujYooScRiKpr1qAA2x8\ne5b8c3e94accd393038a389a09e013685241610d91df20c4bf1d04aef8bb13ff\n2023-06-09 18:11:30\n2024-11-08 16:09:46\nNA\n\n31bf3382e15635e5e6b026c0d714ced9\nca2f839c483e598349ba94ae30130964ee543906\nc149d95797e9dc82cb81168b66dc0a3ddb5129007168fc2be5d63eb0b255e936\nNA\n2023-06-09 18:12:03\ndeja_vu\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\ndeja_vu\n\n\n37\ndynamic_image\nopen\nW0ce45XegHMKg5hIpu6TvjREmZXDemG\nNA\n2023-06-14 14:05:39\n2024-11-08 16:09:46\nNA\n20230614_2122\n52f43580b65f9dbf2989443ab9ba8e00\nb7cf5d0af2745911b245602e905a1802dcdcb221\n6397c943464b458e1bb2e3cd2186cc8e09b9ff1f3a3b03f8dd7a828ca6afa220\nvpqM\\(DrUXdUXlt3sMxKzuMR3uUlpEIi38qEOtFPZc2BBL1NdXOlR4H1HWSmwnEA6 |NA                  |DynamicImage                                                   |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |dynamic_image                                                  |\n|         37|eREC_Examination_Form                                          |open  |Ba7L32JIuH3z7sqe5a5hoD7BgzZZT9g |e5fc99654afb119fabee544fa85ecb98778963ea1a4b9e7801188f4633da7cfd |2024-02-01 11:31:05 |2024-11-08 16:09:46 |     NA|1.06           |a17a12af3ec584bf52fdd9ccc99165e0 |78eb82f2474c8f4406cecba9624d66ff465f3808 |b788b4b0c30d24b9ef7051dded590682401707c378dc8b2db5986df6df71481c |NA                                                               |2024-06-20 11:49:43 |eREC_Examination_Form                                          |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |eREC_Examination_Form                                          |\n|         37|Consent_Online                                                 |open  |w1HeDySCXDaDttbDMNcHYiKqcsIdEfF |558feac954aec8f6e1ace5a5f8df307f39764450f133030ab2c4493a1c3c598b |2022-10-21 11:25:43 |2024-11-08 16:09:46 |     NA|1              |cf910d1d93c659fe238c0a5d51d0e695 |c183764c1da7991a79bd4f8c53a2346f7863e04c |557fbec6339a464faa53c37ffb66d6af6e4051bcb61436f5a40b0a19474fc57e |NA                                                               |2022-10-21 11:58:58 |Eureka Study                                                   |           3|FALSE          |                      2|                        0|                    1|2023-08-03T13:25:28.961Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |Consent_Online                                                 |\n|         37|filter_test                                                    |open  |drkMaJsm7Z9gPalMktt3XtL1PdjmNOV |4136f44f5c7e37ebb2ac2ebbb99178a82d51fe8dd28068d7046529cf4ec35c22 |2024-03-04 11:17:35 |2024-11-08 16:09:46 |     NA|20240304131939 |873c087b97e1157c45f938cf510c38df |48e55afe196e95af9305a41c0977c1165fa04ace |014841e9d66abbb389d3f7f5a1d7462e72983ced61fec5dda88d7cd7d6a74aa4 |NA                                                               |2024-03-04 13:20:13 |filter_test                                                    |           2|FALSE          |                      2|                        0|                    0|2024-03-04T11:24:50.942Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |filter_test                                                    |\n|         37|form_with_id                                                   |open  |uvFBa4B0ZUFicVILv4CgHUTmCi1wvzb |93ba398d78627670f0cf746b0881a38c0b9b11b84741cd945337c6507291d96b |2023-10-03 10:40:44 |2024-11-08 16:09:46 |     NA|34             |f2a03f1c5f37c4575e618a8926830cd9 |463e75fed8e88b957cac1ade28a23b870dad377d |accbab9816a566ecf7bd643364f4a28fcf8f23bda9b78e09cbbd8389144d841b |NA                                                               |2023-10-03 11:18:06 |form_with_id                                                   |           8|FALSE          |                      8|                        0|                    0|2023-10-03T10:17:32.503Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |form_with_id                                                   |\n|         37|geopoint_map                                                   |open  |aPU4FTAcx124KGvWwtOUJhoVniE1GQT |ecce075b71472afbf97fce27a53c1298114cfbe1c129d16684e2e2e9e70d01a7 |2022-07-21 11:00:49 |2024-11-08 16:09:46 |     NA|               |f0b34efc38940a990396576f4643a008 |2d96e27527c1de28998373e741ed64515d9bf8d5 |c7aaf2141eef6b69a355d4c36af3c46341f876a921ebc5d11c55d26109cea6c2 |NA                                                               |2022-07-21 13:09:42 |geopoint_map                                                   |           4|FALSE          |                      2|                        0|                    2|2023-06-22T09:57:57.153Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |geopoint_map                                                   |\n|         37|geopoint_test                                                  |open  |reXze8unsqqOaOVPuUtuHheIDLYkik3 |e174bc81e0ea052e8d282915d638dbb0bc83085ff03ef7ebd90fb5714f3710f4 |2022-06-16 12:19:45 |2024-11-08 16:09:46 |     NA|               |dacea051c12f44b2241eb51b83157c4c |73083a0de82fde3fad47c11471cc97d6e5d1d1b0 |d2817c391c1c4bfba9a9fe9baed5a6190f784d44aafec8fb3d57011b36e48aa2 |NA                                                               |2022-06-16 12:20:55 |geopoint_test                                                  |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |geopoint_test                                                  |\n|         37|Guinée questionnaire sur l'implémentation 22 Septembre 2022 V5 |open  |nXDh9ihm5xWo4zzIoE3xvh6gQ5m6noy |39dcb9596b208fdc147665f2b40ffe3cdbd3130722e44e2cad977fa258e2f22f |2022-09-22 11:42:24 |2024-11-08 16:09:46 |     NA|               |15bfaa3400d9aefe5cc72171abe2a035 |71f3ca02bbb9a91a29deaeef10de97699db0256c |b13c00b634b6774c32604d9202e558438d8973c41542495eae5c7fd50ac48b79 |NA                                                               |2023-11-03 09:44:41 |Guinée questionnaire sur l'implémentation 22 Septembre 2022 V5 |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |Guinée questionnaire sur l'implémentation 22 Septembre 2022 V5 |\n|         37|indexed_repeat_with_internal_cascading_select                  |open  |wYI980zrjTKRLugyaeVbmfgQgPkO5lA |5bcb31a950f149af7ae1c13fa3938867354586d7c516436295a7025e09b08fce |2022-06-09 08:35:34 |2024-11-08 16:09:46 |     NA|               |e61dbda049489bcff50d5d21603a34f0 |053a5d0dd254a88926c733046d41cfd2687df10c |1b303413342ce3f4361dc06841054146b0ecc7e64deb8d173f9c701e2af080ac |NA                                                               |2022-06-09 08:36:48 |indexed_repeat_with_internal_cascading_select                  |           3|FALSE          |                      1|                        0|                    1|2022-06-09T07:38:12.442Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |indexed_repeat_with_internal_cascading_select                  |\n|         37|keppel_tests                                                   |open  |BESa2FBpym5933l4Cfbei9qVPDakvrJ |593755db76bd56b1e3983c845cc7e52c0d7bfcde8c06af238c1898651984617c |2024-11-11 17:19:36 |2024-11-13 10:35:03 |     NA|20241112121812 |463186c30b0be9bcc8af8f6ed1ca3842 |8bb0f68aae735c3d09b5347e6dfff9986b5d84c9 |fe6452156682d3fc4558703a1659d48ba5e6fd4040b28cb79ce8106374de26de |NA                                                               |2024-11-12 12:18:23 |keppel tests                                                   |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            15|user            |Chrissy h. Roberts (User) |2020-10-09 17:44:09   |2020-10-16 12:26:08   |NA                    |keppel_tests                                                   |\n|         37|match_form                                                     |open  |XJzxpoEZMZpTQCkl1eBrNSxBhGLznPA |e69c60b2afbfa409892618c444608458873cfacb6bb9cda3ddb82485d4f12f8d |2024-10-23 16:10:41 |2024-11-08 16:09:46 |     NA|1              |8cc6f5822793161207cfcde7ebd048ad |d2d507d23a34db3e287f17381140bca539a2ba16 |9bcf1ea6371c0e136654a9d38c097d337932d040306f19f8e2c81be50f74f3fe |NA                                                               |2024-10-23 16:11:02 |Match form                                                     |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            15|user            |Chrissy h. Roberts (User) |2020-10-09 17:44:09   |2020-10-16 12:26:08   |NA                    |match_form                                                     |\n|         37|menstrual_health_survey_17Sept24                               |open  |K83bq1Ot9HaIdqfJjQ3ODNegxBOm7s1 |f3238030e50bb455ed2adf6f1b3820763b04a9accbbf762efecc2c11e70500ab |2024-10-09 10:15:11 |2024-11-08 16:09:46 |     NA|20241209       |b41fd8eda5feaaca0fc3650a4e1f002f |2b889982e9d8510efcb29163af00ae82ac20591e |4e200ff622490e0e16f26fe62ee7a1a5547dd970460f7b653b5104004e921519 |NA                                                               |2024-10-16 10:40:11 |Menstrual Health Survey                                        |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |menstrual_health_survey_17Sept24                               |\n|         37|MNCAH_forms                                                    |open  |WwUhGVUAEBD4hPily3DwuMjXxP1P9eo |NA                                                               |2024-02-02 15:06:19 |2024-11-08 16:09:46 |     NA|2402021527     |f8ac28807f10aebd9d748146a625dbdb |8c80e9c9ca710d71f927aa2399d9329ce99189bd |88e7d1449b90aa602d7159327fcf99e99c9d2e419e88366935ab7aa98b2a8163 |wI!kpscHy5FeqJ1i\\)PhH9mXwChuiev2x3Y49FuZ7aLVDyLXFvWiaxuYdRP7k3nfu\nNA\nMNCAH_forms_DRAFT_240131\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nMNCAH_forms\n\n\n37\nODK XLSForm Template v2023.1\nopen\nEBrgrhBwmZRZTy7JcVyNOags9ncZvaL\n07a4ac52a6bdf3c7a5743b0bdc0586a0405f767c5bc614918a789cc343054556\n2023-11-17 13:36:18\n2024-11-08 16:09:46\nNA\n20231117153234\n21b90a37a21cfeae9024e7db8f843b30\nb0c3cb83e85b66435e002b08cdc254d9cdf51980\n00bd59695d5075616deb7a257bad9fa82ca7623c1c94760116ace7a7ec1e10b9\nNA\n2023-11-17 15:32:50\nODK XLSForm Template v2023.1\n4\nFALSE\n4\n0\n0\n2024-03-04T11:19:17.610Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nODK XLSForm Template v2023.1\n\n\n37\nperiod.diary\nopen\nwn1QkzNaPx5RvYBcwvEPBNOyrPb7LiE\n523e22e3eafa21d4313e53ebedf28e1fd4cafe1508e42c1fe8cb9e2c24f08004\n2022-06-14 22:40:55\n2024-11-08 16:09:46\nNA\n\nd4ce754877a34b1ab1c66f4c0ece4f03\n3dbca6a7253e712218ec4d00ebd4889cdc32f8ac\n961ac4fa9273c7fc28454352f6d48a63265b434b5347076ae4348282e0f0f9a6\nNA\n2023-07-12 14:28:02\nperiod.diary\n2\nFALSE\n0\n0\n2\n2023-07-12T13:28:55.318Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nperiod.diary\n\n\n37\nperson_follow_up\nopen\nmzXgji1PokD8AI8oVyAr7davmoLGdfr\n7f3b895b809865e01b2510c9d4f8813e32a94d19669451fe3bf51aa9e89b9cf1\n2024-11-05 11:17:26\n2024-11-08 16:09:46\nNA\n20241105025537\n8b77b29a1c4e2ae705e17df2e1fd0457\ncbd18d95908d3995435eed7f2992c1dafc565eda\nedcea2ab87c5214969e8dc478d3a142cbb230b28194297b6b2d32d60e85d8afe\nNA\n2024-11-05 11:17:33\nPerson follow-up\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n15\nuser\nChrissy h. Roberts (User)\n2020-10-09 17:44:09\n2020-10-16 12:26:08\nNA\nperson_follow_up\n\n\n37\nperson_registration\nopen\nPCgzvMEFLPMTF7EwGke7HabFnQ8kzM4\n1f7c8702e7890510b43b776095ccd794db74169389bdc2bef9907084be673da9\n2024-11-05 09:47:10\n2024-11-08 16:09:46\nNA\n20241031091402\n952a35e4151198d604c5b8829288ce87\n8cf952656f29ac91d6c76a484f817409df09d212\nb91a3943e2eb8c447a2447f2df324c26169eac20cb37768b7830fb05d44eefc2\nNA\n2024-11-05 09:47:19\nPerson registration\n4\nTRUE\n4\n0\n0\n2024-11-08T11:35:45.404Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n15\nuser\nChrissy h. Roberts (User)\n2020-10-09 17:44:09\n2020-10-16 12:26:08\nNA\nperson_registration\n\n\n37\nprepopulate.example\nopen\n6PjXO9YhLRDcfZdORRWZKApXzzxA1vF\nfe45e799aab8e1b7a21a15ee491bcc8c095e7fc4ecb95345274b47667c3114b5\n2021-06-10 13:26:35\n2024-11-08 16:09:46\nNA\n1\n9518e5bc127b144b11fd469684b3ac80\na8261359f6216499c2a137a7608ae8235966485b\nccdd0dd114efbfc17f88e1ff3cab084385d28ce31cff800704298e294ff75154\nNA\n2021-06-10 13:27:20\nprepopulate.example\n13\nFALSE\n8\n0\n5\n2021-06-10T12:53:25.956Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nprepopulate.example\n\n\n37\nPTT_Log\nopen\nKla8h2fn7fpbcq9YD1ryHFEy8T5CaWM\n3b2528a56ee71b1f2a3e53f39d9c4436ede9b8ad68b25980dff3ac3c6d03487d\n2024-11-08 16:05:19\n2024-11-08 16:09:46\nNA\n20241108160453\nc51194048e762b22be57add34c81eef0\n4d1ac72608cc89b9126078592d74c901e0ddeedb\n6e88941b35dc7218b64ff1de58ae95abbd39381b6b581985f81e43dfaae3f872\nNA\n2024-11-08 16:09:28\nPTT_log\n1\nFALSE\n1\n0\n0\n2024-11-08T16:24:25.350Z\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n15\nuser\nChrissy h. Roberts (User)\n2020-10-09 17:44:09\n2020-10-16 12:26:08\nNA\nPTT_Log\n\n\n37\nexpertqre_v20240112\nopen\nRFh837L1OGFDdqqe3ct3fZ5uboO0ZaA\nNA\n2024-01-18 14:52:28\n2024-11-08 16:09:46\nNA\n20240112\n86514ebc3b312a53a53ef01ffc8d0868\na1279a932b5554c96401a67a6eeb752e80340fd4\n925f1a67a2b1765acce7781e9fc55005ddbf2202ef64cb3269ec7aa89d289e1e\nMjx!YQTIhbGNlf3wjgeRVoEnSlWnbEfPR5Fa2S8RQpOlHaPXWvvxs2FS67wfbYLk\nNA\nQuestionnaire – Expert Elicitation: Infections\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n15\nuser\nChrissy h. Roberts (User)\n2020-10-09 17:44:09\n2020-10-16 12:26:08\nNA\nexpertqre_v20240112\n\n\n37\nscan\nopen\nm8ndkVZvroSdbeaZtzd8jIJNNpgcGo8\nNA\n2023-07-07 12:19:41\n2024-11-08 16:09:46\nNA\n\n8cf728f37c1e5a6a57991a55be531002\n6f1f9e9731bf630907fea298780b91a872a71828\n52455f3ba145c05b950ac9877cefe8e471276f7b70b261be88186cc19c97e12b\nh18rXzcd\\(sXAgB5YudHU7tN5PDTWSjYiMSmGSuKaoZXb8Yg7xsNlmATELAemAu9F |NA                  |scan                                                           |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |scan                                                           |\n|         37|socialcontact                                                  |open  |59KbfDuxbnASccLDIO8k1mncP8D6915 |NA                                                               |2022-07-20 11:36:55 |2024-11-08 16:09:46 |   3100|20192310       |0a4bc27525b822f379a5522423107270 |685dd2f797cc3b2c75596ef6ec5032239c68a2b1 |a007fede67456309bf5f49188dcdafc4579d687860c013ff1e938949f2a9a974 |i7K!YgHLrOSCLwb9qcg8wm8nSOa21TBM5kFGHOB6HEMA0l0UfQbxml4QSZmQl3QB |NA                  |Social_Contact_Survey                                          |           0|FALSE          |                      0|                        0|                    0|NA                       |NA                                                                |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |socialcontact                                                  |\n|         37|states                                                         |open  |dMP1Efe2fCyn9BcW4q3QXDjxLCb97hw |83286e46cfe4b556cb1cd45e91d75bebd26709906f7dcf41aa59024f0861b4a9 |2023-06-13 17:23:48 |2024-11-08 16:09:46 |     NA|2              |fb485b922d4b568d7096e995faf56d7f |2599294b363c40796c2575860e6a94d524d6431c |fbe8dd65167d828a9812346633335f1e7cfc8dce7bb6dab618ba94bab0d38744 |NA                                                               |2023-06-13 17:50:44 |states                                                         |           0|FALSE          |                      0|                        0|                    0|NA                       |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |states                                                         |\n|         37|trigram                                                        |open  |4pl79q7EsZChgSmjsuKhelJDP6BPouv |daca99e4776174022dfc49495ff2b35111c45bb5edd2570bd621ee03724c75c0 |2024-08-23 14:50:50 |2024-11-08 16:09:46 |     NA|20240828111348 |4b4c2b75222680d573f9a2cc0eb71aeb |d172b15e1e0659c44b09569106bdcf57e9ca64b3 |28013cedeed1108d737dc4b9724a4393b01d2294e6a6401647bb832ecbd854d7 |NA                                                               |2024-08-28 11:14:00 |ternary_decision_allocator                                     |          19|FALSE          |                     19|                        0|                    0|2024-08-28T10:16:44.511Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            15|user            |Chrissy h. Roberts (User) |2020-10-09 17:44:09   |2020-10-16 12:26:08   |NA                    |trigram                                                        |\n|         37|test                                                           |open  |gDcJBAuyuOwINi98x7nVT1qUoiICrUx |bf93331e74f7805bfbbba6d184a731cffd696866deeb523812e184b55cf9fae3 |2024-04-23 10:51:15 |2024-11-08 16:09:46 |     NA|               |ac4dd7891104bd77a57ed8138617c01b |8cc85507ca03cde240ebacf374ca02d0532d6b25 |e011f476617520eb8e1dc9eacb87c4ae29e47f2f4130b2b4a37f0711e4c9fac2 |NA                                                               |2024-04-23 10:53:08 |test                                                           |           1|FALSE          |                      1|                        0|                    0|2024-04-23T09:53:54.385Z |application/vnd.openxmlformats-officedocument.spreadsheetml.sheet |            15|user            |Chrissy h. Roberts (User) |2020-10-09 17:44:09   |2020-10-16 12:26:08   |NA                    |test                                                           |\n|         37|iov_v4                                                         |open  |zO5rRQ9qdHzWWCJ7CMIuxi3TNRatfYF |5b9a5a604ad8cd36d753fe7a97742c04ce2496090564fe67983b11c8c9ed7f2e |2023-06-13 11:17:19 |2024-11-08 16:09:46 |     NA|               |1a27a53332476c816f62fbc311377374 |ba5b8619c0369fe60868dfae1dce3c78b3a4a567 |b640ba40e05a7347f223a5e3050e27eefa15d8571e01f26db32db1c5d97ff9fb |NA                                                               |2023-06-13 13:56:14 |Trachoma Grader Exam v4                                        |           0|FALSE          |                      0|                        0|                    0|NA                       |NA                                                                |            19|user            |Chrissy h. Roberts (SA)   |2020-10-13 13:38:03   |2020-10-16 12:26:24   |NA                    |iov_v4                                                         |\n|         37|trachoma_grading_test                                          |open  |vv2zLC5LcjGaE4XuvUVcRfZ8sAcLGFs |NA                                                               |2023-06-13 14:51:03 |2024-11-08 16:09:46 |     NA|               |857e24f535a2e4f7dfc022f201c2c5f3 |2e14abea6e83b9a22b82d34c2a6fdb3d79f0f213 |030c65210ff544a069b0f6266a07169b563e22553b8e5b54b441a3852c8116de |qp1QzD1Zi0FRqiV2wdMPpgVWYB2TSWmSnNLU\\)Uy7OR7tlDwu1bqrS7acCZIJ6eFt\nNA\ntrachoma_grading_test\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\ntrachoma_grading_test\n\n\n37\nVersion 4 MSc ODK FORM\nopen\n1txKNg1kWWsSZ7rBb9H9NlvvxwguIca\nd8bbab33debd16bc5879e707933c50a8c615d816d28245c33f5c085340e2f44f\n2023-10-03 10:28:29\n2024-11-08 16:09:46\nNA\n\n2f1794f1769f7e1bfdc94f7f5d503998\nd94e791991fb2ea82a1d3684798c2c8569b947ec\n0929c85e60f026828cdfaeb6fd103c1302e3cb6e000f34a78abc4736afaaf82e\nNA\n2023-10-03 10:28:33\nVersion 4 MSc ODK FORM\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nVersion 4 MSc ODK FORM\n\n\n37\nZim_Photos\nopen\nQmh4mHegPlAi65mGG8E6cg9npzsPtAx\nbbcb36e9fe0ed7cca8982cc4ca8f12ca432615ee63899d36568bb9c867201810\n2022-09-09 10:48:36\n2024-11-08 16:09:46\nNA\n\n118c629659e9e59d18bcbdf248019d16\n667a2e0e3e795282b18bbe30f813e8458c169377\n0bdf80ebc607cff4806ba0019092e102d203691504cab25672aac6dc9a8901fe\nNA\n2022-09-09 10:48:54\nZim_Photos\n0\nFALSE\n0\n0\n0\nNA\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\nZim_Photos\n\n\n\n\n\n\n\n25.3.5 Show details about a specific form\n\nkable(ruODK::form_detail(pid = 37,fid = \"geopoint_map\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nfid\nversion\nstate\nsubmissions\ncreated_at\ncreated_by_id\ncreated_by\nupdated_at\npublished_at\nlast_submission\nhash\n\n\n\n\ngeopoint_map\ngeopoint_map\n\nopen\n4\n2022-07-21T10:00:49.605Z\n19\nChrissy h. Roberts (SA)\n2024-11-08T16:09:46.065Z\n2022-07-21T12:09:42.248Z\n2023-06-22T09:57:57.153Z\nf0b34efc38940a990396576f4643a008\n\n\n\n\n\n\n\n25.3.6 Show a list of submissions to a specific form\n\nkable(ruODK::submission_list(pid = 37,fid = \"geopoint_map\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstance_id\nsubmitter_id\ndevice_id\ncreated_at\nupdated_at\nreview_state\nuser_agent\nsubmitter_id_2\nsubmitter_type\nsubmitter_display_name\nsubmitter_created_at\nsubmitter_updated_at\nsubmitter_deleted_at\ncurrent_version\n\n\n\n\nuuid:e6fe41e0-d9b3-49bf-8562-962e883f6b18\n19\nNA\n2023-06-22 10:57:57\n2023-07-06 11:46:52\nedited\nEnketo/5.0.2 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15\n19\nuser\nChrissy h. Roberts (SA)\n2020-10-13 13:38:03\n2020-10-16 12:26:24\nNA\n19 , 2023-07-06T10:46:52.783Z , uuid:a3c73d2b-0136-443e-ab9c-d831919245e2, TRUE , 19 , user , Chrissy h. Roberts (SA) , 2020-10-13T12:38:03.905Z , 2020-10-16T11:26:24.184Z\n\n\nuuid:0a671884-27ed-4eb8-9102-310149899678\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:13:51\n2023-07-11 11:54:56\nedited\norg.odk.collect.android/v2022.3-beta.2 Dalvik/2.1.0 (Linux; U; Android 12; Pixel 4a Build/SQ3A.220705.003.A1)\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n19 , 2023-07-11T10:54:56.640Z , uuid:16219910-28f2-407c-90ac-6f758713b176, TRUE , 19 , user , Chrissy h. Roberts (SA) , 2020-10-13T12:38:03.905Z , 2020-10-16T11:26:24.184Z\n\n\nuuid:9f6a1fd4-3394-4e5a-b013-e344272cdf70\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:13:24\nNA\nNA\norg.odk.collect.android/v2022.3-beta.2 Dalvik/2.1.0 (Linux; U; Android 12; Pixel 4a Build/SQ3A.220705.003.A1)\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n1094 , 2022-07-21T12:13:24.100Z , uuid:9f6a1fd4-3394-4e5a-b013-e344272cdf70 , TRUE , collect:v2m4bt3bE9ZfeMfi , org.odk.collect.android/v2022.3-beta.2 Dalvik/2.1.0 (Linux; U; Android 12; Pixel 4a Build/SQ3A.220705.003.A1), 1094 , field_key , chrissy , 2021-11-03T12:44:28.375Z\n\n\nuuid:0a65e09e-18e5-4fd3-bbe8-4c57529ccc89\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:11:19\nNA\nNA\norg.odk.collect.android/v2022.3-beta.2 Dalvik/2.1.0 (Linux; U; Android 12; Pixel 4a Build/SQ3A.220705.003.A1)\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n1094 , 2022-07-21T12:11:19.019Z , uuid:0a65e09e-18e5-4fd3-bbe8-4c57529ccc89 , TRUE , collect:v2m4bt3bE9ZfeMfi , org.odk.collect.android/v2022.3-beta.2 Dalvik/2.1.0 (Linux; U; Android 12; Pixel 4a Build/SQ3A.220705.003.A1), 1094 , field_key , chrissy , 2021-11-03T12:44:28.375Z\n\n\n\n\n\n\n\n25.3.7 Pull submissions\nruODK is very powerful and can get quite complicated. The most basic thing you need is a dataframe with your submissions. Here’s how to get one.\n\ndf&lt;-ruODK::odata_submission_get(pid = \"37\",fid=\"geopoint_map\")\n\n\n\n25.3.8 Show data\n\nkable(df[,1:4])\n\n\n\n\n\n\n\n\n\n\ngeopoint_longitude\ngeopoint_latitude\ngeopoint_altitude\ngeopoint_accuracy\n\n\n\n\n152.528542\n-32.19346\n0\n0\n\n\n-2.929474\n60.74228\n0\n0\n\n\n152.504487\n-32.18187\n0\n0\n\n\n152.526883\n-32.20085\n0\n0",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>ruODK Setup and basic functions</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html",
    "href": "ODK_geofencing.html",
    "title": "26  ODK Geofencing",
    "section": "",
    "text": "26.1 Example : Part 1 - Creating the geofence\nIn practice it wouldn’t make a lot of sense to make a grid for the whole of the Earth at very high resolution, as you’re likely to be working on a more controlled area and the data set would get very big. So let’s take Australia as an example. You’ll find Australia within the bounds of latitude -42.0 to -10,0.01 and longitude 110 to 155.0, so the matrix need only extend to these coordinates.\nAt 0.01 (1.1km) resolution, a decomposed grid of points covering the whole of Australia is about 12 million points.\nThis is the start point for an example where we extract points that fall within a multi-polygon shape file and feed these in to ODK as the basis of a geofence. The shape files used here come from the Significant Urban Areas, Urban Centres and Localities of Australia. These are a bunch of open source shapefiles that draw polygons around major urban areas in Australia. These data are provided by the Australian Bureau of Statistics. They can be accessed here.\nThe first steps are done in R and use the libraries tidyverse and sf.\ndownload.file(\"https://www.abs.gov.au/ausstats/subscriber.nsf/log?openagent&1270055004_sua_2016_aust_shape.zip&1270.0.55.004&Data%20Cubes&1E24D1FB300696D2CA2581B1000E15A5&0&July%202016&09.10.2017&Latest\",destfile = \"data/SUA_2016_AUST.zip\")\n\nunzip(zipfile = \"data/SUA_2016_AUST.zip\",exdir = \"data/\")\nmap = read_sf(\"data/SUA_2016_AUST.shp\")  \nmap &lt;- filter(map,str_detect(map$SUA_NAME16,\"Not in any Significant Urban Area\")==FALSE)",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html#example-part-1---creating-the-geofence",
    "href": "ODK_geofencing.html#example-part-1---creating-the-geofence",
    "title": "26  ODK Geofencing",
    "section": "",
    "text": "Read the polygon shapefile and remove any problematic polygons [here there are some that aren’t in any significant urban area]\n\n\n\n\nNow create a grid of all global points at 0.1 degree resolution. The expand_grid command is useful for making a matrix this way.\n\nglobal.0.1 &lt;- expand_grid(lat = seq(-40.0,-10,0.01), lon = seq(110,150.0,0.01))\n\n\n\n\nConvert the points of the matrix to coordinates compatible with sf objects\n\npnts_sf &lt;- st_as_sf(global.0.1, coords = c('lon', 'lat'), crs = st_crs(map))\n\n\n\n\nFind the intersections of the decomposed matrix points and the template polygons\n\nThis can take a while. In our tests, 12 million points took about 10 minutes to compute\n\n\npnts_sf &lt;- st_as_sf(global.0.1, coords = c('lon', 'lat'), crs = st_crs(map))\n\n\npnts.intersection &lt;- pnts_sf %&gt;% mutate(\n                          intersection = as.integer(st_intersects(geometry, map)), \n                          area = if_else(is.na(intersection), '', map$SUA_NAME16[intersection])\n                                       )\n\n\n\n\nfilter the resulting table to include only data points that are inside polygons\nTidy this up so that lat and lon are in different fields, and provide the name of the area in the third column\n\ninside.polygons&lt;-tibble(filter(pnts.intersection,area!=\"\")) %&gt;%\n  mutate(\n    geometry = as.character(geometry),\n    geometry=str_sub(string = geometry,start = 3,end = str_length(geometry)-1),\n\n    area = as.factor(area)\n  ) %&gt;%\n  separate(geometry, c(\"lon\", \"lat\"), \", \") %&gt;%\n  mutate(\n    lon=format(round(as.numeric(lon), digits=2)) ,\n    lat=format(round(as.numeric(lat), digits=2)) ,\n    key = str_c(lon,lat,sep = \"|\")) %&gt;%\n  select(-intersection)\n\n\n\n\nFinally, export the list to a csv file, which we will use in ODK\n\nwrite_csv(inside.polygons,\"output/geofence.data.csv\")",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html#geofence-in-decomposed-csv-format",
    "href": "ODK_geofencing.html#geofence-in-decomposed-csv-format",
    "title": "26  ODK Geofencing",
    "section": "26.2 Geofence in decomposed CSV format",
    "text": "26.2 Geofence in decomposed CSV format\nThe CSV file we just created for Australia at 0.01 degree resolution had 46,000 points that fell within polygons. The table below is a sample of lines",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html#example-part-2---creating-an-xls-form-that-uses-the-geofence",
    "href": "ODK_geofencing.html#example-part-2---creating-an-xls-form-that-uses-the-geofence",
    "title": "26  ODK Geofencing",
    "section": "26.3 Example : Part 2 - Creating an XLS Form that uses the geofence",
    "text": "26.3 Example : Part 2 - Creating an XLS Form that uses the geofence\nThe XLSForm is very simple. It consists of\n\nA geopoint question, which captures the point that will be tested against the polygons. Here it is a placement-map type, but this works with GPS collected data too.\nA pair of calculations, which extract the first (Latitude) and second (Longitude) data points from the geopoint. These are rounded to the same resolution as the polygon data (here 2 decimal places, 0.01 degrees)\nAnother calculation, which concatenates the resolution-matched geopoint data in the same format found in the geofence data set\nA note, which displays the resolution-matched geopoint\nA further calculation, which grabs exactly zero or one line of matching data from the geofence data set, which is sideloaded as a csv file\nA note, which displays the name of the matching polygon",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html#example-part-3---in-action",
    "href": "ODK_geofencing.html#example-part-3---in-action",
    "title": "26  ODK Geofencing",
    "section": "26.4 Example : Part 3 - In action",
    "text": "26.4 Example : Part 3 - In action\nThis image shows a negative result, with the geopoint just outside the limits of the Forster - Tuncurry area\n\nWhilst this one shows a positive result, with the geopoint well inside the limits of Forster - Tuncurry area",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_geofencing.html#performance",
    "href": "ODK_geofencing.html#performance",
    "title": "26  ODK Geofencing",
    "section": "26.5 Performance",
    "text": "26.5 Performance\nThis works fast with 46000 lines (CSV was 1.6 MB) in the geopoint data. We recommend never feeding this a massive data set, but to reduce the geographical scale as you increase the resolution. Remember that XLSForms store GPS data at 6 decimal places, but you aren’t going to get this system to work at that level!\nTo give an example of why not, look at this. Forster - Tuncurry is about 45 km^2. Not a huge area. But as resolution goes up, so the number of points in the matrix expands hugely\n\n\n\n\n\n\n\n\n\nresolution degrees\nGPS decimal places\nresolution (at equator)\npoints in matrix\n\n\n\n\n0.01\n2\n1.1 km\n273\n\n\n0.001\n3\n100 m\n24,926\n\n\n0.0001\n4\n10 m\n2,463,251\n\n\n0.00001\n5\n1 m\n246,020,500\n\n\n\nRemembering that the best resolution the average GPS receiver on a phone will get is about 10 m, we benchmarked performance with 10 metre resolution or 4 decimal places. Conveniently this is also the resolution of the maps that provide the shapefiles.\nThe CSV file is now ~26 MB. Pretty big\nThe form loads on Enketo in about 8 seconds.\nThe resolution is also great\nLook at the boundary of the polygon for Forster - Tuncurry. North of the bridge is inside. South is outside. \nIf we place a marker in ODK on the bridge, it detects the point as being inside the polygon\n\nBut about 10 m south, it detects the point as being outside the polygon\n\nOn ODK Collect, the first time you use this form it will pre-load the data from the CSV, which took about 20 seconds. On subsequent uses, it takes no time at all!\n\nsystem(\"rm data/SUA*\")",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>ODK Geofencing</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html",
    "href": "ODK_Pi.html",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "",
    "text": "28 ODK_Pi\nA deployable ODK Briefcase based data hub based on Raspberry Pi Raspbian.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#note",
    "href": "ODK_Pi.html#note",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.1 Note",
    "text": "28.1 Note\nThis is quite outdated and requires access to an old version of ODK Briefcase, which is no longer supported. We can’t vouch for security. Consider this an antique!",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#background",
    "href": "ODK_Pi.html#background",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.2 Background",
    "text": "28.2 Background\nSome of our users are working in areas with little or no internet connection, so a portable hub for aggregating data in the field, which is able to later push data to ODK aggregate is a useful interim/backup device that can also be preconfigured to output CSVs, PDF copies of data and so on for on the ground data needs.\nThe ODK briefcase software provides all the functionality we need for this purpose. The benefits of putting briefcase on to a Raspberry Pi (RPi) are\n\nThe small size and weight of the RPi mean that you could send it in the post, carry it in your pocket and include it as a ‘just in case’ plan B measure for studies where your baggage allowance is only what you can carry.\nDedicated system means you don’t have to ‘donate’ your own laptop to study team for the duration of the work.\nThe RPi costs ~£35 and can be used with a tablet/phone/TV as display and touchscreen/keyboard/mouse as I/O devices\nRuns off pretty much any power source and has low consumption and heat output. Can be run inside a tupperware box, so good off grid, in the rainforest and so on.\nProvides full linux system including encryption/decryption, R, Python, Java etc.\nSD cards can be cloned, meaning that you can take a box of RPis and set up as many hubs as you need if the scale of project grows.\n\nThese instructions should be sufficient to set up a new installation of the Raspbian operating system, then to install any necessary packages and software (including ODK Briefcase). It will also help you to set up the system so that you can work with Android devices via the MTP protocol (which is not especially obvious) and various other tips and tricks that make all these things fit together.\nNothing in this guide is particularly difficult and not novel, but I think this is the first place where everything is listed as a single set of instructions.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#equipment",
    "href": "ODK_Pi.html#equipment",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.3 Equipment",
    "text": "28.3 Equipment\nRaspberry Pi (Tested on R Pi 3 B+)\nSDHC Micro SD card (16 GB tested) formatted to FAT / FAT32\nNoobs https://www.raspberrypi.org/downloads/noobs/",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#r-pi-setup",
    "href": "ODK_Pi.html#r-pi-setup",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.4 R Pi Setup",
    "text": "28.4 R Pi Setup\n\nInstall Noobs as per instructions. Tested on Noobs Lite (Network install)\n\nCopy all files from noobs folder on to SDHC card\n\nPlug SD card in to RPi.\n\nPower on the RPi, connect to Wifi, follow steps to install Raspbian and update\n\nYou should now have a basic RPi installation of raspbian\n\nFiddle around with your wifi settings, bluetooth etc.\n\nOpen Terminal\n\n======= ### Get current date and time from internet at startup\nThe RPi doesn’t have a real time clock onboard, so I’d recommend putting on a clock board if using this in the field. Otherwise you’ll be relying on internet time. For the time being though, let’s stick with the basic board and grab internet time at startup.\nsudo nano /etc/rc.local\n\nAdd the following line to the rc.local file above where it says exit 0\n\ndate -s \"$(wget -qSO- --max-redirect=0 google.com 2&gt;&1 | grep Date: | cut -d' ' -f5-8)Z\"\n\n28.4.1 Update list of packages and remove bloatware\nI’m guessing you don’t want to play Minecraft on this RPi (though it is a great game) so let’s save about 1.5 GB of SD card space and remove bloatware that comes preinstalled on this version of Raspbian.\nsudo apt-get update  \nsudo apt-get upgrade\nsudo apt-get remove --purge wolfram-engine scratch2 libreoffice* scratch minecraft-pi sonic-pi dillo gpicview\n\nselect ‘y’ or ‘Y’ when prompted to remove these packages\nclean up leftover dependencies from removed packages\n\nsudo apt-get clean\nsudo apt-get autoremove\n\n\n28.4.2 Add required packages\nThis will add a bunch of useful things that you need\nsudo apt-get install apt-file\nsudo apt-file update\nsudo apt-get install libcurl4-openssl-dev libssl-dev libxml2 libxml2-dev libgdal-dev usbmount \n\n\n28.4.3 Install Java Development Kit (JDK)\nsudo apt-get install ca-certificates-java  \nsudo apt-get install openjdk-9-jre\n\n\n28.4.4 Get odk briefcase\ncurl --silent \"https://api.github.com/repos/opendatakit/briefcase/releases/latest\" | grep \"browser_download_url\" | sed -E 's/.*\"([^\"]+)\".*/\\1/' |xargs wget\n\n\n28.4.5 Set Jar files to open on double click\nYou probably don’t want to have to start ODK Briefcase from the command line every time you use it, so this will make jar files run by double-clicking them\n\nAdd a java.desktop file to tell OS how to handle .jar files\n\nsudo nano /usr/share/applications/java.desktop \n[[Desktop Entry]\nName=Java\nComment=Java\nGenericName=Java\nKeywords=java\nExec=/usr/bin/java -jar %f\nTerminal=false\nX-MultipleArgs=false\nType=Application\nMimeType=application/x-java-archive\nStartupNotify=true\nClose the ‘nano’ text editor by pressing CTRL & C and then press “Y” to save the new file\nAfter adding this file you should be able to find an entry called Java in the Open file with… dialog when you right click the jar file.\nSelect always open with this program and you are set to go.\nODK Briefcase should now be able to download data and export decrypted data when provided with a private key (.pem).\n\n\n28.4.6 Add support for connecting android devices\nAndroid uses the MTP protocol to connect to linux. This is no doubt safer but is a lot weirder than the old fashioned approach where it just mounted your tablet as a USB drive on your computer. Without a bit of jiggery pokery it’s hard to figure out how to find the folders. The next few steps make it much easier.\nsudo apt-get install gvfs-backends gvfs-bin gvfs-fuse gvfs-daemons  \n\nPlug in and unlock an Android device\nFind generic parent location of mount using\n\nmount | grep 'gvfsd-fuse'\n\nexample\n\n\ngvfsd-fuse on /run/user/1000/gvfs type fuse.gvfsd-fuse (rw,nosuid,nodev,relatime,user_id=1000,group_id=1000)\n\n\nthe key folder here is /run/user/1000/gvfs/ but it might be different on your system\nChange to the root directory and create a symlink to the parent directory\n\ncd ~\nln -s /run/user/1000/gvfs Android_Device\n\nOpen a file manager window and select Edit &gt; Preferences &gt; Volume Management\nUnselect “Show available options for removable media…”\n\nYou should now have a folder called “Android_Device” in the root directory. When a device is connected to the RPi, this folder should automagically get a new subfolder for the Android MTP protocol.\n\nConnect an Android device by USB cable\nUnlock the device\nSelect “Yes” to any query about using USB/MTB for transfer\nIf you navigate to the Android_Device folder you should see a folder called mtp:host… This has the device’s file system inside\nIf you don’t see that folder, press F5 to refresh\nWhen you switch devices, press F5 to update the folder.\n\nODK briefcase should now be able to pull all data from devices via the “Android_Devices” Folder.\nNavigate to it and you’ll see a uniquely named folder under which you should see the internal and SD card folders of your device. Inside one or the other will be your ODK folder. That’s the one that ODK Briefcase can pull instances from.\n\n\n28.4.7 Wireless push of instances from Android to RPi\nYou can send forms to the RPi through sftp using the excellent andFTP\n\n\n28.4.8 If you want to run a shell script by double clicking\nThis can be useful if you have downstream stuff going on, or if you want to write a script to control a batch of ODK briefcase operations from the command line interface (CLI).\n\nCreate a *.desktop file in your /usr/share/applications\nExample - do.stuff.sh\n\nnano /usr/share/applications/do.stuff.desktop\n[Desktop Entry]\nVersion=1.0  \nExec=/home/yourname/bin/do.stuff.sh  \nName=SSH Server  \nGenericName=SSH Server  \nComment=Run the do stuff script  \nEncoding=UTF-8  \nTerminal=true  \nType=Application  \nCategories=Application;Network;\nUse open with… and set as default and then script will run on double click\n\n\n28.4.9 Install R and pandoc\nUseful for analysis (R) and reporting (Pandoc) sudo apt-get install r-base r-base-dev pandoc pandoc-citeproc\n\n\n28.4.10 Change priority of WIFI networks\nThis can be useful if you want to control the precedence of networks joined over wifi.\nsudo nano /etc/wpa_supplicant/wpa_supplicant.conf\nAdd priority=2 to the wifi_B block and priority=1 to the wifi_A block in the file. Higher number is higher priority so here wifi_B will be joined by preference\nnetwork={  \n    ssid = \"wifi_A\"  \n    psk = \"passwordOfA\"  \n    priority = 1  \n}  \nnetwork={  \n   ssid = \"wifi_B\"  \n   psk = \"passwordOfB\"  \n   priority = 2  \n}",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#for-uk-academics-on-eduroam",
    "href": "ODK_Pi.html#for-uk-academics-on-eduroam",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.5 For UK academics on Eduroam",
    "text": "28.5 For UK academics on Eduroam\nEduroam is a UK wide wifi provider for cross-institutional working. It can be hard to set up on RPi, but instructions here should be helpful. Below worked for me.\nsudo nano /etc/wpa_supplicant/wpa_supplicant.conf\nctrl_interface=/var/run/wpa_supplicant\nctrl_interface_group=netdev\n\ncountry=GB\n\nnetwork={\nssid=\"eduroam\"\nkey_mgmt=WPA-EAP\neap=PEAP\nphase2=\"auth=MSCHAPV2\"\nidentity=\"username@youruniversity.ac.uk\"\nanonymous_identity=\"anonymous@youruniversity.ac.uk\"\npassword=\"pwd\"\n}",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#hotswitching-between-networks",
    "href": "ODK_Pi.html#hotswitching-between-networks",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.6 Hotswitching between networks",
    "text": "28.6 Hotswitching between networks\nLinux provides a nice easy way to switch between networks. A simple sh script would provide a nice clickable way to switch via a GUI, but on RPi you get the added bonus that you can connect physical switches to the pins. A push button could be used to switch between wifi networks (i.e. to move from a fully online network to an offline local wifi network provided by a hotspot. The latter is very useful if you want to use VNC software to allow using an Android tablet as a screen and interface to RPi whilst in the field.\n\nTo shift from one network to another\n\nwpa_cli select_network 0 or number of network is all you need to do.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#set-up-github",
    "href": "ODK_Pi.html#set-up-github",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.7 Set up github",
    "text": "28.7 Set up github\nGithub provides a nice way to get updated analysis scripts on to the RPi automatically. Git is built in to the Raspbian OS, so you just need to connect to GitHub to get your scripts\ngit config --global user.name \"USERNAME\"  \ngit config --global user.email \"you@youremail.com\"  \ngit config --global core.editor nano\nYou could add a git pull script at startup to ensure that all scripts are updated on startup. Alternatively you could set up a cron task.\nTo run a script after login\nsudo nano /etc/profile\nAdd the following line to the end of the file\n./home/pi/your_script_name.sh",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#dropbox",
    "href": "ODK_Pi.html#dropbox",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.8 Dropbox",
    "text": "28.8 Dropbox\nDropbox is a convenient way to get data back off an RPi, though not recommended for sensitive data (at the very least you should encrypt the data).\nDropbox integration for RPi is not great, but Dropbox Uploader is an excellent script for moving stuff to and from Dropbox. These instructions are also very useful.\n\nVisit Dropbox Uploader follow the instructions in the README.MD for the most up to date instructions.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#backup-your-sd-card-so-that-you-can-use-it-elsewhere-or-recover-from-mishaps",
    "href": "ODK_Pi.html#backup-your-sd-card-so-that-you-can-use-it-elsewhere-or-recover-from-mishaps",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.9 Backup your SD card so that you can use it elsewhere or recover from mishaps",
    "text": "28.9 Backup your SD card so that you can use it elsewhere or recover from mishaps\nBEWARE that you have to use the right mount point (in the example I used /dev/disk3). If you try to write to the wrong disk when restoring the SD card, then you will probably brick your system.\nRun df to see what mounts you have. Identify which one is the disk you want to restore to.\nTo clone RPi SD Card sudo dd if=/dev/disk3 of=/Volumes/RPi_Backups/name bs=1m\nTo restore RPi SD Card sudo dd if=/Volumes/RPi_Backups/name of=/dev/disk3 bs=1m",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "ODK_Pi.html#add-a-pitft-screen",
    "href": "ODK_Pi.html#add-a-pitft-screen",
    "title": "27  ODK Pi - A deployable version of ODK on a Raspberry Pi",
    "section": "28.10 Add a PiTFT screen",
    "text": "28.10 Add a PiTFT screen\nTested using the AdaFruit Assembled 480x320 3.5” TFT+ Touchscreen (Resistive)\n\nTo install the screen\n\nShut down the RPi\nPlug the screen on to the GPIO pins of the RPi\nBoot up to Raspbian\n\n\ncd ~\nwget https://raw.githubusercontent.com/adafruit/Raspberry-Pi-Installer-Scripts/master/adafruit-pitft.sh\nchmod +x adafruit-pitft.sh\nsudo ./adafruit-pitft.sh\nThe script will attempt to install the screen automatically. When prompted to select the configuration, choose the screen you are installing, for instance &gt; 4. PiTFT 3.5” resistive touch (320x480)\nNext you need to select the screen rotation you want, for instance &gt; 1. 90 Degrees (Landscape)\nThe next two steps determine the best setting for the screen\n\nWould you like the console to appear on the PiTFT display?\n\nThis option is good if you want to use the screen as a console window. There’s no HDMI output with this config and to be honest I think it is not what most people want, so select No to this one.\n\nWould you like the HDMI display to mirror to the PiTFT display?\n\nThis option will mirror the HDMI output to the PiTFT screen, so is the better choice for most people. Select Yes to this option.\nAfter rebooting, the screen should start working, with some fairly clunky but functional touch support. (Hint : Use a pointy bit of plastic to get screen to work as a mouse)\n\n28.10.1 Set up screen to work better\n\nOpen terminal then go to Edit menu, followed by Preferences.\nChange the font to something a bit clearer like Monospace 14 Bold\n\nAdd some commands to the bin folder to allow hotswitching of the screen (saves power if you don’t need it on).\n\nMake a command for turning screen off\n\nsudo nano /usr/bin/scroff\nAdd this line to the file\nsudo sh -c 'echo \"0\" &gt; /sys/class/backlight/soc\\:backlig\nht/brightness'\npress CTRL + C and Y to save the file and close nano\n\nMake a command for turning screen on\n\nsudo nano /usr/bin/scron\nAdd this line to the file\nsudo sh -c 'echo \"1\" &gt; /sys/class/backlight/soc\\:backlig\nht/brightness'\npress CTRL + C and Y to save the file and close nano\nMake both commands executable\nsudo chmod +x /usr/bin/scroff\nsudo chmod +x /usr/bin/scron\nThen test that they work\nscroff\nshould turn the screen off and scron should turn it on again.\n\nAdd a software keyboard\nQuite hard to use on tiny screen, but could be useful in a pinch\n\nsudo apt-get install matchbox-keyboard\nsudo apt-get install libmatchbox1 -y \nsudo nano /usr/bin/toggle-matchbox-keyboard.sh\nAdd this text to the toggle-matchbox-keyboard.sh file.\n#!/bin/bash\n#This script toggle the virtual keyboard\n\nPID=`pidof matchbox-keyboard`\nif [ ! -e $PID ]; then\n  killall matchbox-keyboard\nelse\n matchbox-keyboard&\nfi\nThen close file and make executable\nsudo chmod +x /usr/bin/toggle-matchbox-keyboard.sh\nKeyboard can then be accessed through menu &gt; Accessories &gt; Keyboard",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>ODK Pi - A deployable version of ODK on a Raspberry Pi</span>"
    ]
  },
  {
    "objectID": "Balanced_Prioritisation.html",
    "href": "Balanced_Prioritisation.html",
    "title": "28  Balanced Prioritisation Tools For One Health",
    "section": "",
    "text": "One Health is an interdisciplinary approach that recognizes the interconnectedness of human, animal, and environmental health.\nWe created these tools to support our One Health research and in particular as part of our ‘Oneography’ study in Rokupr, Sierra Leone. A neologism, Oneography is an approach which uses descriptive, analytical and statistical methods to explore the perspectives of many stakeholders within the One Health framework. This study integrates both qualitative and quantitative approaches and aims to understand the relative prioritisations of One Health pillars in different sectors of a place, community or society.\nThe research instrument presented here is a ‘game’ for one or more player. This includes printed aluminium gameboards and 3D printed play-pieces.\nThe main purpose is to provide a framework for researchers to have conversations with stakeholders by using a tool which forces them to make a choice that balances resources between the various pillars of one health or actors involved in governance of One Health. The player(s) move one or more pawns around the board, and movement towards one corner shows their heightened valuation of that ‘pillar’. This comes at a cost to the other two aspects. The final placement of the pawn is recorded using an image-map within and ODK form. In keeping with our Oneography method, the process of discussion, movement and collaboration when making judgement decisions is as much a part of the research as the quantitative measure of the final pawn placements.\nCore to the function of the quantitative aspect is the use of an SVG ‘image-map’ which is an interactive image that allows pawn placements on the real-world game board to be recorded on a digital twin within an ODK XLSForm.\n\nThis project is in a separate repo https://github.com/chrissyhroberts/Balanced_Prioritisation_Tool_For_OneHealth",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Balanced Prioritisation Tools For One Health</span>"
    ]
  },
  {
    "objectID": "XLSForm_Dictionary.html",
    "href": "XLSForm_Dictionary.html",
    "title": "29  XLSForm Data Dictionary",
    "section": "",
    "text": "This external repo provides a python-based system for converting XLSForms into a human readable html output data dictionary.\nIt’s a work in prgress but core functions are OKish.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>XLSForm Data Dictionary</span>"
    ]
  },
  {
    "objectID": "CSV_populate_Central.html",
    "href": "CSV_populate_Central.html",
    "title": "30  Upload CSV file to ODK Central",
    "section": "",
    "text": "An External Repo which provides functions to send data from a CSV file to an ODK Central project. This is especially useful for pre-populating new forms and also for creating dummy data sets.",
    "crumbs": [
      "ODK",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Upload CSV file to ODK Central</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html",
    "href": "Docker_install_ubuntu.html",
    "title": "31  Install Docker",
    "section": "",
    "text": "31.1 Example\nIn this example, we will show how to create a minimal installation of Ubuntu, link it to a folder on your host system so that you can move files back and forth, then install OpenBUGS, a stats program that runs only on Linux or Windows.",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#create-folders",
    "href": "Docker_install_ubuntu.html#create-folders",
    "title": "31  Install Docker",
    "section": "31.2 Create folders",
    "text": "31.2 Create folders\nFigure out what folder you want to share between your mac and the linux container\nI’m using\n/Users/icrucrob/Documents/Datasets/docker_shared\nto create a folder, start a terminal, navigate to wherever you want with cd and then use mkdir\ncd /Users/icrucrob/Documents/Datasets/mkdir\ndocker_shared",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#create-specification-for-a-docker-container-dockerfile",
    "href": "Docker_install_ubuntu.html#create-specification-for-a-docker-container-dockerfile",
    "title": "31  Install Docker",
    "section": "31.3 Create specification for a docker container (dockerfile)",
    "text": "31.3 Create specification for a docker container (dockerfile)\ncd /Users/icrucrob/Documents/Datasets/\nnano docker_minimal\nPaste the following text in to nano, then press CTRL + x followed by y and then enter to save the dockerfile and quit nano.\nFROM ubuntu:20.04\n\n# Update package repositories and install packages\nRUN apt-get update && \\\n    apt-get clean && \\\n    rm -rf /var/lib/apt/lists/*",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#build-the-docker-container",
    "href": "Docker_install_ubuntu.html#build-the-docker-container",
    "title": "31  Install Docker",
    "section": "31.4 Build the docker container",
    "text": "31.4 Build the docker container\ndocker buildx build -t minimal-ubuntu -f ./docker_minimal .\nIf successful, you should see something like this\n[+] Building 9.5s (7/7) FINISHED docker:desktop-linux =&gt; [internal] load .dockerignore 0.0s \n=&gt; =&gt; transferring context: 2B 0.0s =&gt; [internal] load build definition from docker_minimal 0.0s \n=&gt; =&gt; transferring dockerfile: 574B 0.0s =&gt; [internal] load metadata for docker.io/library/ubuntu:20.04 1.6s \n=&gt; [auth] library/ubuntu:pull token for registry-1.docker.io 0.0s \n=&gt; [1/2] FROM docker.io/library/ubuntu:20.04@sha256:ed4a42283d9943135ed8 3.5s \n=&gt; =&gt; resolve docker.io/library/ubuntu:20.04@sha256:ed4a42283d9943135ed8 0.0s \n=&gt; =&gt; sha256:bf40b7bc7a11b43785755d3c5f23dee03b08e988b32 2.30kB / 2.30kB 0.0s \n=&gt; =&gt; sha256:96d54c3075c9eeaed5561fd620828fd6bb5d80eca 27.51MB / 27.51MB 1.7s \n=&gt; =&gt; sha256:ed4a42283d9943135ed87d4ee34e542f7f5ad9ecf2f 1.13kB / 1.13kB 0.0s \n=&gt; =&gt; sha256:218bb51abbd1864df8be26166f847547b3851a89999ca7b 424B / 424B 0.0s \n=&gt; =&gt; extracting sha256:96d54c3075c9eeaed5561fd620828fd6bb5d80ecae7cb25f 1.7s \n=&gt; [2/2] RUN apt-get update && apt-get clean && rm -rf /var/lib/ 4.3s \n=&gt; exporting to image 0.0s \n=&gt; =&gt; exporting layers 0.0s \n=&gt; =&gt; writing image sha256:8d6b3dce84d6cf918e6ca8f2ec806ddd3aa1a018f8447 0.0s \n=&gt; =&gt; naming to docker.io/library/minimal-ubuntu 0.0s",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#start-the-container",
    "href": "Docker_install_ubuntu.html#start-the-container",
    "title": "31  Install Docker",
    "section": "31.5 Start the container",
    "text": "31.5 Start the container\ndocker run -it -v /Users/icrucrob/Documents/Datasets/docker_shared:/chrissy minimal-ubuntu\nYou should now be able to start and stop the container using the GUI of the docker dashboard. Access the terminal through the exec tab.\nAny files you put in the shared folder will persist on both machines.",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#install-dependencies",
    "href": "Docker_install_ubuntu.html#install-dependencies",
    "title": "31  Install Docker",
    "section": "31.6 Install dependencies",
    "text": "31.6 Install dependencies\nThis should be done in the docker container’s terminal\napt update\napt upgrade\napt install curl\napt install cmake\napt install gcc\napt install gcc-multilib\napt install man-db\napt install gawk",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#get-the-openbugs-source",
    "href": "Docker_install_ubuntu.html#get-the-openbugs-source",
    "title": "31  Install Docker",
    "section": "31.7 Get the OpenBUGS source",
    "text": "31.7 Get the OpenBUGS source\ncurl https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/2018/04/OpenBUGS-3.2.3.tar.gz –output OpenBUGS-3.2.3.tar.gz",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#install-openbugs",
    "href": "Docker_install_ubuntu.html#install-openbugs",
    "title": "31  Install Docker",
    "section": "31.8 Install OpenBUGS",
    "text": "31.8 Install OpenBUGS\ntar -xvzf OpenBUGS-3.2.3.tar.gz\ncd OpenBUGS-3.2.3\n./configure\nmake\nmake install\nunminimize",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Docker_install_ubuntu.html#check-the-installation",
    "href": "Docker_install_ubuntu.html#check-the-installation",
    "title": "31  Install Docker",
    "section": "31.9 Check the installation",
    "text": "31.9 Check the installation\nOpenBUGS\nIf it is working, you should see the OpenBUGS prompt\nOpenBUGS version 3.2.3 rev 1012\ntype 'modelQuit()' to quit\nOpenBUGS&gt;",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Install Docker</span>"
    ]
  },
  {
    "objectID": "Join_photos_randomly.html",
    "href": "Join_photos_randomly.html",
    "title": "32  Join Photos side-by-side with tracked random arrangement",
    "section": "",
    "text": "32.1 Background\nThis python script was written to support a team who needed paired photos to be shown side by side, but with randomisation of which photo went on the left and which on the right in each pair.",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Join Photos side-by-side with tracked random arrangement</span>"
    ]
  },
  {
    "objectID": "Join_photos_randomly.html#inputs",
    "href": "Join_photos_randomly.html#inputs",
    "title": "32  Join Photos side-by-side with tracked random arrangement",
    "section": "32.2 Inputs",
    "text": "32.2 Inputs\nThe inputs are (1) a folder in of image files with unique names\n\nlist.files(\"data/join_photos_randomly/in/\")\n\n[1] \"A.jpg\" \"B.jpg\" \"C.jpg\" \"D.jpg\" \"E.jpg\" \"F.jpg\"\n\n\n\n\n\n\n\n\nand (2) a file photo_pairs_in.csv which lists the pairs and provides a subject identifier.\n\nlibrary(tidyverse)\ndf&lt;-read_csv(\"data/join_photos_randomly/photo_pairs_in.csv\")\n(df)\n\n# A tibble: 3 × 3\n  subject prewash postwash\n    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;   \n1       1 A.jpg   B.jpg   \n2       2 C.jpg   D.jpg   \n3       3 E.jpg   F.jpg",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Join Photos side-by-side with tracked random arrangement</span>"
    ]
  },
  {
    "objectID": "Join_photos_randomly.html#python-script",
    "href": "Join_photos_randomly.html#python-script",
    "title": "32  Join Photos side-by-side with tracked random arrangement",
    "section": "32.3 Python script",
    "text": "32.3 Python script\nCreate a new text document (photo_joiner.py) and save to the same folder as the input csv file.\n##################################################################\n# PHOTO JOINER WITH RANDOMISER\n# This script takes a list of paired photos and joins them\n# Joining is side by side\n# and randomised\n# The script creates a new joined image file in the out folder\n# and also records which image was on the left, which on the right\n#\n# Core code is copyright Chrissy h Roberts\n# Chrissy.Roberts@LSHTM.ac.uk\n# CC-BY 4.0 (https://creativecommons.org/licenses/by/4.0/)\n#\n# Imported libraries are used according to their individual licenses\n# Pillow library used under License: Historical Permission Notice and Disclaimer (HPND) (HPND)\n# Pillow written by Jeffrey A. Clark (Alex)\n##################################################################\n# Import libraries needed for this analysis\n\nimport os\nimport random\nimport csv\nfrom PIL import Image\n\n\n# Input directory containing your photo pairs\ninput_dir = \"in\"\n\n# Output directory for saving the joined images\noutput_dir = \"out\"\n\n# Input CSV file containing subject and photo filenames\ninput_csv_file = \"photo_pairs_in.csv\"\n\n# Output CSV file for logging\noutput_csv_file = \"photo_pairs_out.csv\"\n\n# Set the random seed for reproducible randomness\nrandom_seed = 42  # You can change this seed to any integer you prefer\nrandom.seed(random_seed)\n\n# Create a dictionary to map subjects to photo pairs\nsubject_to_photos = {}\n\n# Read the input CSV file and populate the dictionary\nwith open(input_csv_file, \"r\") as csv_file:\n    csv_reader = csv.DictReader(csv_file)\n    for row in csv_reader:\n        subject = row[\"subject\"]\n        prewash_photo = row[\"prewash\"]\n        postwash_photo = row[\"postwash\"]\n        subject_to_photos[subject] = (prewash_photo, postwash_photo)\n\n# Create a new CSV file for logging with \"left\" and \"right\" columns\ncsv_file = open(output_csv_file, \"w\", newline=\"\")\ncsv_writer = csv.writer(csv_file)\ncsv_writer.writerow([\"subject\", \"left\", \"right\"])\n\n# Process and rename the photos for each subject\nfor subject, (prewash_photo, postwash_photo) in subject_to_photos.items():\n    # Randomly assign which photo is on the left and which is on the right\n    if random.choice([True, False]):\n        left_photo, right_photo = prewash_photo, postwash_photo\n    else:\n        left_photo, right_photo = postwash_photo, prewash_photo\n\n    # Load the left and right photos using PIL\n    left_image = Image.open(os.path.join(input_dir, left_photo))\n    right_image = Image.open(os.path.join(input_dir, right_photo))\n\n    # Concatenate the photos horizontally\n    joined_image = Image.new(\"RGB\", (left_image.width + right_image.width, left_image.height))\n    joined_image.paste(left_image, (0, 0))\n    joined_image.paste(right_image, (left_image.width, 0))\n\n    # Save the joined photo in the output directory\n    joined_photo_filename = f\"{subject}.jpg\"\n    joined_image.save(os.path.join(output_dir, joined_photo_filename))\n\n    # Write the new mapping to the output CSV file\n    csv_writer.writerow([subject, left_photo, right_photo])\n\n    print(f\"Processed subject {subject}: {left_photo} + {right_photo} =&gt; {joined_photo_filename}\")\n\n# Close the output CSV file\ncsv_file.close()\nTo run this script, open a terminal, change directory to the folder and type python3 photo_joiner.py",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Join Photos side-by-side with tracked random arrangement</span>"
    ]
  },
  {
    "objectID": "Join_photos_randomly.html#outputs",
    "href": "Join_photos_randomly.html#outputs",
    "title": "32  Join Photos side-by-side with tracked random arrangement",
    "section": "32.4 Outputs",
    "text": "32.4 Outputs\nThe outputs are saved in a folder (out) in the form of the joined photos\n\n\n\nNote that no correction is made for the sizes, so some clipping will occur if the larger photo is on the right. We recommend using photos that have a consistent size.\nThe data on which photos were on the left or right is stored in the photo_pairs_out.csv file.\n\nlibrary(tidyverse)\ndf&lt;-read_csv(\"data/join_photos_randomly/photo_pairs_out.csv\")\n(df)\n\n# A tibble: 3 × 3\n  subject left  right\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1       1 A.jpg B.jpg\n2       2 C.jpg D.jpg\n3       3 F.jpg E.jpg",
    "crumbs": [
      "Python, bash and other code",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Join Photos side-by-side with tracked random arrangement</span>"
    ]
  },
  {
    "objectID": "badger2040w.html",
    "href": "badger2040w.html",
    "title": "33  Badger 2040w code",
    "section": "",
    "text": "The badger2040w is a wearable e-ink display with a Raspberry Pi 2024W chip. This makes it programmable and modifiable.\nI’ve written some various applications for the Badger, most of which can be found on\nhttps://github.com/chrissyhroberts/badger2040w_code\n\nWorking with Dr Gareth Bestor, we’ve also created a prototype XLSForm interpreter for the badger which has several features of ODK Collect\n\nCreate forms using XLSForm architecture\nStandard survey question types\n\nSelect One\nSelect Multiple\nImage select\nNumeric\nText\n\n\nhttps://github.com/chrissyhroberts/Badger2040_XForm",
    "crumbs": [
      "Miscellany",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Badger 2040w code</span>"
    ]
  },
  {
    "objectID": "comic2pdf.html",
    "href": "comic2pdf.html",
    "title": "34  Comic formats to PDF",
    "section": "",
    "text": "34.1 External Repo\nThis software is hosted on a separate repo at https://github.com/chrissyhroberts/COMICS_CBZ_CBR_to_PDF_Conversion",
    "crumbs": [
      "Miscellany",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Comic formats to PDF</span>"
    ]
  },
  {
    "objectID": "remarkable.html",
    "href": "remarkable.html",
    "title": "35  Remarkable 2 / Paper Pro Template Hacks",
    "section": "",
    "text": "I’m a user of the excellent [Remarkable](remarkable.com) e-ink devices.\nIn most respects the Remarkable does everything it should do, but falls short because it doesn’t allow you to use custom templates.\nThe system is however largely based on open-source software, which makes the remarkable hackable and modifiable.\nThis [external repo](https://github.com/chrissyhroberts/remarkable_2_and_paper_pro_custom_templates) provides some ways to add your own templates and sleep screens to your remarkable.\nLike this sleep screen, which is more fun than the blank screen they ship with.\n\nOr this new template",
    "crumbs": [
      "Miscellany",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Remarkable 2 / Paper Pro Template Hacks</span>"
    ]
  },
  {
    "objectID": "CUPS_Zebra_Printer.html",
    "href": "CUPS_Zebra_Printer.html",
    "title": "36  Zebra ZT220 CUPS Printing",
    "section": "",
    "text": "This External Repo provides a method for printing via CUPS to a Zebra ZT220 printer (And other models most likely).\nIt includes a batch mode which will print stickers from a csv file, with QR Codes.",
    "crumbs": [
      "Miscellany",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Zebra ZT220 CUPS Printing</span>"
    ]
  },
  {
    "objectID": "SayMyName.html",
    "href": "SayMyName.html",
    "title": "37  Say My Name",
    "section": "",
    "text": "This external repo shows how to set up a simple site (i.e. by github pages) which you can include in your email footer and which first lets people hear you say your own name and then redirects to your webpage.\nLike this one does https://chrissyhroberts.github.io/saymyname/\nTo use it yourself, simply clone the repo, add your own audio file named “SayMyName.mp3” and change the link on line 18 to match your profile URL.",
    "crumbs": [
      "Miscellany",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Say My Name</span>"
    ]
  }
]