---
title: "Install Docker "
author: "Chrissy Roberts"
---

# Big data I/O benchmarks

This is a simple vignette which tests out a variety of ways to read and write data files.

For the most part, CSV is preferred data format because it is human readable, but it rapidly becomes unwieldy as the size increases (rows, columns, both). Working with multithreaded I/O interfaces and with binary/database formats can make things much faster.

Here, we simulate a reasonably large data set with 10 million rows and three variables and then test I/O speeds.

These tests include

`read_csv()` and `write_csv()` from tidyverse/dplyr

`fread()` from data.table

`read_parquet()` and `write_parquet()` from arrow package

`read_feather()` and `write_feather()` from arrow package

## Libraries

```{r,warning=F,message=F}
library(tidyverse)
library(data.table)
library(arrow)
library(reticulate)
```

## Create data frame

```{r}

df<-tibble( a = sample(x = c(1:50),10e6,replace=TRUE),
            b = sample(x = c(1:50),10e6,replace=TRUE),
            cc = sample(x = c(1:50),10e6,replace=TRUE)
            )

```

## Benchmarks Write functions

### `write_csv()`

```{r}
gc();system.time(write_csv(df,"data/tmp.csv"))
str_c("size : ",round(file.info("data/tmp.csv")$size/10^6,2)," MB")
```

### `saveRDS`

```{r}
gc();system.time(saveRDS(df,"data/tmp.rds"))
str_c("size : ",round(file.info("data/tmp.rds")$size/10^6,2)," MB")
```

### `write_feather()`

```{r}
system.time(write_feather(df,"data/tmp.feather"))
str_c("size : ",round(file.info("data/tmp.feather")$size/10^6,2)," MB")
```

### `write_parquet()`

```{r}
system.time(write_parquet(df,"data/tmp.parquet"))
str_c("size : ",round(file.info("data/tmp.parquet")$size/10^6,2)," MB")

```

## Benchmark Read functions

### `read_csv()`

```{r,warning=F,message=F}
rm(df);system.time(df<-read_csv("data/tmp.csv"),gcFirst = T)
```

### `fread()`

```{r}
rm(df);system.time(df<-fread("data/tmp.csv"),gcFirst = T)
```

### `read_feather()`

```{r}
rm(df);system.time(df<-read_feather("data/tmp.feather"),gcFirst = T)

```

### `read_parquet()`

```{r}
rm(df);system.time(df<-read_parquet("data/tmp.parquet"),gcFirst = T)
```

### Compare convert csv to parquet in R or python

#### R

```{r,warning=F,message=F}

rm(df)

a<-system.time(df<-read_csv("data/tmp.csv"),gcFirst = T)[3]
b<-system.time(write_parquet(df,"data/tmp.parquet"),gcFirst = T)[3]
a+b


```

#### **Python**

```{python}
import sys
import pandas as pd
import sys
import pyarrow
import time


start = time.time()
df = pd.read_csv("data/tmp.csv")

df.to_parquet("data/tmp.parquet2")
end = time.time()
print(end - start)


```

Running this as a script in the terminal is much faster

use `python3 script.py tmp.csv tmp.parquet`

### Tidy up

```{r}
file.remove("data/tmp.csv", "data/tmp.feather", "data/tmp.parquet", "data/tmp.rds")

```
