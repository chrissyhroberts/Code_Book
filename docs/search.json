[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Code Book",
    "section": "",
    "text": "Preface\nThis code book brings together useful pieces of code that I have created over the years.\nVarious useful things"
  },
  {
    "objectID": "95CI_point_estimate.html#find-the-95-ci-on-a-point-estimate",
    "href": "95CI_point_estimate.html#find-the-95-ci-on-a-point-estimate",
    "title": "1  95% Confidence Intervals on a point estimate",
    "section": "1.1 Find the 95% CI on a point estimate",
    "text": "1.1 Find the 95% CI on a point estimate\nThis simple approach is based on the formula\n\\(p ± z * sqrt((p(1-p))/n))\\)\nWhere\n\\(p\\) = population proportion\n\\(z\\) = zcrit value for 95% confidence level (i.e. 1.96 for a 95% confidence interval)\n\\(n\\) = sample size"
  },
  {
    "objectID": "95CI_point_estimate.html#libraries",
    "href": "95CI_point_estimate.html#libraries",
    "title": "1  95% Confidence Intervals on a point estimate",
    "section": "1.2 Libraries",
    "text": "1.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)"
  },
  {
    "objectID": "95CI_point_estimate.html#section",
    "href": "95CI_point_estimate.html#section",
    "title": "1  95% Confidence Intervals on a point estimate",
    "section": "1.3 ",
    "text": "1.3 \nDefine Some data\n\ndf = tibble(\n            month = 1:10,\n            prevalence = c(0.72,0.62,0.44,0.22,0.17,0.12,0.13,0.09,0.04,0.02)\n            )\n\nkable(df)\n\n\n\n\nmonth\nprevalence\n\n\n\n\n1\n0.72\n\n\n2\n0.62\n\n\n3\n0.44\n\n\n4\n0.22\n\n\n5\n0.17\n\n\n6\n0.12\n\n\n7\n0.13\n\n\n8\n0.09\n\n\n9\n0.04\n\n\n10\n0.02"
  },
  {
    "objectID": "95CI_point_estimate.html#define-a-function-to-calculate-upper-and-lower-confidence-interval",
    "href": "95CI_point_estimate.html#define-a-function-to-calculate-upper-and-lower-confidence-interval",
    "title": "1  95% Confidence Intervals on a point estimate",
    "section": "1.4 Define a function to calculate upper and lower confidence interval",
    "text": "1.4 Define a function to calculate upper and lower confidence interval\n\npoint.estimate.CI <- function(p,z=1.96,n){z * sqrt((p*(1-p))/n)}\n\n\n1.4.1 Capture the upper and lower limit for a given value of n\n\ndf<-df %>% \n  mutate(\n        upper10 = prevalence + point.estimate.CI(prevalence,n = 10),\n        lower10 = prevalence - point.estimate.CI(prevalence,n = 10),\n        upper50 = prevalence + point.estimate.CI(prevalence,n = 50),\n        lower50 = prevalence - point.estimate.CI(prevalence,n = 50),\n        upper1000 = prevalence + point.estimate.CI(prevalence,n = 1000),\n        lower1000 = prevalence - point.estimate.CI(prevalence,n = 1000)\n        ) \n\n\n\n1.4.2 Draw the confindence intervals\nThis chart shows the point estimates (black dots) as well as the 95% confidence intervals obtained when n was 10 (green ribbon), 50 (red ribbon) or 1000 (blue ribbon)\n\nggplot(df,aes(month,prevalence))+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower10,ymax=upper10),alpha=0.4,fill=\"green\")+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower50,ymax=upper50),alpha=0.6,fill=\"red\")+\n  geom_ribbon(aes(x = month,y=prevalence,ymin=lower1000,ymax=upper1000),alpha=0.6,fill=\"blue\")+\n  geom_point()+\n  geom_line()"
  },
  {
    "objectID": "update_R.html#bare-bones-updater-for-r.",
    "href": "update_R.html#bare-bones-updater-for-r.",
    "title": "2  Update R - Bare Bones",
    "section": "2.1 Bare bones updater for R.",
    "text": "2.1 Bare bones updater for R.\nAn annoying feature of R is that updating the R version doesn’t always (if ever) pull all of your installed packages in to the new version.\nUpdater packages for R are also a bit hit and miss.\nThe updateR package is quite popular, but I haven’t had much luck with it.\nIt is here if you want to try it…\n\ndevtools::install_github(“AndreaCirilloAC/updateR”)\n\nMy approach takes the long way round, by creating an installer script for all the currently installed packages. It then saves this script, which you can run once you’ve updated R.\nDoing it this way accounts for weird behaviour where installing packages from within loops or apply commands doesn’t handle dependencies well and leads to lots of packages failing to install.\nThis script contains some code from the updateR package, which has never worked for me on its own.\nThis method can take a while, but in my opinion does a much better job of comprehensively rebuilding your catalogue of libraries\nObviously this only works with CRAN packages, but will spit out a list of packages you need to manually install at the end."
  },
  {
    "objectID": "update_R.html#update-r",
    "href": "update_R.html#update-r",
    "title": "2  Update R - Bare Bones",
    "section": "2.2 Update R",
    "text": "2.2 Update R\n\n2.2.1 Libraries\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(xml2)\nlibrary(rvest)\nlibrary(askpass)\n\n\n\n2.2.2 Define functions\nMost of these are copies or modifications of the updateR functions.\n\n2.2.2.1 list_packages\n\nlist_packages <- function() {\n  all_pkg <- installed.packages() %>%\n    as.data.frame() %>%\n    pull(Package)\n  base_pkg <- installed.packages() %>%\n    as.data.frame() %>%\n    filter(Priority == \"base\") %>%\n    pull(Package)\n  all_pkg[!all_pkg %in% base_pkg]\n}\n\n\n\n2.2.2.2 list available versions\n\nlist_available_versions<-function(){\n  cran <- \"http://cran.rstudio.com/bin/macosx/\"\n  version_regex <- \"(\\\\d+)?\\\\.(\\\\d+)?\\\\.(\\\\d+)?\"\n  page <- read_html(cran)\n  file_url <- page %>%\n    html_nodes(xpath = \"//td/a\")\n  file_url\n}\n\n\n\n2.2.2.3 latest_r_version\nChange ver to match your system requirements. Default is my own need for macOS non arm. (ver = 3)\n\nlatest_r_version <- function(ver=3) {\n  cran <- \"http://cran.rstudio.com/bin/macosx/\"\n  version_regex <- \"(\\\\d+)?\\\\.(\\\\d+)?\\\\.(\\\\d+)?\"\n  page <- read_html(cran)\n  file_url <- page %>%\n    html_nodes(xpath = \"//td/a\") %>% .[ver] %>%\n    html_attr(\"href\") %>%\n    paste(cran, ., sep = \"\")\n  minimal <- page %>%\n    html_nodes(xpath = '//table[1]//tr[1]//td[2]') %>%\n    html_text() %>%\n    trimws() %>%\n    regmatches(., regexpr(\"macOS.*higher.\", .)) %>%\n    regmatches(., regexpr(\"\\\\d+.\\\\d+\", .))\n  r_latest <- regmatches(file_url, regexpr(version_regex, file_url))\n  r_current <- paste(version$major, version$minor, sep = \".\")\n  r_latest_numeric <- as.numeric(sub(\"^(\\\\d)\\\\.\", \"\\\\1\", r_latest))\n  r_current_numeric <- as.numeric(sub(\"^(\\\\d)\\\\.\", \"\\\\1\", r_current))\n  \n  structure(\n    list(update_avail = ifelse(r_latest_numeric > r_current_numeric, T, F),\n         latest = r_latest,\n         url = file_url),\n    current = paste(version$major, version$minor, sep = \".\"),\n    OS_minimal = as.numeric(minimal)\n  )\n}\n\n\n\n2.2.2.4 ask_password\n\nask_password <- function() {\n  askpass::askpass(sprintf(\"Enter password for %s: \", system2(\"whoami\", stdout = TRUE)))\n}\n\n\n\n2.2.2.5 write_package_reinstaller\n\nwrite_package_reinstaller <- function(){\n      # Make a list of currently installed packages\n      packcmds<-NA\n      packages<-list_packages()\n    \n      # Loop through and create an install script\n      for (i in 1:length(packages))\n      {\n      packcmds[i]<-paste(\"install.packages('\",packages[i],\"',dependencies = T)\",sep = \"\")\n      }\n    \n    #write installer script to file\n    write.table(x = packcmds,file = \"packages.install.script.R\",quote = F,row.names = F,col.names=FALSE)\n \n}\n\n\n\n2.2.2.6 update_R\nThis is a modified version of the update_R function from updateR.\n\nupdate_R<-function(force=FALSE,ver)\n{\n  check<-latest_r_version(ver = ver)\n  \n  if(check$update_avail==FALSE){message(\"Current version is up to date, nothing to be done\")}\n  \n  if(check$update_avail==TRUE | force == TRUE)\n  {\n   \n    admin_password <- ask_password()\n    username <- system('whoami', intern = TRUE)\n    command <- paste0(\"echo '\", admin_password, \"' | sudo -S -l\")\n    out <- system(command, intern = TRUE)\n    if (length(out) == 0) {\n      stop(sprintf(\"current user %s does not have admin privileges\", username))\n    }\n    folderpath <- sprintf(\"/Users/%s/Downloads/\",\n                          system2(\"whoami\", stdout = TRUE))\n    pkgfile <- regmatches(check$url, regexpr(\"R.*$\", check$url))\n    fullpath <- sprintf(\"%s%s\", folderpath, pkgfile)\n    # download package, set folder for download\n    message(\"Downloading new version of R\")\n    download.file(check$url, fullpath)\n    \n    message(\"Installinf new version of R\")\n    {\n      message(paste0(\"Installing R-\", check$latest, \"…please wait\"))\n      command <-\n        paste0(\"echo '\",\n               admin_password,\n               \"' | sudo -S installer -pkg \",\n               \"'\",\n               fullpath,\n               \"'\",\n               \" -target /\")\n      system(command, ignore.stdout = TRUE)\n      arg <- paste0(\"--check-signature \", fullpath)\n      system2(\"pkgutil\", arg)\n    }\n  }\n}\n\n\n\n2.2.2.7 reinstall_all_packages\n\nreinstall_all_packages <- function(){\n\n    message(\"Reinstalling packages from source file\")\n    source(\"packages.install.script.R\")\n    \n    new.packages<-list_packages()\n    needinstall<-packages[packages%in%new.packages==FALSE]\n    needinstall<-factor(unique(needinstall))\n    \n    message(needinstall)\n  }\n\n\n\n\n2.2.3 Run the Updater\n\n2.2.3.1 Save the List of packages\n\n#write_package_reinstaller()\n\n\n\n2.2.3.2 Find the appropriate version of R for your machine\n\n#list_available_versions()\n\n\n\n2.2.3.3 Update R\n\n#update_R(ver=3)\n\n\n\n2.2.3.4 Update R\n\n#reinstall_all_packages()"
  },
  {
    "objectID": "utf8_encoding.html#background",
    "href": "utf8_encoding.html#background",
    "title": "3  Normalise utf8 accented text",
    "section": "3.1 Background",
    "text": "3.1 Background\nAccented text can cause problems for R.\nThis method shows how to convert all accents from mixed encodings to UTF-8 text .\nProblems like this often emerge with data shared between PC/Mac/Linux and with data exported from Excel. The simple solution is to use the Encoding() function from the utf8 package.\n\n3.1.1 Libraries\n\nlibrary(utf8)\n\n\n\n3.1.2 Dummy data\nHere we define a vector x, which has accents. To simulate the problem, we set encoding to be mixed between UTF-8 and bytes, but the second entry is actually encoded in Latin byte format with the leading byte 0xE7 rather than in UTF-8.\n\nx <- c(\"fa\\u00E7ile\", \"fa\\xE7ile\", \"fa\\xC3\\xA7ile\")\nEncoding(x) <- c(\"UTF-8\", \"UTF-8\", \"bytes\")\n\nIf we try to convert all entries in the vector to UTF-8, it fails\n\ntry(as_utf8(x))\n\nError in as_utf8(x) : \n  entry 2 has wrong Encoding; marked as \"UTF-8\" but leading byte 0xE7 followed by invalid continuation byte (0x69) at position 4\n\n\nThe simple fix is to change the encoding to match the real data. Here entry two is switched to the correct encoding and we are then able to re-encode it\n\nEncoding(x[2]) <- \"latin1\"\nas_utf8(x)\n\n[1] \"façile\" \"façile\" \"façile\""
  },
  {
    "objectID": "remove_accents.html#background",
    "href": "remove_accents.html#background",
    "title": "4  Remove_Accents",
    "section": "4.1 Background",
    "text": "4.1 Background\nAccents in text can cause all manner of headaches in R.\nI’ve seen a lot of different ways to remove accents, none of which seemed to work for me.\nThis brute-force approach is not sophisticated but is highly effective.\nIt works by simply doing a find and replace with gsub, but providing an exhaustive list of special characters and a second list of their non-accented counterparts.\nDoes the job very effectively. You can modify this by adding other special characters, remembering that the vector position of a character in list a needs to map to the vector position of its counterpart in list b"
  },
  {
    "objectID": "remove_accents.html#define-function",
    "href": "remove_accents.html#define-function",
    "title": "4  Remove_Accents",
    "section": "4.2 Define Function",
    "text": "4.2 Define Function\n\nremoveAccents<-function(x)\n{\na <- c('À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ń', 'Ņ', 'ņ', 'Ň', 'ň', 'ŉ', 'Ō', 'ō', 'Ŏ', 'ŏ', 'Ő', 'ő', 'Œ', 'œ', 'Ŕ', 'ŕ', 'Ŗ', 'ŗ', 'Ř', 'ř', 'Ś', 'ś', 'Ŝ', 'ŝ', 'Ş', 'ş', 'Š', 'š', 'Ţ', 'ţ', 'Ť', 'ť', 'Ŧ', 'ŧ', 'Ũ', 'ũ', 'Ū', 'ū', 'Ŭ', 'ŭ', 'Ů', 'ů', 'Ű', 'ű', 'Ų', 'ų', 'Ŵ', 'ŵ', 'Ŷ', 'ŷ', 'Ÿ', 'Ź', 'ź', 'Ż', 'ż', 'Ž', 'ž', 'ſ', 'ƒ', 'Ơ', 'ơ', 'Ư', 'ư', 'Ǎ', 'ǎ', 'Ǐ', 'ǐ', 'Ǒ', 'ǒ', 'Ǔ', 'ǔ', 'Ǖ', 'ǖ', 'Ǘ', 'ǘ', 'Ǚ', 'ǚ', 'Ǜ', 'ǜ', 'Ǻ', 'ǻ', 'Ǽ', 'ǽ', 'Ǿ', 'ǿ');\nb <- c('A', 'A', 'A', 'A', 'A', 'A', 'AE', 'C', 'E', 'E', 'E', 'E', 'I', 'I', 'I', 'I', 'D', 'N', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'U', 'U', 'U', 'Y', 's', 'a', 'a', 'a', 'a', 'a', 'a', 'ae', 'c', 'e', 'e', 'e', 'e', 'i', 'i', 'i', 'i', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'u', 'u', 'u', 'u', 'y', 'y', 'A', 'a', 'A', 'a', 'A', 'a', 'C', 'c', 'C', 'c', 'C', 'c', 'C', 'c', 'D', 'd', 'D', 'd', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'G', 'g', 'G', 'g', 'G', 'g', 'G', 'g', 'H', 'h', 'H', 'h', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'IJ', 'ij', 'J', 'j', 'K', 'k', 'L', 'l', 'L', 'l', 'L', 'l', 'L', 'l', 'l', 'l', 'N', 'n', 'N', 'n', 'N', 'n', 'n', 'O', 'o', 'O', 'o', 'O', 'o', 'OE', 'oe', 'R', 'r', 'R', 'r', 'R', 'r', 'S', 's', 'S', 's', 'S', 's', 'S', 's', 'T', 't', 'T', 't', 'T', 't', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'W', 'w', 'Y', 'y', 'Y', 'Z', 'z', 'Z', 'z', 'Z', 'z', 's', 'f', 'O', 'o', 'U', 'u', 'A', 'a', 'I', 'i', 'O', 'o', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'A', 'a', 'AE', 'ae', 'O', 'o');\nfor(i in 1:length(a))\n{\nx<-gsub(x = x,pattern = a[i],replacement = b[i])\n}\nreturn(x)\n}"
  },
  {
    "objectID": "remove_accents.html#test-function",
    "href": "remove_accents.html#test-function",
    "title": "4  Remove_Accents",
    "section": "4.3 Test function",
    "text": "4.3 Test function\n\nremoveAccents(c(\"Féàltê\",\"Ýüćčæ\"))\n\n[1] \"Fealte\" \"Yuccae\""
  },
  {
    "objectID": "text_to_speech_R.html#background",
    "href": "text_to_speech_R.html#background",
    "title": "5  Text to Speech in R",
    "section": "5.1 Background",
    "text": "5.1 Background\nThis is a simple bit of script that reaches in to the linux/OS-X system and runs a text to speech function.\nIt may have some practical benefits. I quite like to use it to tell me that a long running script has finished."
  },
  {
    "objectID": "text_to_speech_R.html#define-functions",
    "href": "text_to_speech_R.html#define-functions",
    "title": "5  Text to Speech in R",
    "section": "5.2 Define Functions",
    "text": "5.2 Define Functions\n\nspeakr <- function(message=\"hello, world\")\n{\n  if(.Platform$OS.type == \"unix\"){system(paste(\"say \",message,sep=\"\"))}\n    else {system(paste(\"echo \",message,\"|ptts\",sep=\"\"))}\n}"
  },
  {
    "objectID": "text_to_speech_R.html#example",
    "href": "text_to_speech_R.html#example",
    "title": "5  Text to Speech in R",
    "section": "5.3 Example",
    "text": "5.3 Example\n\n# usage examples\n\nspeakr()\nspeakr(\"Who lives in a pineapple under the sea?\")"
  },
  {
    "objectID": "UK_Postcodes.html#background",
    "href": "UK_Postcodes.html#background",
    "title": "6  UK Postcode Maps",
    "section": "6.1 Background",
    "text": "6.1 Background\nThis is a basic tutorial on how to load shapefiles and geopoints to a leaflet map.\nThere is a lot more detailed information here [https://rstudio.github.io/leaflet/](https://rstudio.github.io/leaflet/)\nThe example is hopefully useful as it uses UK postcode and district shapefiles, which are commonly useful for a variety of purposes.\nUK Postcode data can be found here\n[https://www.freemaptools.com/map-tools.htm](https://www.freemaptools.com/map-tools.htm)\nUK district shapefiles are available here\n[https://www.ordnancesurvey.co.uk/opendatadownload/products.html](https://www.ordnancesurvey.co.uk/opendatadownload/products.html)"
  },
  {
    "objectID": "UK_Postcodes.html#libraries",
    "href": "UK_Postcodes.html#libraries",
    "title": "6  UK Postcode Maps",
    "section": "6.2 Libraries",
    "text": "6.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nlibrary(knitr)\nlibrary(leaflet)"
  },
  {
    "objectID": "UK_Postcodes.html#data",
    "href": "UK_Postcodes.html#data",
    "title": "6  UK Postcode Maps",
    "section": "6.3 Data",
    "text": "6.3 Data\n\n6.3.1 Create a folder to house data\n\nif(!dir.exists(\"data/ukpostcodes\"))(dir.create(\"data/ukpostcodes\"))\nsystem(\"rm -rf data/ukpostcodes/*\")"
  },
  {
    "objectID": "UK_Postcodes.html#download-a-list-of-uk-postcodes-and-gps-locations",
    "href": "UK_Postcodes.html#download-a-list-of-uk-postcodes-and-gps-locations",
    "title": "6  UK Postcode Maps",
    "section": "6.4 Download a list of UK postcodes and gps locations",
    "text": "6.4 Download a list of UK postcodes and gps locations\n\ndownload.file(url = \"https://data.freemaptools.com/download/full-uk-postcodes/ukpostcodes.zip\",destfile =paste(\"data/ukpostcodes/ukpostcodes.zip\"))\n\nsystem (\"unzip data/ukpostcodes/ukpostcodes.zip -d data/ukpostcodes/\")"
  },
  {
    "objectID": "UK_Postcodes.html#download-a-list-of-uk-district-shapefiles",
    "href": "UK_Postcodes.html#download-a-list-of-uk-district-shapefiles",
    "title": "6  UK Postcode Maps",
    "section": "6.5 Download a list of UK district shapefiles",
    "text": "6.5 Download a list of UK district shapefiles\n\n# set a timeout proportional to the size of the dataset and the speed of the connection\noptions(timeout=300)\n\ndownload.file(url = \"https://api.os.uk/downloads/v1/products/BoundaryLine/downloads?area=GB&format=ESRI%C2%AE+Shapefile&redirect\",destfile =paste(\"data/ukpostcodes/bdline_essh_gb.zip\"))\n\nsystem (\"unzip data/ukpostcodes/bdline_essh_gb.zip -d data/ukpostcodes/\")"
  },
  {
    "objectID": "UK_Postcodes.html#read-postcode-data",
    "href": "UK_Postcodes.html#read-postcode-data",
    "title": "6  UK Postcode Maps",
    "section": "6.6 Read Postcode data",
    "text": "6.6 Read Postcode data\n\nukpostcodes <- read_csv(\"data/ukpostcodes/ukpostcodes.csv\")\n\nRows: 1792783 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): postcode\ndbl (3): id, latitude, longitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "UK_Postcodes.html#make-a-small-sample-of-all-postcodes",
    "href": "UK_Postcodes.html#make-a-small-sample-of-all-postcodes",
    "title": "6  UK Postcode Maps",
    "section": "6.7 Make a small sample of all postcodes",
    "text": "6.7 Make a small sample of all postcodes\n\nsample<-ukpostcodes[sample(1:nrow(ukpostcodes),size = 100),] %>% \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude)\n  )\nhead(sample)\n\n# A tibble: 6 × 4\n       id postcode latitude longitude\n    <dbl> <chr>       <dbl>     <dbl>\n1 1610063 BD8 7BN      53.8    -1.77 \n2 1714927 BT30 6NP     54.3    -5.71 \n3 1277368 DN40 1EJ     53.6    -0.205\n4  821166 M44 6ZN      53.4    -2.43 \n5   40173 WR9 7DP      52.3    -2.13 \n6  696265 NG9 6PZ      52.9    -1.25"
  },
  {
    "objectID": "UK_Postcodes.html#read-in-shapefiles-to-a-map",
    "href": "UK_Postcodes.html#read-in-shapefiles-to-a-map",
    "title": "6  UK Postcode Maps",
    "section": "6.8 Read in shapefiles to a map",
    "text": "6.8 Read in shapefiles to a map\n\nmap = read_sf(\"data/ukpostcodes/Data/GB/county_region.dbf\")\n\n\nmapproj <- st_transform(map, st_crs(\"+proj=longlat +init=epsg:4326 +ellps=WGS84 +datum=WGS84 +no_defs\"))\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\n\n\nleaflet(mapproj) %>% \n    addTiles() %>% \n  addMeasure(primaryLengthUnit=\"kilometers\", secondaryLengthUnit=\"meters\")  %>%\n  addScaleBar(options = c(imperial = FALSE)) %>%  \n  addPolygons(color = \"green\",label = mapproj$NAME) %>% \n  addCircleMarkers(lng = sample$longitude,lat = sample$latitude,label = sample$postcode,radius = 0.3)\n\n\n\n\n\n\n6.8.1 remove data\n\nsystem(\"rm -rf data/ukpostcodes/\")"
  },
  {
    "objectID": "Interrupted_Time_Series.html#introduction",
    "href": "Interrupted_Time_Series.html#introduction",
    "title": "7  A pragmatic Introduction to Interrupted Time Series",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThis is a brief introduction to Interrupted Time Series analyses. It’s intended for use by people who have done some reading and understand about concepts like autocorrelation.\nI mean, I won’t be explaining all the technical stuff. This is really here to help you get things done. Buyer beware, you can make mistakes that will jack up your game, so think hard before you publish stuff.\nThe methods below will show you how to carry out an interrupted time series (ITS) using generalised least squares, accounting for autocorrelation via a corARMA model. We’ll start out with some fairly simple models, then build up to something a bit more complicated, before going on to add a control group.\nWe’re going to simulate a study where we are interested to know whether the values of a quantity (quantity.x) have changed significantly in response to some kind of intervention. If it helps you to think of a real-world situation, imagine we take weekly measurements of how many bacteria are found on the handles of public toilet doors. Our intervention might be placing signs about handwashing on all the doors. Can we say that putting up the signs correlated with a decrease in the number of bacteria counted each week?\nAt the core of this analysis is the concept of a counterfactual. We’ll be doing some modelling to estimate what did happen (the factual scenario) and what we expect would have happened (the counterfactual scenario) if we had never hung up the signs about handwashing.\nWe’ll be measuring whether the parameter quantity.x changes in response to the Intervention, but there are different kinds of change that we might expect.\nThe change could take two forms including\n\nA step change immediately after the intervention\n\nfor instance because the signs are so effective that everyone suddenly starts washing their hands much better\n\nA slope/trend change after the intervention\n\nbecause over time, the presence of the signs conditions more and more people to wash their hands. Alternatively, maybe after an initial step change down, it starts to creep back up because people start to ignore the signs\n\n\nDifferent real world scenarios may or may not be compatible with the assumptions that these types of change could occur, so use your knowledge to decide which ones to model.\n\n7.1.1 This document provides examples of three main type of ITS analysis\n\nPart One - Uncontrolled ITS, one intervention\nPart Two - Uncontrolled ITS, two interventions\nPart Three - Controlled ITS, one intervention\n\nIn each case we will build a data set that shows each component of the model in a fairly longhand format. It’s totally possible to use fewer variables and to use interactions (shown further down this document) to model all the various components of the ITS, but this is harder to understand to the novice and harder to decode when it comes to reconciling the data frame against the model coefficients that will be used to interpret what effects the interruption had."
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-one---uncontrolled-its-one-intervention",
    "href": "Interrupted_Time_Series.html#part-one---uncontrolled-its-one-intervention",
    "title": "7  A pragmatic Introduction to Interrupted Time Series",
    "section": "7.2 Part One - Uncontrolled ITS, one intervention",
    "text": "7.2 Part One - Uncontrolled ITS, one intervention\nWe’ll create a dummy dataframe, where\n\nTime = A number, the time in study weeks (1 - 100 weeks)\nIntervention = A binary indication of whether the Intervention has taken place at Time x\nPost.intervention.time = A number, the time elapsed since the Intervention\nquantity.x = The thing we want to measure\n\n\n# create a dummy dataframe\n# Here the interruption takes place at week 51\ndf<-tibble(\n  Time = 1:100,\n  Intervention = c(rep(0,50),rep(1,50)),\n  Post.intervention.time = c(rep(0,50),1:50),\n  quantity.x = c(sort(sample(200:300,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(20:170,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)))\n)\n\ndatatable(df,options = list(pageLength = 100, scrollY = \"200px\"))\n\n\n\n\n\n\nIn the simplest form, the ITS model looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=“ML”)\nUsing the gls command from the nlme package, we can create a model that describes the change in quantity.x across time\n\nmodel.a = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=\"ML\")\n\n# Show a summary of the model\nsummary(model.a)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time + Intervention + Post.intervention.time \n  Data: df \n       AIC      BIC    logLik\n  873.8388 886.8647 -431.9194\n\nCoefficients:\n                           Value Std.Error   t-value p-value\n(Intercept)            295.58286  5.327482  55.48266  0.0000\nTime                    -1.97815  0.181824 -10.87947  0.0000\nIntervention           -24.84346  7.423687  -3.34651  0.0012\nPost.intervention.time  -1.11957  0.257138  -4.35395  0.0000\n\n Correlation: \n                       (Intr) Time   Intrvn\nTime                   -0.870              \nIntervention            0.348 -0.600       \nPost.intervention.time  0.615 -0.707 -0.017\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.39341496 -0.72665716  0.03963433  0.86022509  1.98295020 \n\nResidual standard error: 18.17879 \nDegrees of freedom: 100 total; 96 residual\n\n\nThese coefficients have real world meaning and are not there to be ignored.\nThis tells us that the modelled line passes the y axis at 295.6 units and that prior to the intervention, the average value of quantity.x was falling by 1.98 units per week. At the intervention, there was a step change of -24.8 units and subsequent to the intervention there was a trend in which the value of quantity.x fell by an additional 1.12 units per week. The last distinction is important because that means that after the intervention, the weekly decrease in quantity.x was 1.98 + 1.12 = 3.10\nThese statistics tell us whether the changes that happened at different timepoints were significant with respect to what the line was doing before the interruption. These values can be used to calculate the overall difference in quantity.x between times [i] and [ii] using this formula\nquantity.x[i] = 308.27918 + (Time[i]*-1.97565) + (Intervention[i]*-27.33565) + (Post.intervention.time[i]*-1.17575)\nquantity.x[ii] = 308.27918 + (Time[ii]*-1.97565) + (Intervention[ii]*-27.33565) + (Post.intervention.time[ii]*-1.17575)\nabsolute difference = difference(quantity.x[i] , quantity.x[ii])\nBut what would be nice would be to calculate the counterfactual scenario where the intervention didn’t happen and to estimate the difference between the factual values of quantity.x at time [i] and the counterfactuals at the same time [i]. This is coming up later.\nIt’s nice to add values from models to the df, so we will next copy the modelled values of quantity.x in to the df using the predictSE.gls command from the AICcmodavg package.\n\ndf<-df %>% mutate(\n  model.a.predictions = predictSE.gls (model.a, df, se.fit=T)$fit,\n  model.a.se = predictSE.gls (model.a, df, se.fit=T)$se\n  )\n\nNote that we captured both the predicted value of quantity.x and also the standard error on the estimate. We’ll need that to show the error margins and to draw confidence intervals on our charts.\nLet’s draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.a.predictions - (1.96*model.a.se), ymax = model.a.predictions + (1.96*model.a.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.a.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\nBefore we get too much further though, we need to look at autocorrelation in the data. gls allows us to add corARMA structures by specifying values for p (the autoregressive order) and q (the moving average order).\nA gls model with a corARMA correction looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,correlation= corARMA(p=1, q=1, form = ~ Time),method=“ML”)\nbut the critical issue is how to choose the correct values of p and q. There’s lots of information about this online, so read carefully because it is important. Whether you exactly understand what this is all about or not, you’ll still need to take steps to minimise the problem empirically. What I do here is to apply different values of p and q to the gls model, capturing the value of the Akaike Information Criterion (AIC) for each model. Whichever combination of values of p and q returns the smallest AIC value is the best fit for our modelling.\nRealistically there’s some margin for human biases here. It’s hard to exactly define good numbers to put in to this analysis. I’d recommend a broad range of values because sometime autocorrelations can be quite lagged.\nWe’ll start by defining the basic model and then creating a basic R function that can apply different values of p and q to that model\n\nmod.1 = quantity.x ~ Time + Intervention + Post.intervention.time\n\nfx = function(pval,qval){summary(gls(mod.1, data = df, correlation= corARMA(p=pval,q=qval, form = ~ Time),method=\"ML\"))$AIC}\n\nOur start point is the AIC value of the model we ran earlier where neither p or q were specified (i.e. no autocorrelation)\n\np = summary(gls(mod.1, data = df,method=\"ML\"))$AIC\nmessage(str_c (\"AIC Uncorrelated model = \",p))\n\nAIC Uncorrelated model = 873.838811568721\n\n\nNext we can create a grid of combinations of values of p and q\n\nautocorrel = expand.grid(pval = 0:2, qval = 0:2)\n\nThen we apply the function to all possible combinations of p and q. Expect some not to work with this dummy data set.\n\nfor(i in 2:nrow(autocorrel)){p[i] = try(summary(gls(mod.1, data = df, correlation= corARMA(p=autocorrel$pval[i],q=autocorrel$qval[i], form = ~ Time),method=\"ML\"))$AIC)}\n\nautocorrel<- autocorrel %>%\n  mutate(AIC = as.numeric(p)) %>%\n  arrange(AIC)\n\n\nautocorrel\n\n  pval qval      AIC\n1    0    0 873.8388\n2    0    1 875.8225\n3    1    0 875.8239\n4    1    1 875.8486\n5    0    2 877.6252\n6    2    0 877.6315\n7    2    1 877.8159\n8    1    2 879.6470\n9    2    2 879.8083\n\n\nIn this dataset, the best values of p and q appear to be p = 2 and q = 2 Let’s see what effect that has on our model by comparing it to the original model.a\n\nmodel.b = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time))\n\n\ncoefficients(model.a)\n\n           (Intercept)                   Time           Intervention \n            295.582857              -1.978151             -24.843457 \nPost.intervention.time \n             -1.119568 \n\ncoefficients(model.b)\n\n           (Intercept)                   Time           Intervention \n            295.729095              -1.987147             -24.161953 \nPost.intervention.time \n             -1.123327 \n\n\nYou can see that there’s some quite small changes to the values here, but I didn’t make a dataset that had a lot of autocorrelation in it. I’m sure there are datasets out there where you’d see a big effect. Do also keep in mind that Post.intervention.time describes a weekly trend, so over long time periods, the error would creep up.\nOK, so now we’ve dealt with the autocorrelation, let’s assign the predicted values of model.b on to the df\n\ndf<- df %>% \n  mutate(\n      model.b.predictions = predictSE.gls (model.b, df, se.fit=T)$fit,\n      model.b.se = predictSE.gls (model.b, df, se.fit=T)$se\n        )\n\nNext we need to ask what would have happened if there had been no intervention. This is the counterfactual model.\nThe gls model for the counterfactual looks like this…\ngls(quantity.x ~ Time, data = df,method=“ML”)\nThere’s nothing clever about this, it’s the same model as we had before, only we’ve taken out the intervention and post intervention arguments. Our aim here is to calculate the pre-intervention trend and simply to extrapolate out beyond the intervention time point. This can be done with the predictSE.gls function.\nTo create the counterfactual model, we have to make a new df which truncates the data at the time point immediately before the intervention. Then we run predict on the model, providiing the original df as ‘newdata’.\n\ndf2<-filter(df,Time<51)\nmodel.c = gls(quantity.x ~ Time, data = df2, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\nLet’s have a look at how the new counterfactual (model.c) model compares to the factual model (model.a).\n\ncoefficients(model.a)\n\n           (Intercept)                   Time           Intervention \n            295.582857              -1.978151             -24.843457 \nPost.intervention.time \n             -1.119568 \n\ncoefficients(model.c)\n\n(Intercept)        Time \n 295.282158   -1.966667 \n\n\nAs you can see here, the intercept and slope of the factual and counterfactual models are almost identical, which is what we wanted. If you were a purist who cared about those little differences, you could use the actual values from model.a to calculate the counterfactuals manually by doing this\ny = 310.3877551 + (Time * -2.4089316)\nThat’s great in terms of accuracy of your y value estimates, but it is much harder to calculate the standard errors manually so your precision/confidence intervals becomes a real pain. This is especially true when the model gets more complicated (as we’ll see in part two).\nSo let’s accept a little error in the accuracy and use ‘predict’ because it gives us nice precise confidence estimates, let’s make a new variable with predictions of the counterfactual model, providing the full 100 week data frame as ‘newdata’\n\ndf<-df %>% mutate(\n  model.c.predictions = predictSE.gls (model.c, newdata = df, se.fit=T)$fit,\n  model.c.se = predictSE.gls (model.c, df, se.fit=T)$se\n)\n\nNext we can plot the chart\n\n  ggplot(df,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.c.predictions - (1.96*model.c.se), ymax = model.c.predictions + (1.96*model.c.se)), fill = \"pink\")+\n  geom_line(aes(Time,model.c.predictions),color=\"red\",lty=2)+\n  geom_ribbon(aes(ymin = model.b.predictions - (1.96*model.b.se), ymax = model.b.predictions + (1.96*model.b.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.b.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\nThe solid line with green ribbon is the factual data, the red dashed line with the pink ribbon is the counterfactual. Using the rule of thumb that if the confidence intervals don’t overlap, there’s something significant happening, we can conclude that the interruption preceded a significant step change in quantity.x. That the lines also diverge suggests that there could be a significant trend change.\nThe last remaining thing we need to do is to calculate those relative differences. This is easy because we’ve been adding the modelled values to the df as variables. To get a list of the relative differences at different timepoints, we really only have to do subtract the factual from the counterfactual.\nHere’ we can ask for the relative differences at weeks 1 (start), 50 (pre-intervention), 51 (immediate post-intervention) and 100 (end of the study)\n\nformat(df$model.b.predictions-df$model.c.predictions,scientific = F)[c(1,50,51,100)]\n\n              1              50              51             100 \n\"  0.426456649\" \" -0.577084251\" \"-25.882844282\" \"-81.929397664\" \n\n\nHere we can see that pre-intervention, the difference between factual and counterfactual is essentially zero, which is what we expect. At week 51 we see that the difference is 26 units, almost the same as the step change we saw in the coefficients for model.b above. At week 100 the factual data are 82 units lower than the counterfactual, which is the combined effect of the intervention step change and the intervention trend change."
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-two---uncontrolled-its-two-interventions",
    "href": "Interrupted_Time_Series.html#part-two---uncontrolled-its-two-interventions",
    "title": "7  A pragmatic Introduction to Interrupted Time Series",
    "section": "7.3 Part Two - Uncontrolled ITS, two interventions",
    "text": "7.3 Part Two - Uncontrolled ITS, two interventions\nSome designs may include multiple interventions and it is fairly simple to extend the model to account for this kind of thing We’ll make a new data set that includes a second intervention and a post-intervention-2 trend.\n\n# create a dummy dataframe\n# Here the interruption takes place at week 51\ndf3<-tibble(\n  Time = 1:150,\n  Intervention = c(rep(0,50),rep(1,100)),\n  Post.intervention.time = c(rep(0,50),1:100),\n  Intervention.2 = c(rep(0,100),rep(1,50)),\n  Post.intervention.2.time = c(rep(0,100),1:50),\n  quantity.x = c(sort(sample(2000:2500,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(200:1700,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)),c(sort(sample(200:450,size = 50,replace = T),decreasing = F)+sample(-40:40,50,replace = T)))\n)\n\ndatatable(df3,options = list(pageLength = 100, scrollY = \"200px\"))\n\n\n\n\n\n\nThe new ITS model looks like this\ngls(quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 + Post.intervention.2.time, data = df,method=“ML”, correlation= corARMA(p=2,q=2, form = ~ Time))\nRemember that you should probably deal with autocorrelation at this point. The method is the same as before, so I won’t reproduce it here. I’m just going to make up some values for p and q\n\nmodel.d = gls(quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 + Post.intervention.2.time, data = df3,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time))\n\n# Show a summary of the model\nsummary(model.d)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time + Intervention + Post.intervention.time + Intervention.2 +      Post.intervention.2.time \n  Data: df3 \n       AIC      BIC    logLik\n  1473.666 1506.783 -725.8331\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time \n Parameter estimate(s):\n       Phi1        Phi2      Theta1      Theta2 \n 0.90046120 -0.06218045 -0.29449964 -0.04482048 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)              2490.3446 29.917024  83.24172  0.0000\nTime                       -8.6791  0.940667  -9.22652  0.0000\nIntervention             -277.7190 27.936760  -9.94099  0.0000\nPost.intervention.time    -20.7962  1.488490 -13.97137  0.0000\nIntervention.2            -69.5904 27.959035  -2.48901  0.0139\nPost.intervention.2.time   32.9576  1.488490  22.14162  0.0000\n\n Correlation: \n                         (Intr) Time   Intrvn Pst.n. Intr.2\nTime                     -0.841                            \nIntervention              0.178 -0.398                     \nPost.intervention.time    0.613 -0.820 -0.012              \nIntervention.2           -0.026  0.059  0.092 -0.275       \nPost.intervention.2.time -0.094  0.217  0.308 -0.619 -0.042\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.5939940 -0.6025778 -0.1496218  0.3918488  3.1601189 \n\nResidual standard error: 42.68343 \nDegrees of freedom: 150 total; 144 residual\n\n\nReferring back to the earlier, more simple example, you can probably see that these coefficients are easy to explain.\n\nThe intercept is still the initial average value of quantity.x\nTime is the pre-intervention slope\nIntervention describes the step change that occurs at the intervention timepoint\nPost.intervention.time describes the additional slope change that occurs at the intervention timepoint\nIntervention.2 describes the step change that occurs at the timepoint of the second intervention\nPost.intervention.2.time describes the additional slope change that occurs at the timepoint of the second intervention\n\nLet’s grab the estimated modelled values for the new two intervention study\n\ndf3<-df3 %>% mutate(\n  model.d.predictions = predictSE.gls (model.d, df3, se.fit=T)$fit,\n  model.d.se = predictSE.gls (model.d, df3, se.fit=T)$se\n  )\n\nLet’s draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df3,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.d.predictions - (1.96*model.d.se), ymax = model.d.predictions + (1.96*model.d.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.d.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\nlet’s calculate the first counterfactual, that there were no interventions\n\ndf4<-filter(df3,Time<51)\nmodel.e = gls(quantity.x ~ Time, data = df4, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf3<-df3 %>% mutate(\n  model.e.predictions = predictSE.gls (model.e, newdata = df3, se.fit=T)$fit,\n  model.e.se = predictSE.gls (model.e, df3, se.fit=T)$se\n)\n\nand then the second counterfactual, that only the first intervention happened\n\ndf5<-filter(df3,Time<101)\nmodel.f = gls(quantity.x ~ Time + Intervention + Post.intervention.time, data = df5, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf3<-df3 %>% mutate(\n  model.f.predictions = predictSE.gls (model.f, newdata = df3, se.fit=T)$fit,\n  model.f.se = predictSE.gls (model.f, df3, se.fit=T)$se\n)\n\nFinally, let’s draw the chart that shows the factual data (black,green), the first (red, pink) and second (navy, turquoise) counterfactuals\n\n  ggplot(df3,aes(Time,quantity.x))+\n  geom_ribbon(aes(ymin = model.f.predictions - (1.96*model.d.se), ymax = model.f.predictions + (1.96*model.e.se)), fill = \"lightblue\")+\n  geom_line(aes(Time,model.f.predictions),color=\"blue\",lty=2)+\n  geom_ribbon(aes(ymin = model.e.predictions - (1.96*model.d.se), ymax = model.e.predictions + (1.96*model.e.se)), fill = \"pink\")+\n  geom_line(aes(Time,model.e.predictions),color=\"red\",lty=2)+\n  geom_ribbon(aes(ymin = model.d.predictions - (1.96*model.d.se), ymax = model.d.predictions + (1.96*model.d.se)), fill = \"lightgreen\")+\n  geom_line(aes(Time,model.d.predictions),color=\"black\",lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\nYou can use the stored modelled values to do any calculations you want with regards to the relative differences. Remember that some things can’t ever go below zero and that your hypothesis probably isn’t that there’s a linear trend that continues forever. Think carefully about your counterfactuals in this context."
  },
  {
    "objectID": "Interrupted_Time_Series.html#part-three---controlled-its-one-intervention",
    "href": "Interrupted_Time_Series.html#part-three---controlled-its-one-intervention",
    "title": "7  A pragmatic Introduction to Interrupted Time Series",
    "section": "7.4 Part Three - Controlled ITS, one intervention",
    "text": "7.4 Part Three - Controlled ITS, one intervention\nControlled ITS is about as good as it gets when it comes to time series. The control allows you to calibrate the ITS to account for independent secular and periodic changes. Let’s say that quantity.x and quantity.y are related. Both have exhibited some kind of long-range secular change (for instance there’s the pre-intervention trend in the examples above). Let’s say that both quantity.x and quantity.y are experiencing a parallel trend pre-intervention.\nThe intervention is intented to affect change in quantity.x, but not quantity.y, so post-intervention trends in quantity.y can be taken account for in our model, strenghtening our results.\nLet’s make a data set.\n\n# create a dummy dataframe\n# Here the interruption takes place at week 51\ndf.x<-tibble(\n  x = 1,\n  Time = 1:100,\n  x.Time = x*Time,\n  Intervention = c(rep(0,50),rep(1,50)),\n  x.Intervention = x*Intervention,\n  Post.intervention.time = c(rep(0,50),1:50),\n  x.Post.intervention.time = x * Post.intervention.time,\n  quantity.x = c(sort(sample(200:300,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(20:170,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)))\n)\n\ndf.y<-tibble(\n  x = 0,\n  Time = 1:100,\n  x.Time = x*Time,\n  Intervention = c(rep(0,50),rep(1,50)),\n  x.Intervention = x*Intervention,\n  Post.intervention.time = c(rep(0,50),1:50),\n  x.Post.intervention.time = x * Post.intervention.time,\n  quantity.x = c(sort(sample(500:600,size = 50,replace = T),decreasing = T)+sample(-20:20,50,replace = T),c(sort(sample(280:500,size = 50,replace = T),decreasing = T)+sample(-40:40,50,replace = T)))\n)\n\ndf6 = bind_rows(df.x,df.y) %>% \n  arrange(Time,x)\n\ndatatable(df6,options = list(pageLength = 200, scrollY = \"200px\"))\n\n\n\n\n\n\nLook carefully at the dataset. I’ve introduced the variable x (which differentiates between the control (0) and test (1) groups, plus some new Interaction terms including x.Time, x.Intervention and x.Post.intervention.time. Each of these is just the value of the variable, multiplied by x (which indicates control variables [0, i.e. quantity.y] or variables of interest [1, i.e. quantity.x]). This makes those variables null for the controls and meaninful step or trend changes for the lines of the data relating to quantity.x\nThe ITS model is now a little more complicated. Again, I’ve skipped the step where we test different values of p and q. You should not skip that!\ngls(quantity.x ~ Time + x.Time + Intervention + x.Intervention + Post.intervention.time + x.Post.intervention.time, data = df6, correlation= corARMA(p=1, q=1, form = ~ Time|x),method=“ML”)\nNote that we’ve had to change the form = ~ Time|x) argument in the corARMA to account for the fact that it needs to correct for autocorrelation across time and across the two groups where x == 1 and x == 0\n\nmodel.g = gls(quantity.x ~ x + Time + x.Time + Intervention + x.Intervention + Post.intervention.time + x.Post.intervention.time, data = df6,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time|x))\n\n# Show a summary of the model\nsummary(model.g)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ x + Time + x.Time + Intervention + x.Intervention +      Post.intervention.time + x.Post.intervention.time \n  Data: df6 \n       AIC      BIC   logLik\n  1748.264 1791.142 -861.132\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n      Phi1       Phi2     Theta1     Theta2 \n-0.8226711 -0.3081738  0.9423439  0.5332840 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               593.4091  6.057145  97.96844  0.0000\nx                        -294.0878  8.566097 -34.33160  0.0000\nTime                       -1.9758  0.206268  -9.57865  0.0000\nx.Time                      0.0394  0.291707   0.13497  0.8928\nIntervention               12.1990  8.368284   1.45777  0.1465\nx.Intervention            -40.2387 11.834541  -3.40011  0.0008\nPost.intervention.time     -2.9357  0.293268 -10.01029  0.0000\nx.Post.intervention.time    1.9162  0.414743   4.62014  0.0000\n\n Correlation: \n                         (Intr) x      Time   x.Time Intrvn x.Intr Pst.n.\nx                        -0.707                                          \nTime                     -0.869  0.615                                   \nx.Time                    0.615 -0.869 -0.707                            \nIntervention              0.342 -0.242 -0.594  0.420                     \nx.Intervention           -0.242  0.342  0.420 -0.594 -0.707              \nPost.intervention.time    0.616 -0.435 -0.711  0.503 -0.018  0.012       \nx.Post.intervention.time -0.435  0.616  0.503 -0.711  0.012 -0.018 -0.707\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.48762378 -0.68548597 -0.02441616  0.68659430  2.13099255 \n\nResidual standard error: 18.42259 \nDegrees of freedom: 200 total; 192 residual\n\n\nIf you were a fancy Dan with these things, you could also use interaction terms in your model to achieve the same result\n\nmodel.h = gls(quantity.x ~ Time*x + Intervention*x + Post.intervention.time*x, data = df6,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time|x))\n\n# Show a summary of the model\nsummary(model.h)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ Time * x + Intervention * x + Post.intervention.time *      x \n  Data: df6 \n       AIC      BIC   logLik\n  1748.264 1791.142 -861.132\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n      Phi1       Phi2     Theta1     Theta2 \n-0.8226711 -0.3081739  0.9423439  0.5332840 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               593.4091  6.057145  97.96844  0.0000\nTime                       -1.9758  0.206268  -9.57865  0.0000\nx                        -294.0878  8.566097 -34.33160  0.0000\nIntervention               12.1990  8.368284   1.45777  0.1465\nPost.intervention.time     -2.9357  0.293268 -10.01029  0.0000\nTime:x                      0.0394  0.291707   0.13497  0.8928\nx:Intervention            -40.2387 11.834541  -3.40011  0.0008\nx:Post.intervention.time    1.9162  0.414743   4.62014  0.0000\n\n Correlation: \n                         (Intr) Time   x      Intrvn Pst.n. Time:x x:Intr\nTime                     -0.869                                          \nx                        -0.707  0.615                                   \nIntervention              0.342 -0.594 -0.242                            \nPost.intervention.time    0.616 -0.711 -0.435 -0.018                     \nTime:x                    0.615 -0.707 -0.869  0.420  0.503              \nx:Intervention           -0.242  0.420  0.342 -0.707  0.012 -0.594       \nx:Post.intervention.time -0.435  0.503  0.616  0.012 -0.707 -0.711 -0.018\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.48762378 -0.68548597 -0.02441616  0.68659430  2.13099255 \n\nResidual standard error: 18.42259 \nDegrees of freedom: 200 total; 192 residual\n\n\nSee, they’re identical! Personally I prefer to have the variables like x.Intervention written out in my datasets in full. Working with interactions confuses me and I also don’t like the way R presents the coefficients out of order.\nSo let’s look at the longhand version in detail\n\ncoefficients(model.g)\n\n             (Intercept)                        x                     Time \n            593.40907021            -294.08779642              -1.97576601 \n                  x.Time             Intervention           x.Intervention \n              0.03937222              12.19903376             -40.23870252 \n  Post.intervention.time x.Post.intervention.time \n             -2.93569500               1.91617161 \n\n\nThe interpretation of this is as follows\n\nIntercept is the average value of the control group at the start of the study\nx is the difference between Intercept and the value of quantity.x at the start of the study. To calculate the actual value on the y axis, you’d do 593.4-294.1 = 299.3. When we draw the chart below, you’ll see that the line for quantity.x starts at 299.3 units at week 1.\nTime is the pre-intervention slope for the control group\nx.Time is the difference between Time and the values of quantity.x (see how the control group influences our results!)\nIntervention describes the step change that occurs at the intervention timepoint in the control group\nx.Intervention describes the difference in the step changes that occurs at the intervention timepoint betweent the two groups\nPost.intervention.time describes the slope change that occurs at the intervention timepoint in the control group\nx.Post.intervention.time describes the difference in the slope changes that occurs at the intervention timepoint in the control group\n\nLet’s grab the estimated modelled values for the new controlled intervention study\n\ndf6<-df6 %>% mutate(\n  model.g.predictions = predictSE.gls (model.g, df6, se.fit=T)$fit,\n  model.g.se = predictSE.gls (model.g, df6, se.fit=T)$se\n  )\n\nThen draw a picture on which we will show the raw data points, then add the modelled data as a line describing the factual observations and a ribbon showing the 95% confidence interval on the model\n\n  ggplot(df6,aes(Time,quantity.x))+geom_point(color=\"grey\")+\n  geom_ribbon(aes(ymin = model.g.predictions - (1.96*model.g.se), ymax = model.g.predictions + (1.96*model.g.se),fill=factor(x)),alpha=0.4)+\n  geom_line(aes(Time,model.g.predictions,color=factor(x)),lty=1)+\n  geom_point(alpha=0.3)\n\n\n\n\nSo let’s calculate the counterfactuals for each of these and add the predictions to the data set\n\ndf7<-filter(df6,Time<51)\nmodel.i = gls(quantity.x ~ x + Time + x.Time, data = df7, correlation= corARMA(p=1, q=1, form = ~ Time|x),method=\"ML\")\n\ndf6<-df6 %>% mutate(\n  model.i.predictions = predictSE.gls (model.i, newdata = df6, se.fit=T)$fit,\n  model.i.se = predictSE.gls (model.i, df6, se.fit=T)$se\n)\n\nThen plot the results\n\n  ggplot(df6,aes(Time,quantity.x))+geom_point(color=\"grey\")+\n  geom_ribbon(aes(ymin = model.g.predictions - (1.96*model.g.se), ymax = model.g.predictions + (1.96*model.g.se),fill=factor(x)),alpha=0.4)+\n  geom_ribbon(aes(ymin = model.i.predictions - (1.96*model.i.se), ymax = model.i.predictions + (1.96*model.i.se),fill=factor(x)),alpha=0.2)+\n  geom_line(aes(Time,model.g.predictions,color=factor(x)),lty=1)+\n  geom_line(aes(Time,model.i.predictions,color=factor(x)),lty=2)\n\n\n\n\nWe can see that in the group of interest, there’s been a big change in the amount of quantity.x since the intervention, but there’s also been a big change in quantity.y the control group. Is the effect we are seeing in the group of interest just happening because of the decline that is happening in both groups, which could indicate that some extrinsic factor influenced change in both groups. If this were the case (assuming that the control group is NOT affected by the intervention) then we’d be incorrectly attributing the result to the intervetion.\nLooking closely at the tables of coefficients, we can find some clues. Let’s look back at the fully controlled analysis\n\nsummary(model.g)\n\nGeneralized least squares fit by maximum likelihood\n  Model: quantity.x ~ x + Time + x.Time + Intervention + x.Intervention +      Post.intervention.time + x.Post.intervention.time \n  Data: df6 \n       AIC      BIC   logLik\n  1748.264 1791.142 -861.132\n\nCorrelation Structure: ARMA(2,2)\n Formula: ~Time | x \n Parameter estimate(s):\n      Phi1       Phi2     Theta1     Theta2 \n-0.8226711 -0.3081738  0.9423439  0.5332840 \n\nCoefficients:\n                             Value Std.Error   t-value p-value\n(Intercept)               593.4091  6.057145  97.96844  0.0000\nx                        -294.0878  8.566097 -34.33160  0.0000\nTime                       -1.9758  0.206268  -9.57865  0.0000\nx.Time                      0.0394  0.291707   0.13497  0.8928\nIntervention               12.1990  8.368284   1.45777  0.1465\nx.Intervention            -40.2387 11.834541  -3.40011  0.0008\nPost.intervention.time     -2.9357  0.293268 -10.01029  0.0000\nx.Post.intervention.time    1.9162  0.414743   4.62014  0.0000\n\n Correlation: \n                         (Intr) x      Time   x.Time Intrvn x.Intr Pst.n.\nx                        -0.707                                          \nTime                     -0.869  0.615                                   \nx.Time                    0.615 -0.869 -0.707                            \nIntervention              0.342 -0.242 -0.594  0.420                     \nx.Intervention           -0.242  0.342  0.420 -0.594 -0.707              \nPost.intervention.time    0.616 -0.435 -0.711  0.503 -0.018  0.012       \nx.Post.intervention.time -0.435  0.616  0.503 -0.711  0.012 -0.018 -0.707\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.48762378 -0.68548597 -0.02441616  0.68659430  2.13099255 \n\nResidual standard error: 18.42259 \nDegrees of freedom: 200 total; 192 residual\n\n\nWe need to look closely at the numbers here.\nIn this case, there was a pre-intervention decline in the control group (-1.98 units/week). The quantity.x group was also declining, at a very slightly lower rate (-1.94 units/week)\nAt the time of the intervention, the control group exhibited a step change of 12.20 units. The quantity.x group meanwhile went down 40.24 units relative to that, so overall it goes down 28.04 units.\nFinally, post-intervention, the rate of decline in the control group dipped (-2.94 units/week) whilst the quantity.x group actually went down less steeply (-2.94 + 1.92 = -1.02 units/week).\nWe might not care too much about these specific rates and numbers. Mostly we care about the big headline number of how much quantity.x has changed compared to the counterfactual, but do take one last look at the table, where the p-values tell you if each coefficient represents an independently significant change.\nIn this case, Time is significant, indicating that there was a significant change with time prior to the intervention. x.Time is not significant, which means that the pre-intervention trends for quantity.x and the control group are the same. I interpret this to mean that both groups were exhibiting similar trends prior to intervention\nMeanwhile, neither Intervention, nor x.Intervention are significant, so maybe we should interpret this to mean that the intervention did not have any immediate effect in the form of a step change.\nFinally, both Post.intervention.time and x.Post.intervention.time are significant, which suggests that something influenced the trends in both groups, but that the quantity.x group was affected differently to the control. That much is clear from the pictures, but how we interpret it is something to think about.\nIt’s possible that the Intervention affected both groups, which would suggest that this is a badly chosen control.\nIt’s also possible that some extrinsic factor affected both groups around the time that the intervention happened. That’s not good because it leaves us in a position where we can’t determine how much of the effect is due to the intervention and how much is due to the other factors.\nIn this dummy data, the controls are pretty bad because they change a lot. This is why a lot of the challenge of doing time series\nLet’s finally prove what the control is doing (and not doing) by looking at the numbers. At the end of the study, the fully controlled ITS model estimates that there’s a difference between the factual and counterfactual values of quantity.x\n\ndf6$model.g.predictions[200]-df6$model.i.predictions[200]\n\n      200 \n-79.46584 \n\n\nLet’s then compare that estimate to an estimate from the same data, but uncontrolled.\n\nmodel.j = gls(quantity.x ~ Time + Intervention  + Post.intervention.time , data = df.x,method=\"ML\", correlation= corARMA(p=2,q=2, form = ~ Time))\n\ndf.x <- df.x %>% \n  mutate(\n  model.j.predictions = predictSE.gls (model.j, newdata = df.x, se.fit=T)$fit,\n  model.j.se = predictSE.gls (model.j, df.x, se.fit=T)$se  \n  )\n\ndf8<-filter(df.x,Time<51)\n\nmodel.k = gls(quantity.x ~ Time, data = df8, correlation= corARMA(p=1, q=1, form = ~ Time),method=\"ML\")\n\ndf.x<-df.x %>% mutate(\n  model.k.predictions = predictSE.gls (model.k, newdata = df.x, se.fit=T)$fit,\n  model.k.se = predictSE.gls (model.k, df.x, se.fit=T)$se\n  )\n\ndf.x$model.j.predictions[100]-df.x$model.k.predictions[100]\n\n      100 \n-78.41196 \n\n\nThere’s not much in it here. In both cases, we estimate a change of around 78 to 79 units in the value of quantity.x Really the controls are there to give you a subjective (eyeballing the charts) and statistical view (looking at the summary tables) of whether you should worry that extrinsic factors might have led you to incorrectly attributing the results to your intervention. In my opinion this is a bit of a silly game because you can’t really ever know whether a control group is being affected by the intervention, the extrinsic factor, or both. If you’ve defined your control group before you start, you might be shooting yourself in the foot by choosing the wrong thing. If you haven’t you might be cherry picking a control group simply because it doesn’t change when you, for instance, check it with an uncorrected time series analysis.\nIn the former case, you don’t know what to do with the data, nor know how to interpret the effects. In the latter case, there’s not really a lot of point, because a control group you’ve specifically cherry-picked for lack of any effect won’t contribute much to the model and so has little value in the statistics or analysis.\nControlled ITS is pretty much the gold standard, though personally I don’t think it often adds much to use the control. Some people like to use synthesised controls, which is probably more robust."
  },
  {
    "objectID": "Interrupted_Time_Series.html#final-word",
    "href": "Interrupted_Time_Series.html#final-word",
    "title": "7  A pragmatic Introduction to Interrupted Time Series",
    "section": "7.5 Final word",
    "text": "7.5 Final word\nYou should now be able to do a fairly robust interrupted time series, using controls if you want and calculating counterfactuals for each part of the model. Why not try modifying the controlled ITS to have two interruptions? Or maybe add some periodic covariates, adding effects for seasons, or specific calendar events, or temperature, humidity, region…"
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#make-new-functions",
    "href": "new_lm_predict_left_predict_right.html#make-new-functions",
    "title": "8  New lm Predict Left & Predict Right",
    "section": "8.1 Make new functions",
    "text": "8.1 Make new functions\nFirst we create a pair of new functions which decorate an lm object to the right.\n\nlm_right <- function(formula,data,...){\n                                      mod <- lm(formula,data)\n                                      class(mod) <- c('lm_right',class(mod))\n                                      mod\n                                      }\n\nand another that decorates an lm object to the left.\n\n  lm_left <- function(formula,data,...){\n                                        mod <- lm(formula,data)\n                                        class(mod) <- c('lm_left',class(mod))\n                                        mod\n                                        }\n\nThen create a new function which truncates the data of a model to the right of a defined point\n\n  predictdf.lm_right <- \n    function(model, xseq, se, level){\n                            init_range = range(model$model$x)\n                            xseq <- xseq[xseq >=init_range[1]]\n                            ggplot2:::predictdf.default(model, xseq[-length(xseq)], se, level)\n                                    }\n\nand a counterpart which does the same, but to the left\n\n predictdf.lm_left <- \n    function(model, xseq, se, level){\n                            init_range = range(model$model$x)\n                            xseq <- xseq[xseq <=init_range[2]]\n                            ggplot2:::predictdf.default(model, xseq[-length(xseq)], se, level)\n                                    }\n\nNow we can apply the new functions to a dummy dataset to get a feel for what they do"
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#dummy-data-set",
    "href": "new_lm_predict_left_predict_right.html#dummy-data-set",
    "title": "8  New lm Predict Left & Predict Right",
    "section": "8.2 Dummy Data Set",
    "text": "8.2 Dummy Data Set\nThe dummy data set is a simulated time series where some kind of intervention took place at time point 85. The variable intv shows if the intervention has happened, whilst intv_trend counts the time elapsed since the intervention. Time is the study time from the first point. count is a measurable outcome.\n\nint = 85\nset.seed(42)\n\ndf <- data.frame(\n                count = as.integer(rpois(132, 9) + rnorm(132, 1, 1)),\n                time = 1:132,  \n                at_risk = rep(\n                          c(4305, 4251, 4478, 4535, 4758, 4843, 4893, 4673, 4522, 4454, 4351),\n                          each  = 12\n                             )\n                ) %>% \n  mutate(\n    month = rep(factor(month.name, levels = month.name),11),\n    intv = ifelse(time >= int, 1, 0),\n    intv_trend = c(rep(0, (int - 1)),1:(length(unique(time)) - (int - 1))),\n    lag_count = dplyr::lag(count)\n  )\nhead(df)\n\n  count time at_risk    month intv intv_trend lag_count\n1    14    1    4305  January    0          0        NA\n2    16    2    4305 February    0          0        14\n3     8    3    4305    March    0          0        16\n4    13    4    4305    April    0          0         8\n5     9    5    4305      May    0          0        13\n6     9    6    4305     June    0          0         9"
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#apply-a-model-to-the-data",
    "href": "new_lm_predict_left_predict_right.html#apply-a-model-to-the-data",
    "title": "8  New lm Predict Left & Predict Right",
    "section": "8.3 Apply a model to the data",
    "text": "8.3 Apply a model to the data\n\nfit <- glm(\n          count ~ month + time + intv + intv_trend + log(lag_count) + offset(log(at_risk)),\n          family = \"poisson\",\n          data = df\n          )\n\nsummary(fit)\n\n\nCall:\nglm(formula = count ~ month + time + intv + intv_trend + log(lag_count) + \n    offset(log(at_risk)), family = \"poisson\", data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3196  -0.5288  -0.0629   0.5677   1.9655  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)    -5.819498   0.215788 -26.969  < 2e-16 ***\nmonthFebruary  -0.041578   0.144861  -0.287 0.774095    \nmonthMarch     -0.012683   0.144400  -0.088 0.930007    \nmonthApril      0.177167   0.137702   1.287 0.198234    \nmonthMay       -0.008227   0.143182  -0.057 0.954180    \nmonthJune      -0.139175   0.148671  -0.936 0.349209    \nmonthJuly      -0.076264   0.147079  -0.519 0.604092    \nmonthAugust    -0.003828   0.144473  -0.026 0.978864    \nmonthSeptember -0.013810   0.145474  -0.095 0.924368    \nmonthOctober    0.101205   0.141138   0.717 0.473338    \nmonthNovember   0.071865   0.141538   0.508 0.611634    \nmonthDecember   0.166069   0.139051   1.194 0.232358    \ntime           -0.006706   0.001521  -4.409 1.04e-05 ***\nintv            0.123208   0.123167   1.000 0.317149    \nintv_trend      0.013809   0.003776   3.657 0.000255 ***\nlog(lag_count) -0.036098   0.067583  -0.534 0.593249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 175.48  on 130  degrees of freedom\nResidual deviance: 141.17  on 115  degrees of freedom\n  (1 observation deleted due to missingness)\nAIC: 703.88\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#predict-extrapolate-data",
    "href": "new_lm_predict_left_predict_right.html#predict-extrapolate-data",
    "title": "8  New lm Predict Left & Predict Right",
    "section": "8.4 Predict / Extrapolate data",
    "text": "8.4 Predict / Extrapolate data\nLet’s split the data in to three ‘phases’ including “Pre-intervention”,“Intervention” and “Post-Intervention”. We’ll then predict the direction of travel on the model using predict\n\ndf$group = rep(c(\"Control\",\"PreIntervention\",\"Intervention\",\"PostIntervention\"), c(30,32, 35,35))\n\n\ndf$predict = c(NA, predict(fit, type=\"response\"))\n\nkable(df[50:70,])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncount\ntime\nat_risk\nmonth\nintv\nintv_trend\nlag_count\ngroup\npredict\n\n\n\n\n50\n10\n50\n4758\nFebruary\n0\n0\n14\nPreIntervention\n8.810524\n\n\n51\n8\n51\n4758\nMarch\n0\n0\n10\nPreIntervention\n9.118286\n\n\n52\n11\n52\n4758\nApril\n0\n0\n8\nPreIntervention\n11.039525\n\n\n53\n7\n53\n4758\nMay\n0\n0\n11\nPreIntervention\n9.005955\n\n\n54\n12\n54\n4758\nJune\n0\n0\n7\nPreIntervention\n7.976886\n\n\n55\n3\n55\n4758\nJuly\n0\n0\n12\nPreIntervention\n8.275475\n\n\n56\n10\n56\n4758\nAugust\n0\n0\n3\nPreIntervention\n9.291217\n\n\n57\n11\n57\n4758\nSeptember\n0\n0\n10\nPreIntervention\n8.748828\n\n\n58\n6\n58\n4758\nOctober\n0\n0\n11\nPreIntervention\n9.716138\n\n\n59\n7\n59\n4758\nNovember\n0\n0\n6\nPreIntervention\n9.579476\n\n\n60\n9\n60\n4758\nDecember\n0\n0\n7\nPreIntervention\n10.397414\n\n\n61\n10\n61\n4843\nJanuary\n0\n0\n9\nPreIntervention\n8.823483\n\n\n62\n14\n62\n4843\nFebruary\n0\n0\n10\nPreIntervention\n8.375655\n\n\n63\n10\n63\n4843\nMarch\n0\n0\n14\nIntervention\n8.460194\n\n\n64\n10\n64\n4843\nApril\n0\n0\n10\nIntervention\n10.284761\n\n\n65\n13\n65\n4843\nMay\n0\n0\n10\nIntervention\n8.487231\n\n\n66\n6\n66\n4843\nJune\n0\n0\n13\nIntervention\n7.326063\n\n\n67\n8\n67\n4843\nJuly\n0\n0\n6\nIntervention\n7.968960\n\n\n68\n14\n68\n4843\nAugust\n0\n0\n8\nIntervention\n8.422441\n\n\n69\n12\n69\n4843\nSeptember\n0\n0\n14\nIntervention\n8.117401\n\n\n70\n6\n70\n4843\nOctober\n0\n0\n12\nIntervention\n9.096444"
  },
  {
    "objectID": "new_lm_predict_left_predict_right.html#plot-models",
    "href": "new_lm_predict_left_predict_right.html#plot-models",
    "title": "8  New lm Predict Left & Predict Right",
    "section": "8.5 Plot models",
    "text": "8.5 Plot models\nA strength of this approach is that the new functions act like geoms in a ggplot.\nHere, we plot the pre-intervention trend, its extrapolation to the right (using method “lm_right”). We also show the post-intervention trend, extrapolating it backwards to the left (using method “lm_left”).\n\n  ggplot(data = df, aes(x = time, y = predict)) +\n\n    geom_line() +\n    geom_smooth(data=filter(df,group==\"Control\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"PreIntervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"Intervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"PostIntervention\"),method=\"lm\", se=TRUE, aes(colour=group),fullrange=FALSE)+\n    geom_smooth(data=filter(df,group==\"Intervention\"),method=\"lm_left\", se=TRUE, aes(colour=group),fullrange=TRUE, linetype = \"dashed\",alpha=0.1)+\n    geom_smooth(data=filter(df,group==\"PreIntervention\"),method=\"lm_right\", se=TRUE, aes(colour=group),fullrange=TRUE, linetype = \"dashed\",alpha=0.1)\n\nWarning: Removed 1 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values (`geom_line()`).\n\n\n\n\n\nThis is a neat trick, but in most circumstances you can probably make good use of an extrapolation from the truncated data as described in the section on Interrupted Time Series Analysis"
  },
  {
    "objectID": "power_calc_gen_case_control.html#background",
    "href": "power_calc_gen_case_control.html#background",
    "title": "9  Genetic Power Calculator",
    "section": "9.1 Background",
    "text": "9.1 Background\nThese power calculations are useful for genetic case control studies They use the gap package and pbsize2 command\ncitation Jing Hua Zhao (2013). gap: Genetic Analysis Package. R package version 1.1-10"
  },
  {
    "objectID": "power_calc_gen_case_control.html#libraries",
    "href": "power_calc_gen_case_control.html#libraries",
    "title": "9  Genetic Power Calculator",
    "section": "9.2 Libraries",
    "text": "9.2 Libraries\n\nlibrary(gap)\n\nLoading required package: gap.datasets\n\n\ngap version 1.5-1\n\nlibrary(knitr)"
  },
  {
    "objectID": "power_calc_gen_case_control.html#usage",
    "href": "power_calc_gen_case_control.html#usage",
    "title": "9  Genetic Power Calculator",
    "section": "9.3 Usage",
    "text": "9.3 Usage\n\n9.3.1 Define study characteristics\n\nprevalence=0.1\nalpha=0.05\nnum.cases=2000\nnum.controls=2000\nnum.total=num.cases+num.controls\nprop.cases=num.cases/num.total\n\n\n\n9.3.2 Define ranges of minor allele frequency and effect sizes\n\nmaf_values<-seq(0.01,0.49,by=0.01)\nrisk_values<-seq(1,1.6,by=0.01)\n\n\n\n9.3.3 Calculate power at all possible combinations\n\nx<-matrix(nrow=length(risk_values),ncol=length(maf_values))\n\nfor (i in 1:length(risk_values)){risk = risk_values[i];for (j in 1:length(maf_values)){maf = maf_values[j];try(x[i,j]<-pbsize2(N=num.total,fc=prop.cases,alpha=alpha,gamma=risk,p=maf,kp=prevalence,model=\"additive\"))}}\n\n\n\n9.3.4 Define a matrix which contains the results\n\nxx<-as.data.frame(x,row.names=as.character(risk_values));names(xx)<-as.character(maf_values)\n\nkable(xx)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18\n0.19\n0.2\n0.21\n0.22\n0.23\n0.24\n0.25\n0.26\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0.37\n0.38\n0.39\n0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n\n\n\n\n1\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n0.0500000\n\n\n1.01\n0.0501394\n0.0502758\n0.0504094\n0.0505402\n0.0506680\n0.0507930\n0.0509151\n0.0510343\n0.0511507\n0.0512642\n0.0513748\n0.0514826\n0.0515875\n0.0516896\n0.0517887\n0.0518851\n0.0519786\n0.0520692\n0.0521569\n0.0522418\n0.0523239\n0.0524031\n0.0524795\n0.0525530\n0.0526237\n0.0526916\n0.0527566\n0.0528187\n0.0528781\n0.0529346\n0.0529882\n0.0530391\n0.0530871\n0.0531323\n0.0531747\n0.0532142\n0.0532509\n0.0532849\n0.0533160\n0.0533443\n0.0533698\n0.0533924\n0.0534123\n0.0534294\n0.0534437\n0.0534551\n0.0534638\n0.0534697\n0.0534728\n\n\n1.02\n0.0505549\n0.0510983\n0.0516302\n0.0521506\n0.0526594\n0.0531566\n0.0536424\n0.0541166\n0.0545792\n0.0550304\n0.0554700\n0.0558980\n0.0563146\n0.0567196\n0.0571131\n0.0574951\n0.0578655\n0.0582245\n0.0585720\n0.0589080\n0.0592325\n0.0595455\n0.0598471\n0.0601372\n0.0604159\n0.0606832\n0.0609390\n0.0611834\n0.0614165\n0.0616381\n0.0618483\n0.0620472\n0.0622348\n0.0624110\n0.0625759\n0.0627295\n0.0628718\n0.0630028\n0.0631226\n0.0632312\n0.0633285\n0.0634146\n0.0634895\n0.0635532\n0.0636058\n0.0636473\n0.0636776\n0.0636968\n0.0637050\n\n\n1.03\n0.0512433\n0.0524609\n0.0536530\n0.0548193\n0.0559599\n0.0570746\n0.0581635\n0.0592265\n0.0602636\n0.0612747\n0.0622599\n0.0632190\n0.0641521\n0.0650591\n0.0659401\n0.0667951\n0.0676240\n0.0684268\n0.0692036\n0.0699543\n0.0706789\n0.0713776\n0.0720502\n0.0726969\n0.0733176\n0.0739123\n0.0744811\n0.0750240\n0.0755410\n0.0760322\n0.0764976\n0.0769372\n0.0773511\n0.0777394\n0.0781020\n0.0784390\n0.0787504\n0.0790364\n0.0792969\n0.0795320\n0.0797417\n0.0799262\n0.0800854\n0.0802195\n0.0803285\n0.0804124\n0.0804713\n0.0805052\n0.0805144\n\n\n1.04\n0.0522012\n0.0543582\n0.0564707\n0.0585383\n0.0605609\n0.0625382\n0.0644700\n0.0663560\n0.0681962\n0.0699903\n0.0717382\n0.0734398\n0.0750949\n0.0767036\n0.0782656\n0.0797810\n0.0812497\n0.0826717\n0.0840469\n0.0853752\n0.0866568\n0.0878917\n0.0890797\n0.0902210\n0.0913156\n0.0923635\n0.0933648\n0.0943196\n0.0952279\n0.0960897\n0.0969052\n0.0976744\n0.0983974\n0.0990744\n0.0997053\n0.1002904\n0.1008298\n0.1013234\n0.1017716\n0.1021743\n0.1025317\n0.1028440\n0.1031113\n0.1033337\n0.1035114\n0.1036444\n0.1037331\n0.1037774\n0.1037776\n\n\n1.05\n0.0534259\n0.0567856\n0.0600782\n0.0633027\n0.0664583\n0.0695444\n0.0725602\n0.0755051\n0.0783785\n0.0811802\n0.0839095\n0.0865661\n0.0891497\n0.0916600\n0.0940969\n0.0964600\n0.0987492\n0.1009645\n0.1031057\n0.1051728\n0.1071657\n0.1090844\n0.1109290\n0.1126995\n0.1143960\n0.1160185\n0.1175672\n0.1190422\n0.1204436\n0.1217716\n0.1230263\n0.1242080\n0.1253168\n0.1263529\n0.1273165\n0.1282079\n0.1290273\n0.1297749\n0.1304510\n0.1310558\n0.1315896\n0.1320527\n0.1324453\n0.1327677\n0.1330203\n0.1332032\n0.1333168\n0.1333614\n0.1333372\n\n\n1.06\n0.0549148\n0.0597397\n0.0644721\n0.0691099\n0.0736510\n0.0780937\n0.0824362\n0.0866773\n0.0908155\n0.0948499\n0.0987794\n0.1026031\n0.1063204\n0.1099307\n0.1134334\n0.1168282\n0.1201146\n0.1232926\n0.1263620\n0.1293225\n0.1321743\n0.1349173\n0.1375517\n0.1400775\n0.1424949\n0.1448042\n0.1470055\n0.1490993\n0.1510857\n0.1529652\n0.1547380\n0.1564046\n0.1579654\n0.1594208\n0.1607712\n0.1620171\n0.1631589\n0.1641971\n0.1651322\n0.1659646\n0.1666949\n0.1673235\n0.1678510\n0.1682778\n0.1686045\n0.1688316\n0.1689595\n0.1689889\n0.1689201\n\n\n1.07\n0.0566656\n0.0632174\n0.0696502\n0.0759589\n0.0821393\n0.0881877\n0.0941006\n0.0998752\n0.1055088\n0.1109993\n0.1163447\n0.1215436\n0.1265945\n0.1314963\n0.1362483\n0.1408497\n0.1453001\n0.1495991\n0.1537466\n0.1577426\n0.1615871\n0.1652803\n0.1688226\n0.1722142\n0.1754558\n0.1785477\n0.1814906\n0.1842851\n0.1869319\n0.1894316\n0.1917852\n0.1939932\n0.1960566\n0.1979761\n0.1997525\n0.2013868\n0.2028797\n0.2042321\n0.2054449\n0.2065189\n0.2074550\n0.2082540\n0.2089168\n0.2094442\n0.2098370\n0.2100962\n0.2102224\n0.2102166\n0.2100794\n\n\n1.08\n0.0586762\n0.0672167\n0.0756109\n0.0838493\n0.0919236\n0.0998267\n0.1075524\n0.1150951\n0.1224505\n0.1296145\n0.1365840\n0.1433562\n0.1499292\n0.1563012\n0.1624709\n0.1684376\n0.1742007\n0.1797600\n0.1851154\n0.1902674\n0.1952163\n0.1999629\n0.2045079\n0.2088523\n0.2129971\n0.2169435\n0.2206928\n0.2242462\n0.2276052\n0.2307711\n0.2337453\n0.2365294\n0.2391248\n0.2415330\n0.2437555\n0.2457938\n0.2476493\n0.2493235\n0.2508178\n0.2521337\n0.2532725\n0.2542357\n0.2550244\n0.2556402\n0.2560842\n0.2563577\n0.2564618\n0.2563979\n0.2561670\n\n\n1.09\n0.0609447\n0.0717356\n0.0823531\n0.0927803\n0.1030025\n0.1130072\n0.1227836\n0.1323228\n0.1416172\n0.1506607\n0.1594481\n0.1679757\n0.1762403\n0.1842398\n0.1919728\n0.1994385\n0.2066365\n0.2135673\n0.2202314\n0.2266299\n0.2327642\n0.2386358\n0.2442467\n0.2495988\n0.2546944\n0.2595358\n0.2641253\n0.2684655\n0.2725589\n0.2764080\n0.2800153\n0.2833835\n0.2865152\n0.2894127\n0.2920786\n0.2945154\n0.2967254\n0.2987109\n0.3004743\n0.3020178\n0.3033434\n0.3044532\n0.3053493\n0.3060334\n0.3065075\n0.3067734\n0.3068326\n0.3066869\n0.3063377\n\n\n1.1\n0.0634695\n0.0767725\n0.0898755\n0.1027499\n0.1153713\n0.1277191\n0.1397763\n0.1515287\n0.1629649\n0.1740757\n0.1848540\n0.1952944\n0.2053934\n0.2151485\n0.2245584\n0.2336231\n0.2423432\n0.2507201\n0.2587559\n0.2664532\n0.2738149\n0.2808446\n0.2875457\n0.2939223\n0.2999783\n0.3057180\n0.3111456\n0.3162653\n0.3210815\n0.3255985\n0.3298204\n0.3337516\n0.3373960\n0.3407576\n0.3438405\n0.3466482\n0.3491845\n0.3514529\n0.3534568\n0.3551993\n0.3566836\n0.3579125\n0.3588890\n0.3596155\n0.3600946\n0.3603287\n0.3603198\n0.3600702\n0.3595817\n\n\n1.11\n0.0662489\n0.0823259\n0.0981762\n0.1137539\n0.1290205\n0.1439443\n0.1584994\n0.1726649\n0.1864244\n0.1997654\n0.2126787\n0.2251578\n0.2371987\n0.2487996\n0.2599605\n0.2706827\n0.2809689\n0.2908228\n0.3002490\n0.3092526\n0.3178395\n0.3260159\n0.3337882\n0.3411633\n0.3481479\n0.3547491\n0.3609738\n0.3668288\n0.3723210\n0.3774572\n0.3822438\n0.3866873\n0.3907937\n0.3945692\n0.3980193\n0.4011496\n0.4039654\n0.4064715\n0.4086729\n0.4105738\n0.4121786\n0.4134912\n0.4145154\n0.4152544\n0.4157116\n0.4158900\n0.4157921\n0.4154206\n0.4147777\n\n\n1.12\n0.0692815\n0.0883940\n0.1072521\n0.1257848\n0.1439344\n0.1616540\n0.1789061\n0.1956617\n0.2118987\n0.2276008\n0.2427572\n0.2573610\n0.2714092\n0.2849018\n0.2978410\n0.3102315\n0.3220793\n0.3333921\n0.3441784\n0.3544476\n0.3642096\n0.3734749\n0.3822541\n0.3905581\n0.3983976\n0.4057835\n0.4127264\n0.4192366\n0.4253244\n0.4309996\n0.4362718\n0.4411501\n0.4456434\n0.4497600\n0.4535079\n0.4568947\n0.4599275\n0.4626132\n0.4649579\n0.4669677\n0.4686480\n0.4700038\n0.4710400\n0.4717607\n0.4721700\n0.4722713\n0.4720679\n0.4715625\n0.4707578\n\n\n1.13\n0.0725659\n0.0949751\n0.1170986\n0.1388317\n0.1600902\n0.1808077\n0.2009328\n0.2204265\n0.2392608\n0.2574164\n0.2748815\n0.2916506\n0.3077232\n0.3231030\n0.3377969\n0.3518148\n0.3651682\n0.3778706\n0.3899366\n0.4013815\n0.4122212\n0.4224721\n0.4321504\n0.4412723\n0.4498540\n0.4579112\n0.4654592\n0.4725128\n0.4790865\n0.4851940\n0.4908484\n0.4960624\n0.5008476\n0.5052156\n0.5091767\n0.5127411\n0.5159179\n0.5187159\n0.5211431\n0.5232071\n0.5249145\n0.5262717\n0.5272844\n0.5279576\n0.5282961\n0.5283039\n0.5279845\n0.5273410\n0.5263761\n\n\n1.14\n0.0761009\n0.1020668\n0.1277094\n0.1528786\n0.1774564\n0.2013520\n0.2244972\n0.2468425\n0.2683541\n0.2890110\n0.3088028\n0.3277274\n0.3457894\n0.3629992\n0.3793711\n0.3949226\n0.4096738\n0.4236464\n0.4368632\n0.4493479\n0.4611242\n0.4722160\n0.4826469\n0.4924401\n0.5016182\n0.5102030\n0.5182156\n0.5256761\n0.5326038\n0.5390170\n0.5449330\n0.5503682\n0.5553378\n0.5598564\n0.5639374\n0.5675933\n0.5708357\n0.5736753\n0.5761219\n0.5781846\n0.5798715\n0.5811900\n0.5821467\n0.5827475\n0.5829976\n0.5829014\n0.5824628\n0.5816849\n0.5805704\n\n\n1.15\n0.0798850\n0.1096662\n0.1390758\n0.1679049\n0.1959929\n0.2232197\n0.2494984\n0.2747688\n0.2989930\n0.3221507\n0.3442357\n0.3652529\n0.3852160\n0.4041452\n0.4220659\n0.4390068\n0.4549994\n0.4700767\n0.4842723\n0.4976204\n0.5101549\n0.5219092\n0.5329156\n0.5432058\n0.5528100\n0.5617572\n0.5700749\n0.5777895\n0.5849256\n0.5915066\n0.5975542\n0.6030890\n0.6081299\n0.6126948\n0.6167999\n0.6204604\n0.6236902\n0.6265019\n0.6289072\n0.6309164\n0.6325391\n0.6337835\n0.6346571\n0.6351663\n0.6353169\n0.6351134\n0.6345598\n0.6336592\n0.6324138\n\n\n1.16\n0.0839170\n0.1177696\n0.1511866\n0.1838840\n0.2156497\n0.2463297\n0.2758169\n0.3040421\n0.3309658\n0.3565724\n0.3808643\n0.4038582\n0.4255813\n0.4460685\n0.4653603\n0.4835005\n0.5005357\n0.5165130\n0.5314800\n0.5454837\n0.5585701\n0.5707838\n0.5821678\n0.5927630\n0.6026084\n0.6117410\n0.6201956\n0.6280048\n0.6351992\n0.6418074\n0.6478559\n0.6533693\n0.6583704\n0.6628801\n0.6669176\n0.6705005\n0.6736448\n0.6763652\n0.6786745\n0.6805847\n0.6821061\n0.6832479\n0.6840182\n0.6844238\n0.6844706\n0.6841634\n0.6835058\n0.6825008\n0.6811502\n\n\n1.17\n0.0881956\n0.1263727\n0.1640275\n0.2007837\n0.2363675\n0.2705871\n0.3033160\n0.3344783\n0.3640380\n0.3919894\n0.4183498\n0.4431534\n0.4664467\n0.4882846\n0.5087274\n0.5278387\n0.5456834\n0.5623265\n0.5778320\n0.5922622\n0.6056771\n0.6181340\n0.6296874\n0.6403889\n0.6502868\n0.6594265\n0.6678502\n0.6755974\n0.6827046\n0.6892054\n0.6951310\n0.7005100\n0.7053685\n0.7097306\n0.7136180\n0.7170506\n0.7200462\n0.7226208\n0.7247889\n0.7265632\n0.7279550\n0.7289738\n0.7296283\n0.7299253\n0.7298709\n0.7294695\n0.7287246\n0.7276388\n0.7262133\n\n\n1.18\n0.0927193\n0.1354700\n0.1775816\n0.2185655\n0.2580770\n0.2958842\n0.3318428\n0.3658757\n0.3979570\n0.4280984\n0.4563395\n0.4827392\n0.5073700\n0.5303127\n0.5516528\n0.5714780\n0.5898757\n0.6069315\n0.6227285\n0.6373463\n0.6508603\n0.6633418\n0.6748578\n0.6854707\n0.6952386\n0.7042154\n0.7124508\n0.7199908\n0.7268774\n0.7331492\n0.7388416\n0.7439867\n0.7486139\n0.7527496\n0.7564178\n0.7596402\n0.7624360\n0.7648225\n0.7668149\n0.7684268\n0.7696697\n0.7705537\n0.7710873\n0.7712775\n0.7711300\n0.7706491\n0.7698378\n0.7686979\n0.7672302\n\n\n1.19\n0.0974867\n0.1450549\n0.1918283\n0.2371848\n0.2806999\n0.3221009\n0.3612308\n0.3980188\n0.4324578\n0.4645866\n0.4944759\n0.5222179\n0.5479185\n0.5716908\n0.5936510\n0.6139152\n0.6325967\n0.6498049\n0.6656439\n0.6802120\n0.6936012\n0.7058975\n0.7171804\n0.7275233\n0.7369940\n0.7456546\n0.7535617\n0.7607673\n0.7673185\n0.7732584\n0.7786256\n0.7834556\n0.7877799\n0.7916271\n0.7950228\n0.7979900\n0.8005488\n0.8027173\n0.8045113\n0.8059444\n0.8070286\n0.8077738\n0.8081884\n0.8082792\n0.8080515\n0.8075089\n0.8066542\n0.8054883\n0.8040112\n\n\n1.2\n0.1024962\n0.1551198\n0.2067440\n0.2565910\n0.3041490\n0.3491070\n0.3913026\n0.4306820\n0.4672685\n0.5011391\n0.5324063\n0.5612050\n0.5876821\n0.6119899\n0.6342803\n0.6547020\n0.6733974\n0.6905018\n0.7061417\n0.7204351\n0.7334911\n0.7454102\n0.7562842\n0.7661973\n0.7752260\n0.7834399\n0.7909020\n0.7976692\n0.8037931\n0.8093199\n0.8142914\n0.8187450\n0.8227142\n0.8262289\n0.8293155\n0.8319978\n0.8342965\n0.8362299\n0.8378138\n0.8390618\n0.8399857\n0.8405952\n0.8408984\n0.8409014\n0.8406090\n0.8400245\n0.8391498\n0.8379851\n0.8365296\n\n\n1.21\n0.1077461\n0.1656557\n0.2223016\n0.2767274\n0.3283292\n0.3767630\n0.4218725\n0.4636342\n0.5021162\n0.5374468\n0.5697920\n0.5993391\n0.6262843\n0.6508252\n0.6731546\n0.6934570\n0.7119065\n0.7286653\n0.7438835\n0.7576987\n0.7702367\n0.7816119\n0.7919279\n0.8012783\n0.8097476\n0.8174115\n0.8243382\n0.8305889\n0.8362181\n0.8412746\n0.8458020\n0.8498391\n0.8534204\n0.8565762\n0.8593335\n0.8617161\n0.8637447\n0.8654373\n0.8668095\n0.8678745\n0.8686437\n0.8691261\n0.8693293\n0.8692590\n0.8689192\n0.8683125\n0.8674400\n0.8663015\n0.8648952\n\n\n1.22\n0.1132344\n0.1766524\n0.2384707\n0.2975320\n0.3531385\n0.4049228\n0.4527499\n0.4966438\n0.5367335\n0.5732136\n0.6063161\n0.6362908\n0.6633912\n0.6878658\n0.7099522\n0.7298735\n0.7478367\n0.7640318\n0.7786318\n0.7917937\n0.8036587\n0.8143541\n0.8239936\n0.8326791\n0.8405012\n0.8475407\n0.8538697\n0.8595519\n0.8646440\n0.8691962\n0.8732529\n0.8768532\n0.8800318\n0.8828191\n0.8852416\n0.8873228\n0.8890827\n0.8905388\n0.8917061\n0.8925971\n0.8932224\n0.8935904\n0.8937079\n0.8935799\n0.8932098\n0.8925994\n0.8917492\n0.8906583\n0.8893241\n\n\n1.23\n0.1189591\n0.1880984\n0.2552176\n0.3189377\n0.3784692\n0.4334357\n0.4837429\n0.5294827\n0.5708637\n0.6081628\n0.6416910\n0.6717702\n0.6987185\n0.7228402\n0.7444206\n0.7637228\n0.7809870\n0.7964304\n0.8102481\n0.8226146\n0.8336853\n0.8435981\n0.8524754\n0.8604251\n0.8675426\n0.8739122\n0.8796080\n0.8846951\n0.8892312\n0.8932664\n0.8968452\n0.9000063\n0.9027836\n0.9052069\n0.9073017\n0.9090906\n0.9105927\n0.9118245\n0.9128000\n0.9135309\n0.9140268\n0.9142954\n0.9143426\n0.9141726\n0.9137882\n0.9131906\n0.9123795\n0.9113532\n0.9101089\n\n\n1.24\n0.1249179\n0.1999805\n0.2725053\n0.3408728\n0.4042090\n0.4621484\n0.5146613\n0.5619309\n0.6042662\n0.6420430\n0.6756638\n0.7055322\n0.7320364\n0.7555404\n0.7763786\n0.7948543\n0.8112401\n0.8257788\n0.8386856\n0.8501504\n0.8603402\n0.8694016\n0.8774631\n0.8846369\n0.8910213\n0.8967020\n0.9017537\n0.9062419\n0.9102234\n0.9137479\n0.9168585\n0.9195927\n0.9219833\n0.9240584\n0.9258426\n0.9273567\n0.9286187\n0.9296440\n0.9304453\n0.9310332\n0.9314163\n0.9316015\n0.9315938\n0.9313967\n0.9310121\n0.9304407\n0.9296816\n0.9287327\n0.9275907\n\n\n1.25\n0.1311085\n0.2122844\n0.2902939\n0.3632618\n0.4302427\n0.4909075\n0.5453199\n0.5937806\n0.6367209\n0.6746322\n0.7080207\n0.7373793\n0.7631712\n0.7858216\n0.8057144\n0.8231919\n0.8385568\n0.8520750\n0.8639791\n0.8744715\n0.8837282\n0.8919017\n0.8991242\n0.9055101\n0.9111585\n0.9161548\n0.9205732\n0.9244776\n0.9279233\n0.9309581\n0.9336235\n0.9359549\n0.9379831\n0.9397347\n0.9412321\n0.9424949\n0.9435393\n0.9443794\n0.9450265\n0.9454901\n0.9457779\n0.9458955\n0.9458473\n0.9456360\n0.9452629\n0.9447281\n0.9440303\n0.9431669\n0.9421342\n\n\n1.26\n0.1375280\n0.2249943\n0.3085409\n0.3860260\n0.4564533\n0.5195620\n0.5755418\n0.6248398\n0.6680318\n0.7057415\n0.7385893\n0.7671636\n0.7920051\n0.8136006\n0.8323818\n0.8487274\n0.8629672\n0.8753868\n0.8862325\n0.8957159\n0.9040190\n0.9112974\n0.9176846\n0.9232949\n0.9282262\n0.9325623\n0.9363750\n0.9397259\n0.9426676\n0.9452455\n0.9474982\n0.9494590\n0.9511563\n0.9526143\n0.9538536\n0.9548919\n0.9557438\n0.9564216\n0.9569356\n0.9572940\n0.9575031\n0.9575681\n0.9574922\n0.9572776\n0.9569250\n0.9564341\n0.9558031\n0.9550292\n0.9541084\n\n\n1.27\n0.1441736\n0.2380932\n0.3272010\n0.4090844\n0.4827238\n0.5479651\n0.6051609\n0.6549347\n0.6980299\n0.7352170\n0.7672397\n0.7947857\n0.8184740\n0.8388513\n0.8563938\n0.8715128\n0.8845611\n0.8958396\n0.9056046\n0.9140734\n0.9214304\n0.9278317\n0.9334098\n0.9382766\n0.9425272\n0.9462421\n0.9494899\n0.9523285\n0.9548074\n0.9569685\n0.9588477\n0.9604751\n0.9618768\n0.9630743\n0.9640864\n0.9649285\n0.9656136\n0.9661525\n0.9665541\n0.9668252\n0.9669716\n0.9669970\n0.9669044\n0.9666951\n0.9663694\n0.9659266\n0.9653646\n0.9646806\n0.9638705\n\n\n1.28\n0.1510420\n0.2515624\n0.3462269\n0.4323544\n0.5089386\n0.5759765\n0.6340241\n0.6839128\n0.7265748\n0.7629407\n0.7938838\n0.8201929\n0.8425636\n0.8615987\n0.8778146\n0.8916500\n0.9034757\n0.9136035\n0.9222953\n0.9297703\n0.9362123\n0.9417753\n0.9465879\n0.9507584\n0.9543773\n0.9575209\n0.9602531\n0.9626278\n0.9646906\n0.9664797\n0.9680276\n0.9693614\n0.9705042\n0.9714754\n0.9722912\n0.9729652\n0.9735088\n0.9739311\n0.9742396\n0.9744403\n0.9745378\n0.9745351\n0.9744344\n0.9742367\n0.9739419\n0.9735489\n0.9730556\n0.9724590\n0.9717550\n\n\n1.29\n0.1581299\n0.2653824\n0.3655692\n0.4557525\n0.5349846\n0.6034643\n0.6619938\n0.7116440\n0.7535556\n0.7888301\n0.8184737\n0.8433755\n0.8643038\n0.8819121\n0.8967510\n0.9092810\n0.9198850\n0.9288810\n0.9365320\n0.9430558\n0.9486324\n0.9534110\n0.9575150\n0.9610468\n0.9640915\n0.9667199\n0.9689909\n0.9709538\n0.9726497\n0.9741130\n0.9753726\n0.9764525\n0.9773730\n0.9781509\n0.9788004\n0.9793331\n0.9797587\n0.9800849\n0.9803181\n0.9804631\n0.9805236\n0.9805021\n0.9804002\n0.9802184\n0.9799563\n0.9796126\n0.9791853\n0.9786713\n0.9780667\n\n\n1.3\n0.1654335\n0.2795320\n0.3851773\n0.4791957\n0.5607531\n0.6303063\n0.6889491\n0.7380216\n0.7788910\n0.8128367\n0.8409987\n0.8643624\n0.8837627\n0.8998971\n0.9133434\n0.9245771\n0.9339879\n0.9418945\n0.9485573\n0.9541890\n0.9589633\n0.9630224\n0.9664827\n0.9694397\n0.9719721\n0.9741445\n0.9760105\n0.9776142\n0.9789923\n0.9801753\n0.9811883\n0.9820525\n0.9827852\n0.9834011\n0.9839120\n0.9843279\n0.9846568\n0.9849053\n0.9850785\n0.9851805\n0.9852140\n0.9851811\n0.9850828\n0.9849194\n0.9846902\n0.9843939\n0.9840283\n0.9835906\n0.9830771\n\n\n1.31\n0.1729487\n0.2939891\n0.4049990\n0.5026015\n0.5861403\n0.6563918\n0.7147866\n0.7629630\n0.8025286\n0.8349444\n0.8614816\n0.8832155\n0.9010398\n0.9156877\n0.9277565\n0.9377297\n0.9459981\n0.9528765\n0.9586186\n0.9634289\n0.9674727\n0.9708834\n0.9737692\n0.9762178\n0.9783008\n0.9800766\n0.9815927\n0.9828884\n0.9839958\n0.9849414\n0.9857471\n0.9864309\n0.9870076\n0.9874896\n0.9878869\n0.9882077\n0.9884588\n0.9886455\n0.9887719\n0.9888413\n0.9888559\n0.9888172\n0.9887258\n0.9885818\n0.9883843\n0.9881321\n0.9878230\n0.9874544\n0.9870227\n\n\n1.32\n0.1806715\n0.3087306\n0.4249817\n0.5258894\n0.6110491\n0.6816222\n0.7394214\n0.7864091\n0.8244428\n0.8551658\n0.8799741\n0.9000244\n0.9162592\n0.9294386\n0.9401709\n0.9489413\n0.9561355\n0.9620601\n0.9669588\n0.9710257\n0.9744153\n0.9772512\n0.9796325\n0.9816387\n0.9833339\n0.9847698\n0.9859885\n0.9870241\n0.9879044\n0.9886522\n0.9892861\n0.9898213\n0.9902703\n0.9906434\n0.9909489\n0.9911936\n0.9913830\n0.9915213\n0.9916119\n0.9916573\n0.9916592\n0.9916187\n0.9915361\n0.9914114\n0.9912436\n0.9910315\n0.9907731\n0.9904658\n0.9901065\n\n\n1.33\n0.1885971\n0.3237322\n0.4450723\n0.5489813\n0.6353896\n0.7059119\n0.7627873\n0.8083232\n0.8446337\n0.8735395\n0.8965527\n0.9149002\n0.9295629\n0.9413177\n0.9507759\n0.9584174\n0.9646179\n0.9696719\n0.9738103\n0.9772145\n0.9800273\n0.9823616\n0.9843066\n0.9859334\n0.9872987\n0.9884479\n0.9894174\n0.9902365\n0.9909291\n0.9915143\n0.9920078\n0.9924224\n0.9927684\n0.9930542\n0.9932866\n0.9934711\n0.9936122\n0.9937132\n0.9937768\n0.9938050\n0.9937989\n0.9937594\n0.9936867\n0.9935803\n0.9934396\n0.9932632\n0.9930493\n0.9927957\n0.9924994\n\n\n1.34\n0.1967209\n0.3389691\n0.4652177\n0.5718023\n0.6590799\n0.7291891\n0.7848360\n0.8286905\n0.8631235\n0.8901257\n0.9113131\n0.9279703\n0.9411046\n0.9514998\n0.9597623\n0.9663604\n0.9716556\n0.9759267\n0.9793896\n0.9822118\n0.9845233\n0.9864257\n0.9879987\n0.9893048\n0.9903935\n0.9913041\n0.9920676\n0.9927090\n0.9932484\n0.9937019\n0.9940823\n0.9944002\n0.9946641\n0.9948808\n0.9950558\n0.9951934\n0.9952973\n0.9953700\n0.9954137\n0.9954298\n0.9954193\n0.9953826\n0.9953197\n0.9952303\n0.9951137\n0.9949685\n0.9947933\n0.9945858\n0.9943437\n\n\n1.35\n0.2050376\n0.3544154\n0.4853651\n0.5942813\n0.6820471\n0.7513955\n0.8055371\n0.8475154\n0.8799545\n0.9050030\n0.9243663\n0.9393726\n0.9510438\n0.9601607\n0.9673172\n0.9729648\n0.9774465\n0.9810232\n0.9838943\n0.9862120\n0.9880937\n0.9896295\n0.9908894\n0.9919280\n0.9927878\n0.9935023\n0.9940978\n0.9945953\n0.9950114\n0.9953593\n0.9956498\n0.9958912\n0.9960905\n0.9962532\n0.9963836\n0.9964852\n0.9965608\n0.9966124\n0.9966417\n0.9966497\n0.9966370\n0.9966041\n0.9965507\n0.9964766\n0.9963810\n0.9962627\n0.9961204\n0.9959522\n0.9957559\n\n\n1.36\n0.2135418\n0.3700448\n0.5054628\n0.6163516\n0.7042271\n0.7724864\n0.8248765\n0.8648203\n0.8951858\n0.9182640\n0.9358334\n0.9492512\n0.9595415\n0.9674728\n0.9736200\n0.9784129\n0.9821732\n0.9851420\n0.9875010\n0.9893874\n0.9909051\n0.9921335\n0.9931334\n0.9939516\n0.9946243\n0.9951797\n0.9956399\n0.9960221\n0.9963401\n0.9966046\n0.9968243\n0.9970060\n0.9971551\n0.9972761\n0.9973723\n0.9974466\n0.9975009\n0.9975370\n0.9975560\n0.9975587\n0.9975456\n0.9975168\n0.9974723\n0.9974115\n0.9973339\n0.9972385\n0.9971239\n0.9969887\n0.9968308\n\n\n1.37\n0.2222280\n0.3858305\n0.5254599\n0.6379513\n0.7255654\n0.7924306\n0.8428559\n0.8806431\n0.9088901\n0.9300117\n0.9458420\n0.9577515\n0.9667553\n0.9736011\n0.9788387\n0.9828727\n0.9860010\n0.9884442\n0.9903656\n0.9918873\n0.9931006\n0.9940744\n0.9948608\n0.9954996\n0.9960212\n0.9964491\n0.9968015\n0.9970925\n0.9973333\n0.9975326\n0.9976973\n0.9978328\n0.9979434\n0.9980325\n0.9981029\n0.9981566\n0.9981952\n0.9982200\n0.9982319\n0.9982314\n0.9982188\n0.9981943\n0.9981576\n0.9981083\n0.9980460\n0.9979696\n0.9978782\n0.9977703\n0.9976442\n\n\n1.38\n0.2310901\n0.4017452\n0.5453072\n0.6590240\n0.7460171\n0.8112093\n0.8594908\n0.8950349\n0.9211507\n0.9403563\n0.9545220\n0.9650174\n0.9728371\n0.9787006\n0.9831281\n0.9864959\n0.9890772\n0.9910710\n0.9926228\n0.9938399\n0.9948015\n0.9955668\n0.9961799\n0.9966743\n0.9970752\n0.9974020\n0.9976695\n0.9978892\n0.9980700\n0.9982188\n0.9983412\n0.9984413\n0.9985226\n0.9985877\n0.9986387\n0.9986771\n0.9987043\n0.9987210\n0.9987281\n0.9987258\n0.9987143\n0.9986938\n0.9986640\n0.9986245\n0.9985748\n0.9985142\n0.9984418\n0.9983563\n0.9982564\n\n\n1.39\n0.2401221\n0.4177614\n0.5649573\n0.6795186\n0.7655466\n0.8288160\n0.8748092\n0.9080577\n0.9320589\n0.9494114\n0.9620030\n0.9711876\n0.9779298\n0.9829145\n0.9866284\n0.9894181\n0.9915309\n0.9931448\n0.9943878\n0.9953532\n0.9961090\n0.9967053\n0.9971793\n0.9975587\n0.9978642\n0.9981117\n0.9983130\n0.9984775\n0.9986121\n0.9987224\n0.9988126\n0.9988860\n0.9989452\n0.9989924\n0.9990290\n0.9990562\n0.9990751\n0.9990862\n0.9990900\n0.9990868\n0.9990768\n0.9990599\n0.9990359\n0.9990046\n0.9989653\n0.9989176\n0.9988607\n0.9987934\n0.9987148\n\n\n1.4\n0.2493174\n0.4338516\n0.5843648\n0.6993899\n0.7841278\n0.8452551\n0.8888500\n0.9197819\n0.9417110\n0.9572919\n0.9684112\n0.9763940\n0.9821660\n0.9863727\n0.9894648\n0.9917579\n0.9934739\n0.9947699\n0.9957576\n0.9965171\n0.9971064\n0.9975673\n0.9979308\n0.9982195\n0.9984505\n0.9986363\n0.9987867\n0.9989088\n0.9990082\n0.9990892\n0.9991552\n0.9992086\n0.9992514\n0.9992852\n0.9993113\n0.9993304\n0.9993433\n0.9993505\n0.9993523\n0.9993489\n0.9993404\n0.9993267\n0.9993076\n0.9992829\n0.9992522\n0.9992149\n0.9991704\n0.9991178\n0.9990563\n\n\n1.41\n0.2586693\n0.4499883\n0.6034868\n0.7185989\n0.8017432\n0.8605412\n0.9016609\n0.9302846\n0.9502058\n0.9641112\n0.9738677\n0.9807597\n0.9856670\n0.9891919\n0.9917475\n0.9936183\n0.9950015\n0.9960342\n0.9968130\n0.9974059\n0.9978617\n0.9982151\n0.9984916\n0.9987096\n0.9988829\n0.9990214\n0.9991328\n0.9992227\n0.9992956\n0.9993546\n0.9994025\n0.9994410\n0.9994717\n0.9994958\n0.9995142\n0.9995275\n0.9995362\n0.9995408\n0.9995413\n0.9995380\n0.9995309\n0.9995199\n0.9995049\n0.9994856\n0.9994617\n0.9994328\n0.9993983\n0.9993575\n0.9993096\n\n\n1.42\n0.2681711\n0.4661442\n0.6222829\n0.7371124\n0.8183843\n0.8746979\n0.9132973\n0.9396469\n0.9576429\n0.9699792\n0.9784864\n0.9843980\n0.9885420\n0.9914751\n0.9935723\n0.9950876\n0.9961942\n0.9970110\n0.9976204\n0.9980797\n0.9984295\n0.9986985\n0.9989072\n0.9990706\n0.9991995\n0.9993019\n0.9993838\n0.9994496\n0.9995026\n0.9995453\n0.9995798\n0.9996074\n0.9996292\n0.9996463\n0.9996591\n0.9996683\n0.9996741\n0.9996769\n0.9996767\n0.9996737\n0.9996679\n0.9996592\n0.9996475\n0.9996326\n0.9996141\n0.9995918\n0.9995652\n0.9995337\n0.9994966\n\n\n1.43\n0.2778154\n0.4822921\n0.6407157\n0.7549031\n0.8340500\n0.8877564\n0.9238203\n0.9479525\n0.9641203\n0.9750009\n0.9823736\n0.9874119\n0.9908884\n0.9933125\n0.9950215\n0.9962401\n0.9971191\n0.9977604\n0.9982338\n0.9985870\n0.9988535\n0.9990567\n0.9992131\n0.9993346\n0.9994298\n0.9995050\n0.9995648\n0.9996125\n0.9996508\n0.9996815\n0.9997061\n0.9997257\n0.9997412\n0.9997531\n0.9997620\n0.9997683\n0.9997721\n0.9997737\n0.9997732\n0.9997706\n0.9997659\n0.9997591\n0.9997500\n0.9997385\n0.9997244\n0.9997073\n0.9996869\n0.9996627\n0.9996342\n\n\n1.44\n0.2875951\n0.4984053\n0.6587505\n0.7719499\n0.8487472\n0.8997550\n0.9332949\n0.9552859\n0.9697332\n0.9792754\n0.9856267\n0.9898940\n0.9927917\n0.9947818\n0.9961651\n0.9971383\n0.9978316\n0.9983317\n0.9986967\n0.9989664\n0.9991679\n0.9993203\n0.9994366\n0.9995263\n0.9995962\n0.9996510\n0.9996943\n0.9997287\n0.9997561\n0.9997781\n0.9997955\n0.9998094\n0.9998202\n0.9998285\n0.9998347\n0.9998389\n0.9998414\n0.9998422\n0.9998416\n0.9998394\n0.9998357\n0.9998304\n0.9998234\n0.9998146\n0.9998039\n0.9997908\n0.9997753\n0.9997568\n0.9997350\n\n\n1.45\n0.2975025\n0.5144578\n0.6763558\n0.7882372\n0.8624890\n0.9107371\n0.9417893\n0.9617307\n0.9745727\n0.9828945\n0.9883339\n0.9919263\n0.9943263\n0.9959496\n0.9970618\n0.9978339\n0.9983770\n0.9987642\n0.9990437\n0.9992482\n0.9993996\n0.9995130\n0.9995989\n0.9996648\n0.9997156\n0.9997553\n0.9997865\n0.9998111\n0.9998306\n0.9998462\n0.9998585\n0.9998682\n0.9998757\n0.9998815\n0.9998857\n0.9998885\n0.9998901\n0.9998905\n0.9998898\n0.9998880\n0.9998851\n0.9998810\n0.9998757\n0.9998691\n0.9998609\n0.9998510\n0.9998392\n0.9998252\n0.9998086\n\n\n1.46\n0.3075301\n0.5304240\n0.6935034\n0.8037552\n0.8752948\n0.9207509\n0.9493728\n0.9673688\n0.9787251\n0.9859429\n0.9905747\n0.9935809\n0.9955565\n0.9968723\n0.9977608\n0.9983693\n0.9987919\n0.9990896\n0.9993023\n0.9994562\n0.9995691\n0.9996530\n0.9997161\n0.9997640\n0.9998008\n0.9998294\n0.9998516\n0.9998691\n0.9998830\n0.9998939\n0.9999025\n0.9999093\n0.9999145\n0.9999185\n0.9999213\n0.9999232\n0.9999241\n0.9999243\n0.9999236\n0.9999222\n0.9999199\n0.9999168\n0.9999128\n0.9999078\n0.9999017\n0.9998942\n0.9998853\n0.9998747\n0.9998621\n\n\n1.47\n0.3176698\n0.5462791\n0.7101681\n0.8184990\n0.8871892\n0.9298475\n0.9561150\n0.9722789\n0.9822707\n0.9884976\n0.9924196\n0.9949206\n0.9965371\n0.9975970\n0.9983023\n0.9987788\n0.9991055\n0.9993330\n0.9994937\n0.9996088\n0.9996925\n0.9997541\n0.9998001\n0.9998348\n0.9998612\n0.9998816\n0.9998974\n0.9999098\n0.9999195\n0.9999272\n0.9999332\n0.9999379\n0.9999415\n0.9999441\n0.9999461\n0.9999473\n0.9999479\n0.9999479\n0.9999473\n0.9999461\n0.9999444\n0.9999421\n0.9999391\n0.9999353\n0.9999307\n0.9999251\n0.9999184\n0.9999104\n0.9999009\n\n\n1.48\n0.3279138\n0.5619993\n0.7263283\n0.8324690\n0.8982014\n0.9380806\n0.9620850\n0.9765361\n0.9852839\n0.9906278\n0.9939306\n0.9959994\n0.9973143\n0.9981630\n0.9987194\n0.9990902\n0.9993412\n0.9995139\n0.9996345\n0.9997201\n0.9997817\n0.9998267\n0.9998600\n0.9998849\n0.9999038\n0.9999183\n0.9999295\n0.9999381\n0.9999449\n0.9999503\n0.9999544\n0.9999576\n0.9999601\n0.9999619\n0.9999632\n0.9999640\n0.9999643\n0.9999642\n0.9999637\n0.9999629\n0.9999615\n0.9999598\n0.9999575\n0.9999547\n0.9999513\n0.9999471\n0.9999421\n0.9999361\n0.9999289\n\n\n1.49\n0.3382540\n0.5775616\n0.7419655\n0.8456701\n0.9083642\n0.9455052\n0.9673499\n0.9802113\n0.9878327\n0.9923953\n0.9951617\n0.9968635\n0.9979270\n0.9986025\n0.9990388\n0.9993256\n0.9995172\n0.9996475\n0.9997375\n0.9998007\n0.9998458\n0.9998785\n0.9999024\n0.9999202\n0.9999337\n0.9999439\n0.9999517\n0.9999578\n0.9999625\n0.9999662\n0.9999690\n0.9999712\n0.9999729\n0.9999741\n0.9999750\n0.9999755\n0.9999756\n0.9999755\n0.9999752\n0.9999745\n0.9999735\n0.9999722\n0.9999705\n0.9999684\n0.9999659\n0.9999628\n0.9999590\n0.9999546\n0.9999492\n\n\n1.5\n0.3486820\n0.5929440\n0.7570643\n0.8581112\n0.9177137\n0.9521768\n0.9719748\n0.9833703\n0.9899789\n0.9938549\n0.9961598\n0.9975521\n0.9984073\n0.9989419\n0.9992821\n0.9995026\n0.9996480\n0.9997457\n0.9998125\n0.9998589\n0.9998917\n0.9999152\n0.9999323\n0.9999450\n0.9999545\n0.9999616\n0.9999671\n0.9999713\n0.9999746\n0.9999771\n0.9999791\n0.9999805\n0.9999817\n0.9999825\n0.9999830\n0.9999834\n0.9999834\n0.9999833\n0.9999830\n0.9999825\n0.9999818\n0.9999808\n0.9999795\n0.9999780\n0.9999761\n0.9999738\n0.9999711\n0.9999677\n0.9999637\n\n\n1.51\n0.3591896\n0.6081256\n0.7716127\n0.8698055\n0.9262883\n0.9581507\n0.9760217\n0.9860744\n0.9917780\n0.9950544\n0.9969650\n0.9980980\n0.9987820\n0.9992027\n0.9994664\n0.9996349\n0.9997446\n0.9998175\n0.9998667\n0.9999005\n0.9999243\n0.9999411\n0.9999533\n0.9999623\n0.9999689\n0.9999739\n0.9999777\n0.9999806\n0.9999828\n0.9999846\n0.9999859\n0.9999869\n0.9999877\n0.9999882\n0.9999886\n0.9999887\n0.9999888\n0.9999887\n0.9999884\n0.9999880\n0.9999875\n0.9999868\n0.9999859\n0.9999847\n0.9999833\n0.9999817\n0.9999796\n0.9999771\n0.9999741\n\n\n1.52\n0.3697684\n0.6230866\n0.7856017\n0.8807691\n0.9341283\n0.9634813\n0.9795492\n0.9883795\n0.9932795\n0.9960358\n0.9976115\n0.9985286\n0.9990727\n0.9994020\n0.9996052\n0.9997333\n0.9998156\n0.9998696\n0.9999057\n0.9999302\n0.9999473\n0.9999593\n0.9999679\n0.9999742\n0.9999788\n0.9999823\n0.9999849\n0.9999869\n0.9999885\n0.9999896\n0.9999905\n0.9999912\n0.9999917\n0.9999921\n0.9999923\n0.9999924\n0.9999924\n0.9999923\n0.9999921\n0.9999919\n0.9999915\n0.9999909\n0.9999903\n0.9999894\n0.9999884\n0.9999872\n0.9999857\n0.9999838\n0.9999816\n\n\n1.53\n0.3804099\n0.6378087\n0.7990252\n0.8910214\n0.9412750\n0.9682217\n0.9826126\n0.9903364\n0.9945272\n0.9968349\n0.9981280\n0.9988666\n0.9992972\n0.9995535\n0.9997093\n0.9998061\n0.9998675\n0.9999073\n0.9999336\n0.9999513\n0.9999635\n0.9999720\n0.9999781\n0.9999825\n0.9999857\n0.9999881\n0.9999899\n0.9999912\n0.9999923\n0.9999931\n0.9999937\n0.9999941\n0.9999945\n0.9999947\n0.9999948\n0.9999949\n0.9999949\n0.9999948\n0.9999947\n0.9999945\n0.9999942\n0.9999938\n0.9999933\n0.9999927\n0.9999920\n0.9999910\n0.9999899\n0.9999886\n0.9999869\n\n\n1.54\n0.3911058\n0.6522744\n0.8118800\n0.9005844\n0.9477703\n0.9724231\n0.9852629\n0.9919911\n0.9955595\n0.9974827\n0.9985388\n0.9991307\n0.9994696\n0.9996681\n0.9997869\n0.9998597\n0.9999052\n0.9999344\n0.9999534\n0.9999662\n0.9999748\n0.9999808\n0.9999851\n0.9999881\n0.9999903\n0.9999920\n0.9999932\n0.9999941\n0.9999948\n0.9999954\n0.9999958\n0.9999961\n0.9999963\n0.9999965\n0.9999966\n0.9999966\n0.9999966\n0.9999965\n0.9999964\n0.9999962\n0.9999960\n0.9999958\n0.9999954\n0.9999950\n0.9999944\n0.9999938\n0.9999929\n0.9999920\n0.9999907\n\n\n1.55\n0.4018474\n0.6664678\n0.8241657\n0.9094823\n0.9536565\n0.9761345\n0.9875476\n0.9933848\n0.9964102\n0.9980056\n0.9988640\n0.9993360\n0.9996014\n0.9997543\n0.9998445\n0.9998989\n0.9999325\n0.9999538\n0.9999675\n0.9999766\n0.9999827\n0.9999869\n0.9999899\n0.9999920\n0.9999935\n0.9999946\n0.9999955\n0.9999961\n0.9999966\n0.9999969\n0.9999972\n0.9999974\n0.9999975\n0.9999976\n0.9999977\n0.9999977\n0.9999977\n0.9999977\n0.9999976\n0.9999975\n0.9999973\n0.9999971\n0.9999968\n0.9999965\n0.9999961\n0.9999957\n0.9999951\n0.9999943\n0.9999934\n\n\n1.56\n0.4126264\n0.6803742\n0.8358843\n0.9177408\n0.9589751\n0.9794022\n0.9895101\n0.9945541\n0.9971082\n0.9984259\n0.9991203\n0.9994948\n0.9997018\n0.9998189\n0.9998870\n0.9999275\n0.9999521\n0.9999676\n0.9999774\n0.9999839\n0.9999882\n0.9999911\n0.9999932\n0.9999946\n0.9999957\n0.9999964\n0.9999970\n0.9999974\n0.9999977\n0.9999980\n0.9999981\n0.9999983\n0.9999984\n0.9999984\n0.9999985\n0.9999985\n0.9999985\n0.9999984\n0.9999984\n0.9999983\n0.9999982\n0.9999980\n0.9999978\n0.9999976\n0.9999973\n0.9999970\n0.9999966\n0.9999960\n0.9999954\n\n\n1.57\n0.4234342\n0.6939803\n0.8470401\n0.9253874\n0.9637673\n0.9822702\n0.9911898\n0.9955314\n0.9976787\n0.9987622\n0.9993214\n0.9996173\n0.9997778\n0.9998671\n0.9999182\n0.9999482\n0.9999662\n0.9999773\n0.9999844\n0.9999889\n0.9999920\n0.9999940\n0.9999954\n0.9999964\n0.9999971\n0.9999976\n0.9999980\n0.9999983\n0.9999985\n0.9999987\n0.9999988\n0.9999989\n0.9999989\n0.9999990\n0.9999990\n0.9999990\n0.9999990\n0.9999990\n0.9999989\n0.9999988\n0.9999988\n0.9999987\n0.9999985\n0.9999984\n0.9999982\n0.9999979\n0.9999976\n0.9999972\n0.9999967\n\n\n1.58\n0.4342624\n0.7072742\n0.8576399\n0.9324505\n0.9680730\n0.9847793\n0.9926225\n0.9963452\n0.9981431\n0.9990302\n0.9994786\n0.9997111\n0.9998351\n0.9999029\n0.9999411\n0.9999631\n0.9999762\n0.9999842\n0.9999892\n0.9999924\n0.9999945\n0.9999960\n0.9999969\n0.9999976\n0.9999981\n0.9999984\n0.9999987\n0.9999989\n0.9999990\n0.9999991\n0.9999992\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999993\n0.9999992\n0.9999992\n0.9999991\n0.9999990\n0.9999989\n0.9999987\n0.9999986\n0.9999983\n0.9999980\n0.9999977\n\n\n1.59\n0.4451027\n0.7202450\n0.8676921\n0.9389591\n0.9719310\n0.9869677\n0.9938403\n0.9970203\n0.9985197\n0.9992429\n0.9996008\n0.9997828\n0.9998781\n0.9999293\n0.9999577\n0.9999739\n0.9999834\n0.9999891\n0.9999926\n0.9999949\n0.9999963\n0.9999973\n0.9999979\n0.9999984\n0.9999987\n0.9999990\n0.9999991\n0.9999993\n0.9999994\n0.9999994\n0.9999995\n0.9999995\n0.9999995\n0.9999996\n0.9999996\n0.9999996\n0.9999996\n0.9999995\n0.9999995\n0.9999995\n0.9999994\n0.9999994\n0.9999993\n0.9999992\n0.9999991\n0.9999990\n0.9999988\n0.9999986\n0.9999984\n\n\n1.6\n0.4559468\n0.7328836\n0.8772072\n0.9449428\n0.9753783\n0.9888704\n0.9948720\n0.9975784\n0.9988238\n0.9994111\n0.9996955\n0.9998374\n0.9999102\n0.9999487\n0.9999698\n0.9999816\n0.9999884\n0.9999925\n0.9999950\n0.9999965\n0.9999975\n0.9999982\n0.9999986\n0.9999989\n0.9999992\n0.9999993\n0.9999994\n0.9999995\n0.9999996\n0.9999996\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999997\n0.9999996\n0.9999996\n0.9999995\n0.9999995\n0.9999994\n0.9999993\n0.9999992\n0.9999990\n0.9999989\n\n\n\n\n\n\n\n9.3.5 Define a palette for the results\n\n#set the colours used for the plot\nwut.colors<-rev(c(\"red\",\"magenta\",\"pink\",\"orange\",\"yellow\",\"green\",\"dark green\", \"blue\",\"purple\",\"dark blue\"))\n\n\n#print plot of power\n\nimage(risk_values,maf_values,as.matrix(xx),zlim=c(0,1),col= wut.colors,breaks=seq(0,1,by=0.1),useRaster=T,main=paste(\"Power @ Prevalence :\",prevalence,\"| Cases :\",num.cases,\"| Controls :\",num.controls,\"| alpha :\",alpha),xlab=\"Relative Risk\",ylab=\"Minor allele frequency\",cex.main=0.8);legend(x=1,y=0.4,cex=0.5,legend=c(\"0.00-0.09\",\"0.10-0.19\",\"0.20-0.29\",\"0.30-0.39\",\"0.40-0.49\",\"0.50-0.59\",\"0.60-0.69\",\"0.70-0.79\",\"0.80-0.89\",\"0.90-1.00\"),fill=wut.colors,title=\"Power\")"
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#background",
    "href": "Diagnostics_Power_Calculation.html#background",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.1 Background",
    "text": "10.1 Background\nFor the findings of a proposed study of diagnostic accuracy to be robust, we require a minimum number of infected and uninfected cases. The total number of samples required to accurately evaluate the diagnostic devices can be calculated using the formulas below.\nThe number \\(\\LARGE n\\) of specimens needed to obtain precision in diagnostic performance estimates is calculated using the formula\n\\(\\LARGE n = \\frac{(1.96+1.28)^2 * (p*(1-p))}{(p-po)^2/m}\\)\nwhere\n\\(\\LARGE p\\) is the expected sensitivity of the novel diagnostic\n\\(\\LARGE p_{0}\\) is the minimum acceptable sensitivity of the novel diagnostic\n\\(\\LARGE m\\) is the estimated prevalence of infection/disease/condition/state in the population\nWhen \\(\\LARGE n\\) is chosen this way, you can design the test to ensure that the lower limit of the confidence interval for the estimate of sensitivity/specificity is not likely to exceed \\(\\LARGE p0\\).\nThis is based on Banoo, S. et al. Evaluation of diagnostic tests for infectious diseases: general principles. Nature Reviews Microbiology 4, S20–S32 (2006)."
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#libraries",
    "href": "Diagnostics_Power_Calculation.html#libraries",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.2 Libraries",
    "text": "10.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout"
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#define-a-function-that-applies-the-formula",
    "href": "Diagnostics_Power_Calculation.html#define-a-function-that-applies-the-formula",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.3 Define a function that applies the formula",
    "text": "10.3 Define a function that applies the formula\n\nrequired_specimens_min_acceptable<-function(p,po,m)(((1.96+1.28)^2)*(p*(1-p)))/((p-po)^2)/m"
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#create-a-data-set-with-likely-values",
    "href": "Diagnostics_Power_Calculation.html#create-a-data-set-with-likely-values",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.4 Create a data set with likely values",
    "text": "10.4 Create a data set with likely values\n\ndf<- expand_grid(\n                p = seq(0.80,0.99,0.01),\n                m = seq(0.02,0.16,0.02)\n                )"
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#apply-the-function",
    "href": "Diagnostics_Power_Calculation.html#apply-the-function",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.5 Apply the function",
    "text": "10.5 Apply the function\n\n10.5.1 At 1%, 5%, 10% and 15% tolerance in lower limit precision in estimate\n\ndf.5pc<- df %>% \n  mutate(\n        po = p-0.05,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"five_percent\"\n        )\n\ndf.10pc<- df %>% \n  mutate(\n        po = p-0.10,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"ten_percent\"\n        )\n\ndf.15pc<- df %>% \n  mutate(\n        po = p-0.15,\n        n = required_specimens_min_acceptable(p = p, po = po, m = m),\n        lower = \"fifteen_percent\"\n        )\n\ndf<-bind_rows(df.5pc,df.10pc,df.15pc) %>% \n  mutate(lower = factor(lower,levels=c(\"five_percent\",\"ten_percent\",\"fifteen_percent\")))\nrm(df.5pc,df.10pc,df.15pc)"
  },
  {
    "objectID": "Diagnostics_Power_Calculation.html#draw-chart",
    "href": "Diagnostics_Power_Calculation.html#draw-chart",
    "title": "10  Diagnostic Evaluation Power Calculation",
    "section": "10.6 Draw chart",
    "text": "10.6 Draw chart\nThis shows the values of \\(\\LARGE n\\) (y axis) for various values of \\(\\LARGE p\\)\nColoured lines show different underlying prevalence values and facets show different acceptable levels of precision in the estimate, here 5%, 10% and 15%, indicating that for a given value of \\(\\LARGE p\\) such as 0.8, a precision as low as 0.75, 0.7 or 0.65 would be minimally acceptable.\nThe ggplotly view allows you to explore results visually.\n\nggplotly(ggplot(df,aes(p,n,color=factor(m)))+geom_line()+facet_wrap(.~lower))"
  },
  {
    "objectID": "hazratio_chart.html#background",
    "href": "hazratio_chart.html#background",
    "title": "11  Hazard Ratio Chart from Summary Data",
    "section": "11.1 Background",
    "text": "11.1 Background\nOften a user just wants to draw a quick chart of study results using only summary data. For instance, you may be looking at a table of hazard ratios and their confidence intervals and would like to visualise the results instead of working with the table.\nThis short script can take a vector of estimates, along with 95% CIs and will draw a chart."
  },
  {
    "objectID": "hazratio_chart.html#libraries",
    "href": "hazratio_chart.html#libraries",
    "title": "11  Hazard Ratio Chart from Summary Data",
    "section": "11.2 Libraries",
    "text": "11.2 Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "hazratio_chart.html#dummy-data",
    "href": "hazratio_chart.html#dummy-data",
    "title": "11  Hazard Ratio Chart from Summary Data",
    "section": "11.3 Dummy Data",
    "text": "11.3 Dummy Data\nStart by building a data frame using tibble. You should provide some labels (here IMD), along with an estimate (ABSOLUTE.CHANGE) and both lower (LCI) and upper (UCL) confidence limits\n\ndf <- tibble( \n            IMD = c(\"IMD1\",\"IMD2\",\"IMD3\",\"IMD4\",\"IMD5\"),\n            ABSOLUTE.CHANGE  = c(-3.1,-6.6,-1.6,-2.0,-0.9),\n            LCI = c(-0.4,-4.4,0.49,-1.06,-0.2),\n            UCI = c(-5.7,-8.8,-3.62,-2.9,-1.5)\n)\n\ndf\n\n# A tibble: 5 × 4\n  IMD   ABSOLUTE.CHANGE   LCI   UCI\n  <chr>           <dbl> <dbl> <dbl>\n1 IMD1             -3.1 -0.4  -5.7 \n2 IMD2             -6.6 -4.4  -8.8 \n3 IMD3             -1.6  0.49 -3.62\n4 IMD4             -2   -1.06 -2.9 \n5 IMD5             -0.9 -0.2  -1.5"
  },
  {
    "objectID": "hazratio_chart.html#chart",
    "href": "hazratio_chart.html#chart",
    "title": "11  Hazard Ratio Chart from Summary Data",
    "section": "11.4 Chart",
    "text": "11.4 Chart\nThere’s nothing clever about this chart. It uses geom_point to draw the estimates, then uses geom_errorbar to add the upper and lower confidence limits.\n\nggplot(df,aes(x=IMD,y=ABSOLUTE.CHANGE,colour=IMD))+\n       geom_errorbar( mapping=aes(x=IMD, y=ABSOLUTE.CHANGE, ymin=LCI, ymax=UCI), width=0.1, linewidth=1)+\n       geom_point(size=5)+\n       geom_hline(yintercept = 0,lty=2)  +\n       ylim (-10,10)\n\n\n\n\nIf you have some point estimates and know the sample size, but don’t have confidence intervals, then you can calculate the confidence intervals using [this method](../examples/95CI_point_estimate.qmd)"
  },
  {
    "objectID": "ruODK_basics.html#background",
    "href": "ruODK_basics.html#background",
    "title": "12  ruODK Setup and basic functions",
    "section": "12.1 Background",
    "text": "12.1 Background\nruODK is a great package for interacting with ODK Central via R. Full details of how to use it are here.\nTo connect your ODK Central account to R, you’ll need to set up your passwords in the r.environ file so they’re not exposed in scripts. Your passwords will be stored in plain text on your r.environ so be cautious about which machines you work with this way. There’s also options to add you password manually each time, but we’ll assume you’re on a secure machine when you do this."
  },
  {
    "objectID": "ruODK_basics.html#add-your-credentials-to-r.environ",
    "href": "ruODK_basics.html#add-your-credentials-to-r.environ",
    "title": "12  ruODK Setup and basic functions",
    "section": "12.2 Add your credentials to r.environ",
    "text": "12.2 Add your credentials to r.environ\nRun this command to open the r.environ file\n\nusethis::edit_r_environ(scope = “user”)\n\nInside renviron file, be careful not to delete anything, but add these lines (substituting your ODK central URL and email address in the appropriate places.\n\nODKC_URL=“https://central.xxx.com”\nODKC_UN=“mustafa.orbach@xxx.com”\n\nOn a new line, add the password you use to log in to the system. Substitute xxx for your password\n\nODKC_PW=“xxx”\n\nOn a new line, add your password for decryption (if you use project level encryption). Substitute xxx for your password\n\nODKC_PP=“xxx”\n\nFinally, save the r.environ file and restart R to load it in to R."
  },
  {
    "objectID": "ruODK_basics.html#using-ruodk-to-download-a-data-set",
    "href": "ruODK_basics.html#using-ruodk-to-download-a-data-set",
    "title": "12  ruODK Setup and basic functions",
    "section": "12.3 Using ruODK to download a data set",
    "text": "12.3 Using ruODK to download a data set\n\n12.3.1 Libraries\n\nlibrary(ruODK)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\n\n\n\n12.3.2 Connect to server\nMy servers are at central.lshtm.ac.uk. You need to change this to yours. Sys.getenv() pulls the passwords from the r.environ file in a way that is more secure than just adding to the script.\n\n#connect to server\nruODK::ru_setup(\n  url = \"https://central.lshtm.ac.uk\",\n  un = Sys.getenv(\"ODKC_UN\"),\n  pw = Sys.getenv(\"ODKC_PW\"),\n  tz = \"Europe/London\",\n  verbose = TRUE\n)\n\n<ruODK settings>\n  Default ODK Central Project ID:  \n  Default ODK Central Form ID:  \n  Default ODK Central URL: https://central.lshtm.ac.uk \n  Default ODK Central Username: chrissy.roberts@lshtm.ac.uk \n  Default ODK Central Password: run ruODK::get_default_pw() to show \n  Default ODK Central Passphrase: run ruODK::get_default_pp() to show \n  Default Time Zone: Europe/London \n  Default ODK Central Version: 1.1 \n  Default HTTP GET retries: 3 \n  Verbose messages: TRUE \n  Test ODK Central Project ID:  \n  Test ODK Central Form ID:  \n  Test ODK Central Form ID (ZIP tests):  \n  Test ODK Central Form ID (Attachment tests):  \n  Test ODK Central Form ID (Parsing tests):  \n  Test ODK Central Form ID (WKT tests):  \n  Test ODK Central URL:  \n  Test ODK Central Username:  \n  Test ODK Central Password: run ruODK::get_test_pw() to show \n  Test ODK Central Passphrase: run ruODK::get_test_pp() to show \n  Test ODK Central Version: 1.1 \n\n\n\n\n12.3.3 Show a list of projects\nYou’ll want to know the pid number, given here as id\n\nkable(ruODK::project_list()[1:2,-9])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nname\ndescription\narchived\nkey_id\ncreated_at\nupdated_at\ndeleted_at\nforms\napp_users\ndatasets\nlast_submission\n\n\n\n\n37\n000_Scratch_Project__Chrissy\nNA\nFALSE\nNA\n2021-06-10 12:43:13\n2022-10-21 11:38:39\nNA\n10\n1\n0\n2022-10-21 11:48:21\n\n\n18\n000_Scratch_project__Chrissy_Sham\nNA\nFALSE\n466\n2021-01-27 19:27:38\n2022-05-02 12:21:00\nNA\n19\n2\n0\n2022-09-24 13:18:49\n\n\n\n\n\n\n\n12.3.4 Show a list of forms that are included in the project\n\nkable(ruODK::form_list(pid = 37))\n\nWarning: Automatic coercion from integer to character was deprecated in purrr 1.0.0.\nℹ Please use an explicit call to `as.character()` within `map_chr()` instead.\nℹ The deprecated feature was likely used in the ruODK package.\n  Please report the issue at <\u001b]8;;https://github.com/ropensci/ruODK/issues\u0007https://github.com/ropensci/ruODK/issues\u001b]8;;\u0007>.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nfid\nversion\nstate\nsubmissions\ncreated_at\ncreated_by_id\ncreated_by\nupdated_at\npublished_at\nlast_submission\nhash\n\n\n\n\nEureka Study\nConsent_Online\n1\nopen\n2\n2022-10-21 11:25:43\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:58:58\n2022-10-21 11:58:58\n2022-10-21 11:48:21\ncf910d1d93c659fe238c0a5d51d0e695\n\n\ngeopoint_map\ngeopoint_map\n\nopen\n3\n2022-07-21 11:00:49\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\n2022-07-21 13:09:42\n2022-07-21 13:13:51\nf0b34efc38940a990396576f4643a008\n\n\ngeopoint_test\ngeopoint_test\n\nopen\n0\n2022-06-16 12:19:45\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\n2022-06-16 12:20:55\nNA\ndacea051c12f44b2241eb51b83157c4c\n\n\nGuinée questionnaire sur l’implémentation 22 Septembre 2022 V5\nGuinée questionnaire sur l’implémentation 22 Septembre 2022 V5\n\nopen\n0\n2022-09-22 11:42:24\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\nNA\nNA\n15bfaa3400d9aefe5cc72171abe2a035\n\n\nindexed_repeat_with_internal_cascading_select\nindexed_repeat_with_internal_cascading_select\n\nopen\n3\n2022-06-09 08:35:34\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\n2022-06-09 08:36:48\n2022-06-09 08:38:12\ne61dbda049489bcff50d5d21603a34f0\n\n\nperiod.diary\nperiod.diary\n\nopen\n0\n2022-06-14 22:40:55\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\nNA\nNA\nd4ce754877a34b1ab1c66f4c0ece4f03\n\n\nprepopulate.example\nprepopulate.example\n1\nopen\n13\n2021-06-10 13:26:35\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\n2021-06-10 13:27:20\n2021-06-10 13:53:25\n9518e5bc127b144b11fd469684b3ac80\n\n\nSocial_Contact_Survey\nsocialcontact\n20192310\nopen\n0\n2022-07-20 11:36:55\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\nNA\nNA\n0a4bc27525b822f379a5522423107270\n\n\nV2_02_Test_de_grossesse\nV2_02_Test_de_grossesse\n1907081903\nopen\n0\n2022-09-07 11:41:10\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\nNA\nNA\n25d707ef9c7a4ea257bec37627426f60\n\n\nZim_Photos\nZim_Photos\n\nopen\n0\n2022-09-09 10:48:36\n19\nChrissy h. Roberts (SA)\n2022-10-21 11:38:39\n2022-09-09 10:48:54\nNA\n118c629659e9e59d18bcbdf248019d16\n\n\n\n\n\n\n\n12.3.5 Show details about a specific form\n\nkable(ruODK::form_detail(pid = 37,fid = \"geopoint_map\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nfid\nversion\nstate\nsubmissions\ncreated_at\ncreated_by_id\ncreated_by\nupdated_at\npublished_at\nlast_submission\nhash\n\n\n\n\ngeopoint_map\ngeopoint_map\n\nopen\n3\n2022-07-21T10:00:49.605Z\n19\nChrissy h. Roberts (SA)\n2022-10-21T10:38:39.291Z\n2022-07-21T12:09:42.248Z\n2022-07-21T12:13:51.306Z\nf0b34efc38940a990396576f4643a008\n\n\n\n\n\n\n\n12.3.6 Show a list of submissions to a specific form\n\nkable(ruODK::submission_list(pid = 37,fid = \"geopoint_map\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninstance_id\nsubmitter_id\ndevice_id\ncreated_at\nupdated_at\nreview_state\nsubmitter_id_2\nsubmitter_type\nsubmitter_display_name\nsubmitter_created_at\nsubmitter_updated_at\nsubmitter_deleted_at\n\n\n\n\nuuid:0a671884-27ed-4eb8-9102-310149899678\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:13:51\n2022-09-15 11:08:57\nNA\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n\n\nuuid:9f6a1fd4-3394-4e5a-b013-e344272cdf70\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:13:24\nNA\nNA\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n\n\nuuid:0a65e09e-18e5-4fd3-bbe8-4c57529ccc89\n1094\ncollect:v2m4bt3bE9ZfeMfi\n2022-07-21 13:11:19\nNA\nNA\n1094\nfield_key\nchrissy\n2021-11-03 12:44:28\nNA\nNA\n\n\n\n\n\n\n\n12.3.7 Pull submissions\nruODK is very powerful and can get quite complicated. The most basic thing you need is a dataframe with your submissions. Here’s how to get one.\n\ndf<-ruODK::odata_submission_get(pid = \"37\",fid=\"geopoint_map\")\n\n\n\n12.3.8 Show data\n\nkable(df[,1:4])\n\n\n\n\n\n\n\n\n\n\ngeopoint_longitude\ngeopoint_latitude\ngeopoint_altitude\ngeopoint_accuracy\n\n\n\n\n-0.054687\n52.07809\n0\n0\n\n\n152.504487\n-32.18187\n0\n0\n\n\n152.526883\n-32.20085\n0\n0"
  },
  {
    "objectID": "diva_gis.html#background",
    "href": "diva_gis.html#background",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.1 Background",
    "text": "13.1 Background\nUsing Administrative data from [DIVA-GIS](www.diva-gis.org), this script will convert the geographical administrative data (levels 0-3) for one or more country and output an itemsets.csv file that works with ODK and preserves accented characters (tested only on roman characters). The primary purpose here is to create a list that is compatible with the cascading select system that ODK uses to filter down to more granular responses in questions where you’d want to capture district > area > village or similar.\nThis also removes accents from internal variables like the XLSForm ‘name’ column.\nAccents will appear on screen, but won’t be preserved in data frames. This is desirable because working with mixed data that may or may not include accents is a pain."
  },
  {
    "objectID": "diva_gis.html#data",
    "href": "diva_gis.html#data",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.2 Data",
    "text": "13.2 Data\nYou will need to get administrative data from [DIVA GIS](http://www.diva-gis.org/datadown#) for each of the countries you want to include.\nUnzip this data to a folder in the same directory as this script,\nThis example uses Uganda and Democratic Republic of Congo."
  },
  {
    "objectID": "diva_gis.html#libraries",
    "href": "diva_gis.html#libraries",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.3 Libraries",
    "text": "13.3 Libraries\n\nlibrary(knitr)\n\n\n13.3.1 Create a folder to house data\n\nif(!dir.exists(\"data/divadownload/\")){dir.create(\"data/divadownload\")}\nsystem(\"rm -rf data/divadownload/*\")\n\n\n\n13.3.2 Define target countries\nSpecify ISO codes (3 digit) to tell R which data sets to include. A full list of ISO codes is available [here](#https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3 )\n\ncountries<-c(\"UGA\",\"COD\")"
  },
  {
    "objectID": "diva_gis.html#download-and-bind-data",
    "href": "diva_gis.html#download-and-bind-data",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.4 Download and bind data",
    "text": "13.4 Download and bind data\nFor each country, find the data set in the folder structure and load up then bind together\n\noptions(timeout=120)\nfor(i in 1:length(countries))\n{\n  \n  #download the data set for country i\n  download.file(url = paste(\"http://biogeo.ucdavis.edu/data/diva/adm/\",countries[i],\"_adm.zip\",sep=\"\"),destfile =paste(\"data/divadownload/\",countries[i],\"_adm.zip\",sep=\"\"))\n\n  #unzip the data for country i\n  system (paste(\"unzip data/divadownload/\",countries[i],\"_adm.zip -d data/divadownload/\",countries[i],sep=\"\"))\n  \n  #read the level 3 data for country i\n  level3data<-read.csv(list.files(pattern = paste(countries[i],\"_adm3.csv\",sep=\"\"),full.names = T,recursive = T))\n  #select only the relevant columns\n  level3data<-level3data[,c(\"NAME_0\",\"NAME_1\",\"NAME_2\",\"NAME_3\",\"ISO\")]\n  \n  #for the first country, create a new df\n  if(i==1){full.data.output<-level3data}\n  #for countries 2 to n, bind data\n  if(i!=1){full.data.output<-rbind(full.data.output,level3data)}\n}"
  },
  {
    "objectID": "diva_gis.html#define-function-to-remove-accents",
    "href": "diva_gis.html#define-function-to-remove-accents",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.5 Define function to remove accents",
    "text": "13.5 Define function to remove accents\n\n#define function to remove accents from text\nremoveAccents<-function(x)\n{\n  a <- c('À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'ń', 'Ņ', 'ņ', 'Ň', 'ň', 'ŉ', 'Ō', 'ō', 'Ŏ', 'ŏ', 'Ő', 'ő', 'Œ', 'œ', 'Ŕ', 'ŕ', 'Ŗ', 'ŗ', 'Ř', 'ř', 'Ś', 'ś', 'Ŝ', 'ŝ', 'Ş', 'ş', 'Š', 'š', 'Ţ', 'ţ', 'Ť', 'ť', 'Ŧ', 'ŧ', 'Ũ', 'ũ', 'Ū', 'ū', 'Ŭ', 'ŭ', 'Ů', 'ů', 'Ű', 'ű', 'Ų', 'ų', 'Ŵ', 'ŵ', 'Ŷ', 'ŷ', 'Ÿ', 'Ź', 'ź', 'Ż', 'ż', 'Ž', 'ž', 'ſ', 'ƒ', 'Ơ', 'ơ', 'Ư', 'ư', 'Ǎ', 'ǎ', 'Ǐ', 'ǐ', 'Ǒ', 'ǒ', 'Ǔ', 'ǔ', 'Ǖ', 'ǖ', 'Ǘ', 'ǘ', 'Ǚ', 'ǚ', 'Ǜ', 'ǜ', 'Ǻ', 'ǻ', 'Ǽ', 'ǽ', 'Ǿ', 'ǿ');\n  b <- c('A', 'A', 'A', 'A', 'A', 'A', 'AE', 'C', 'E', 'E', 'E', 'E', 'I', 'I', 'I', 'I', 'D', 'N', 'O', 'O', 'O', 'O', 'O', 'O', 'U', 'U', 'U', 'U', 'Y', 's', 'a', 'a', 'a', 'a', 'a', 'a', 'ae', 'c', 'e', 'e', 'e', 'e', 'i', 'i', 'i', 'i', 'n', 'o', 'o', 'o', 'o', 'o', 'o', 'u', 'u', 'u', 'u', 'y', 'y', 'A', 'a', 'A', 'a', 'A', 'a', 'C', 'c', 'C', 'c', 'C', 'c', 'C', 'c', 'D', 'd', 'D', 'd', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'E', 'e', 'G', 'g', 'G', 'g', 'G', 'g', 'G', 'g', 'H', 'h', 'H', 'h', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'I', 'i', 'IJ', 'ij', 'J', 'j', 'K', 'k', 'L', 'l', 'L', 'l', 'L', 'l', 'L', 'l', 'l', 'l', 'N', 'n', 'N', 'n', 'N', 'n', 'n', 'O', 'o', 'O', 'o', 'O', 'o', 'OE', 'oe', 'R', 'r', 'R', 'r', 'R', 'r', 'S', 's', 'S', 's', 'S', 's', 'S', 's', 'T', 't', 'T', 't', 'T', 't', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'W', 'w', 'Y', 'y', 'Y', 'Z', 'z', 'Z', 'z', 'Z', 'z', 's', 'f', 'O', 'o', 'U', 'u', 'A', 'a', 'I', 'i', 'O', 'o', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'U', 'u', 'A', 'a', 'AE', 'ae', 'O', 'o');\n  for(i in 1:length(a))\n  {\n    x<-gsub(x = x,pattern = a[i],replacement = b[i])\n  }\n  return(x)\n}"
  },
  {
    "objectID": "diva_gis.html#tidy-data",
    "href": "diva_gis.html#tidy-data",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.6 Tidy data",
    "text": "13.6 Tidy data\n\n#Set correct names so that label is descriptive country name\nnames(full.data.output)[which(names(full.data.output)==\"NAME_0\")]<-\"label\"\n#set correct names so that ISO code is NAME_0 (level 0 country data)\nnames(full.data.output)[which(names(full.data.output)==\"ISO\")]<-\"NAME_0\"\n\n\n#get rid of whitespace and dots and so on [might need to add more]\nfull.data.output<-full.data.output[,c(\"NAME_0\",\"NAME_1\",\"NAME_2\",\"NAME_3\",\"label\")]\nfull.data.output$NAME_1<-gsub(full.data.output$NAME_1,pattern = \"/| |'|//.\",replacement = \"_\")\nfull.data.output$NAME_2<-gsub(full.data.output$NAME_2,pattern = \"/| |'|//.\",replacement = \"_\")\nfull.data.output$NAME_3<-gsub(full.data.output$NAME_3,pattern = \"/| |'|//.\",replacement = \"_\")\n\n#find level zero and blank out levels 1,2,3\nlevel0<-full.data.output\nlevel0[,2:4]<-\"\"\nlevel0<-unique(level0)\nlevel0$list_name<-\"NAME_0\"\n\n\n#find level one and blank out levels 2,3\nlevel1<-full.data.output[,c(2,1,3,4,5)]\nlevel1[,3:4]<-\"\"\nlevel1<-unique(level1)\nlevel1$list_name<-\"NAME_1\"\nlevel1$label<-level1$NAME_1\nx<-level1\nx$NAME_1<-\"\"\nx$label<-\"\"\nx<-unique(x)\nx$NAME_1<-\"Other\"\nx$label<-\"Other\"\nlevel1<-rbind(level1,x)\nrm(x)\n\n\n#find level two and blank out level 3\nlevel2<-full.data.output[,c(3,1,2,4,5)]\nlevel2[,4]<-\"\"\nlevel2<-unique(level2)\nlevel2$list_name<-\"NAME_2\"\nlevel2$label<-level2$NAME_2\nx<-level2\nx$NAME_2<-\"\"\nx$label<-\"\"\nx<-unique(x)\nx$NAME_2<-\"Other\"\nx$label<-\"Other\"\nlevel2<-rbind(level2,x)\nrm(x)\n\n#find level three\nlevel3<-full.data.output[,c(4,1,2,3,5)]\nlevel3<-unique(level3)\nlevel3$list_name<-\"NAME_3\"\nlevel3$label<-level3$NAME_3\nx<-level3\nx$NAME_3<-\"\"\nx$label<-\"\"\nx<-unique(x)\nx$NAME_3<-\"Other\"\nx$label<-\"Other\"\nlevel3<-rbind(level3,x)\nrm(x)\n\n\n#put it all together\noutput<-level0\noutput<-rbind(output,level1)\noutput<-rbind(output,level2)\noutput<-rbind(output,level3)\n\n\n# remove accents from name, levels 0-3, leaving them only in label.\noutput<-output[,c(\"list_name\",\"NAME_0\",\"NAME_1\",\"NAME_2\",\"NAME_3\",\"label\")]\noutput$name<-output$label\noutput$name<-removeAccents(output$name)\noutput$NAME_0<-removeAccents(output$NAME_0)\noutput$NAME_1<-removeAccents(output$NAME_1)\noutput$NAME_2<-removeAccents(output$NAME_2)\noutput$NAME_3<-removeAccents(output$NAME_3)"
  },
  {
    "objectID": "diva_gis.html#show-output",
    "href": "diva_gis.html#show-output",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.7 Show output",
    "text": "13.7 Show output\n\nkable(head(output,100))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlist_name\nNAME_0\nNAME_1\nNAME_2\nNAME_3\nlabel\nname\n\n\n\n\n1\nNAME_0\nUGA\n\n\n\nUganda\nUganda\n\n\n968\nNAME_0\nCOD\n\n\n\nDemocratic Republic of the Congo\nDemocratic Republic of the Congo\n\n\n12\nNAME_1\nUGA\nAdjumani\n\n\nAdjumani\nAdjumani\n\n\n7\nNAME_1\nUGA\nApac\n\n\nApac\nApac\n\n\n29\nNAME_1\nUGA\nArua\n\n\nArua\nArua\n\n\n65\nNAME_1\nUGA\nBugiri\n\n\nBugiri\nBugiri\n\n\n82\nNAME_1\nUGA\nBundibugyo\n\n\nBundibugyo\nBundibugyo\n\n\n92\nNAME_1\nUGA\nBushenyi\n\n\nBushenyi\nBushenyi\n\n\n122\nNAME_1\nUGA\nBusia\n\n\nBusia\nBusia\n\n\n132\nNAME_1\nUGA\nGulu\n\n\nGulu\nGulu\n\n\n155\nNAME_1\nUGA\nHoima\n\n\nHoima\nHoima\n\n\n169\nNAME_1\nUGA\nIganga\n\n\nIganga\nIganga\n\n\n194\nNAME_1\nUGA\nJinja\n\n\nJinja\nJinja\n\n\n205\nNAME_1\nUGA\nKabale\n\n\nKabale\nKabale\n\n\n224\nNAME_1\nUGA\nKabarole\n\n\nKabarole\nKabarole\n\n\n239\nNAME_1\nUGA\nKaberamaido\n\n\nKaberamaido\nKaberamaido\n\n\n248\nNAME_1\nUGA\nKalangala\n\n\nKalangala\nKalangala\n\n\n255\nNAME_1\nUGA\nKampala\n\n\nKampala\nKampala\n\n\n260\nNAME_1\nUGA\nKamuli\n\n\nKamuli\nKamuli\n\n\n283\nNAME_1\nUGA\nKamwenge\n\n\nKamwenge\nKamwenge\n\n\n292\nNAME_1\nUGA\nKanungu\n\n\nKanungu\nKanungu\n\n\n302\nNAME_1\nUGA\nKapchorwa\n\n\nKapchorwa\nKapchorwa\n\n\n318\nNAME_1\nUGA\nKasese\n\n\nKasese\nKasese\n\n\n340\nNAME_1\nUGA\nKatakwi\n\n\nKatakwi\nKatakwi\n\n\n358\nNAME_1\nUGA\nKayunga\n\n\nKayunga\nKayunga\n\n\n367\nNAME_1\nUGA\nKibale\n\n\nKibale\nKibale\n\n\n386\nNAME_1\nUGA\nKiboga\n\n\nKiboga\nKiboga\n\n\n400\nNAME_1\nUGA\nKisoro\n\n\nKisoro\nKisoro\n\n\n414\nNAME_1\nUGA\nKitgum\n\n\nKitgum\nKitgum\n\n\n433\nNAME_1\nUGA\nKotido\n\n\nKotido\nKotido\n\n\n453\nNAME_1\nUGA\nKumi\n\n\nKumi\nKumi\n\n\n469\nNAME_1\nUGA\nKyenjojo\n\n\nKyenjojo\nKyenjojo\n\n\n483\nNAME_1\nUGA\nLake_Albert\n\n\nLake_Albert\nLake_Albert\n\n\n484\nNAME_1\nUGA\nLake_Victoria\n\n\nLake_Victoria\nLake_Victoria\n\n\n485\nNAME_1\nUGA\nLira\n\n\nLira\nLira\n\n\n513\nNAME_1\nUGA\nLuwero\n\n\nLuwero\nLuwero\n\n\n533\nNAME_1\nUGA\nMasaka\n\n\nMasaka\nMasaka\n\n\n556\nNAME_1\nUGA\nMasindi\n\n\nMasindi\nMasindi\n\n\n570\nNAME_1\nUGA\nMayuge\n\n\nMayuge\nMayuge\n\n\n577\nNAME_1\nUGA\nMbale\n\n\nMbale\nMbale\n\n\n608\nNAME_1\nUGA\nMbarara\n\n\nMbarara\nMbarara\n\n\n654\nNAME_1\nUGA\nMoroto\n\n\nMoroto\nMoroto\n\n\n665\nNAME_1\nUGA\nMoyo\n\n\nMoyo\nMoyo\n\n\n673\nNAME_1\nUGA\nMpigi\n\n\nMpigi\nMpigi\n\n\n690\nNAME_1\nUGA\nMubende\n\n\nMubende\nMubende\n\n\n710\nNAME_1\nUGA\nMukono\n\n\nMukono\nMukono\n\n\n738\nNAME_1\nUGA\nNakapiripirit\n\n\nNakapiripirit\nNakapiripirit\n\n\n748\nNAME_1\nUGA\nNakasongola\n\n\nNakasongola\nNakasongola\n\n\n757\nNAME_1\nUGA\nNebbi\n\n\nNebbi\nNebbi\n\n\n776\nNAME_1\nUGA\nNtungamo\n\n\nNtungamo\nNtungamo\n\n\n791\nNAME_1\nUGA\nPader\n\n\nPader\nPader\n\n\n809\nNAME_1\nUGA\nPallisa\n\n\nPallisa\nPallisa\n\n\n837\nNAME_1\nUGA\nRakai\n\n\nRakai\nRakai\n\n\n864\nNAME_1\nUGA\nRukungiri\n\n\nRukungiri\nRukungiri\n\n\n875\nNAME_1\nUGA\nSembabule\n\n\nSembabule\nSembabule\n\n\n882\nNAME_1\nUGA\nSironko\n\n\nSironko\nSironko\n\n\n902\nNAME_1\nUGA\nSoroti\n\n\nSoroti\nSoroti\n\n\n919\nNAME_1\nUGA\nTororo\n\n\nTororo\nTororo\n\n\n943\nNAME_1\nUGA\nWakiso\n\n\nWakiso\nWakiso\n\n\n960\nNAME_1\nUGA\nYumbe\n\n\nYumbe\nYumbe\n\n\n9682\nNAME_1\nCOD\nEquateur\n\n\nÉquateur\nEquateur\n\n\n993\nNAME_1\nCOD\nBandundu\n\n\nBandundu\nBandundu\n\n\n1010\nNAME_1\nCOD\nBas-Congo\n\n\nBas-Congo\nBas-Congo\n\n\n1022\nNAME_1\nCOD\nKasai-Occidental\n\n\nKasaï-Occidental\nKasai-Occidental\n\n\n1033\nNAME_1\nCOD\nKasai-Oriental\n\n\nKasaï-Oriental\nKasai-Oriental\n\n\n1046\nNAME_1\nCOD\nKatanga\n\n\nKatanga\nKatanga\n\n\n1069\nNAME_1\nCOD\nKinshasa_City\n\n\nKinshasa_City\nKinshasa_City\n\n\n1071\nNAME_1\nCOD\nKivu\n\n\nKivu\nKivu\n\n\n1093\nNAME_1\nCOD\nOrientale\n\n\nOrientale\nOrientale\n\n\n11\nNAME_1\nUGA\nOther\n\n\nOther\nOther\n\n\n9681\nNAME_1\nCOD\nOther\n\n\nOther\nOther\n\n\n13\nNAME_2\nUGA\nAdjumani\nEast_Moyo\n\nEast_Moyo\nEast_Moyo\n\n\n72\nNAME_2\nUGA\nApac\nKole\n\nKole\nKole\n\n\n121\nNAME_2\nUGA\nApac\nKwania\n\nKwania\nKwania\n\n\n17\nNAME_2\nUGA\nApac\nMaruzi\n\nMaruzi\nMaruzi\n\n\n22\nNAME_2\nUGA\nApac\nOyam\n\nOyam\nOyam\n\n\n293\nNAME_2\nUGA\nArua\nArua_Municipality\n\nArua_Municipality\nArua_Municipality\n\n\n31\nNAME_2\nUGA\nArua\nAyivu\n\nAyivu\nAyivu\n\n\n37\nNAME_2\nUGA\nArua\nKoboko\n\nKoboko\nKoboko\n\n\n42\nNAME_2\nUGA\nArua\nMadi-Okollo\n\nMadi-Okollo\nMadi-Okollo\n\n\n48\nNAME_2\nUGA\nArua\nMaracha\n\nMaracha\nMaracha\n\n\n55\nNAME_2\nUGA\nArua\nTerego\n\nTerego\nTerego\n\n\n61\nNAME_2\nUGA\nArua\nVurra\n\nVurra\nVurra\n\n\n652\nNAME_2\nUGA\nBugiri\nBukooli\n\nBukooli\nBukooli\n\n\n823\nNAME_2\nUGA\nBundibugyo\nBwamba\n\nBwamba\nBwamba\n\n\n89\nNAME_2\nUGA\nBundibugyo\nNtoroko\n\nNtoroko\nNtoroko\n\n\n922\nNAME_2\nUGA\nBushenyi\nBuhweju\n\nBuhweju\nBuhweju\n\n\n96\nNAME_2\nUGA\nBushenyi\nBunyaruguru\n\nBunyaruguru\nBunyaruguru\n\n\n101\nNAME_2\nUGA\nBushenyi\nIgara\n\nIgara\nIgara\n\n\n108\nNAME_2\nUGA\nBushenyi\nRuhinda\n\nRuhinda\nRuhinda\n\n\n115\nNAME_2\nUGA\nBushenyi\nSheema\n\nSheema\nSheema\n\n\n1222\nNAME_2\nUGA\nBusia\nSamia-Bugwe\n\nSamia-Bugwe\nSamia-Bugwe\n\n\n1322\nNAME_2\nUGA\nGulu\nAswa\n\nAswa\nAswa\n\n\n137\nNAME_2\nUGA\nGulu\nGulu\n\nGulu\nGulu\n\n\n141\nNAME_2\nUGA\nGulu\nKilak\n\nKilak\nKilak\n\n\n145\nNAME_2\nUGA\nGulu\nNwoya\n\nNwoya\nNwoya\n\n\n149\nNAME_2\nUGA\nGulu\nOmoro\n\nOmoro\nOmoro\n\n\n1552\nNAME_2\nUGA\nHoima\nBugahya\n\nBugahya\nBugahya\n\n\n164\nNAME_2\nUGA\nHoima\nBuhaguzi\n\nBuhaguzi\nBuhaguzi\n\n\n1692\nNAME_2\nUGA\nIganga\nBugweri\n\nBugweri\nBugweri"
  },
  {
    "objectID": "diva_gis.html#write-output-to-itemsets.csv",
    "href": "diva_gis.html#write-output-to-itemsets.csv",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.8 Write output to itemsets.csv",
    "text": "13.8 Write output to itemsets.csv\n\nwrite.csv(x = output,file = \"output/diva_itemsets.csv\",quote = F,row.names = F)"
  },
  {
    "objectID": "diva_gis.html#delete-the-raw-data",
    "href": "diva_gis.html#delete-the-raw-data",
    "title": "13  GIS Data to ODK itemlist",
    "section": "13.9 Delete the raw data",
    "text": "13.9 Delete the raw data\n\nsystem(\"rm -rf data/divadownload/\")"
  },
  {
    "objectID": "Univariate_regressions_reported_as_a_table.html#univariate-regressions---reported-as-a-table",
    "href": "Univariate_regressions_reported_as_a_table.html#univariate-regressions---reported-as-a-table",
    "title": "14  Univariate regressions, reported as a table",
    "section": "14.1 Univariate Regressions - reported as a table",
    "text": "14.1 Univariate Regressions - reported as a table\nThe tbl_uvregression() function produces a table of univariate regression models. The function is a wrapper for tbl_regression(), and as a result, accepts nearly identical function arguments. The function’s results can be modified in similar ways to tbl_regression().\nThis example uses the built-in trial data set\n\nlibrary (gtsummary)\nlibrary(tidyverse)\nlibrary(knitr)\n\nShow the data set\n\nkable(head(trial,50))\n\n\n\n\ntrt\nage\nmarker\nstage\ngrade\nresponse\ndeath\nttdeath\n\n\n\n\nDrug A\n23\n0.160\nT1\nII\n0\n0\n24.00\n\n\nDrug B\n9\n1.107\nT2\nI\n1\n0\n24.00\n\n\nDrug A\n31\n0.277\nT1\nII\n0\n0\n24.00\n\n\nDrug A\nNA\n2.067\nT3\nIII\n1\n1\n17.64\n\n\nDrug A\n51\n2.767\nT4\nIII\n1\n1\n16.43\n\n\nDrug B\n39\n0.613\nT4\nI\n0\n1\n15.64\n\n\nDrug A\n37\n0.354\nT1\nII\n0\n0\n24.00\n\n\nDrug A\n32\n1.739\nT1\nI\n0\n1\n18.43\n\n\nDrug A\n31\n0.144\nT1\nII\n0\n0\n24.00\n\n\nDrug B\n34\n0.205\nT3\nI\n0\n1\n10.53\n\n\nDrug B\n42\n0.513\nT1\nIII\n0\n0\n24.00\n\n\nDrug B\n63\n0.060\nT3\nI\n1\n0\n24.00\n\n\nDrug B\n54\n0.831\nT4\nIII\n0\n1\n14.34\n\n\nDrug B\n21\n0.258\nT4\nI\n0\n1\n12.89\n\n\nDrug B\n48\n0.128\nT1\nI\n0\n1\n22.68\n\n\nDrug B\n71\n0.445\nT4\nIII\n0\n1\n8.71\n\n\nDrug A\n38\n2.083\nT4\nIII\n1\n0\n24.00\n\n\nDrug B\n49\n0.157\nT2\nII\n0\n1\n15.21\n\n\nDrug A\n57\n0.066\nT1\nIII\n0\n0\n24.00\n\n\nDrug A\n46\n0.325\nT1\nII\n0\n0\n24.00\n\n\nDrug A\n47\n0.266\nT2\nI\n0\n0\n24.00\n\n\nDrug A\n52\n0.719\nT2\nII\n0\n0\n24.00\n\n\nDrug A\n61\n1.713\nT4\nI\n0\n1\n16.92\n\n\nDrug A\n38\n0.096\nT4\nI\n0\n1\n23.89\n\n\nDrug B\n34\n0.105\nT4\nII\n0\n1\n6.32\n\n\nDrug B\n49\n0.043\nT2\nIII\n0\n1\n15.77\n\n\nDrug A\n63\n0.981\nT4\nII\n1\n0\n24.00\n\n\nDrug B\n67\n1.156\nT1\nII\n0\n0\n24.00\n\n\nDrug B\n68\n0.105\nT4\nII\n0\n1\n15.45\n\n\nDrug A\n78\n0.175\nT3\nI\n1\n1\n17.43\n\n\nDrug B\n36\n0.309\nT1\nIII\n1\n0\n24.00\n\n\nDrug A\n37\n1.869\nT2\nII\n0\n1\n20.90\n\n\nDrug B\n53\n2.008\nT3\nI\n0\n0\n24.00\n\n\nDrug A\n36\n1.894\nT3\nI\n1\n0\n24.00\n\n\nDrug A\n51\n0.160\nT3\nI\n0\n0\n24.00\n\n\nDrug A\n48\n1.209\nT3\nIII\nNA\n1\n21.19\n\n\nDrug B\n57\n0.108\nT1\nII\n0\n1\n12.52\n\n\nDrug A\n31\n0.611\nT1\nII\n1\n0\n24.00\n\n\nDrug B\n37\n0.222\nT4\nIII\n0\n1\n15.59\n\n\nDrug B\n28\n0.803\nT4\nII\n0\n1\n18.00\n\n\nDrug B\n40\n0.370\nT3\nII\n0\n1\n18.02\n\n\nDrug B\n49\nNA\nT1\nIII\n1\n1\n12.43\n\n\nDrug A\n61\n0.177\nT4\nIII\n0\n1\n12.10\n\n\nDrug B\n56\n1.479\nT3\nI\n1\n0\n24.00\n\n\nDrug A\n54\n0.161\nT4\nIII\n1\n1\n17.42\n\n\nDrug B\n71\n0.737\nT1\nI\n1\n0\n24.00\n\n\nDrug A\n38\n0.124\nT1\nIII\n1\n0\n24.00\n\n\nDrug B\n31\n0.092\nT2\nII\n0\n0\n24.00\n\n\nDrug B\n48\n0.385\nT1\nII\n0\n1\n12.19\n\n\nDrug B\nNA\n0.210\nT4\nII\n0\n1\n10.02\n\n\n\n\n\nPerform univariate analyses\n\ntrial %>%\n\n  select(response, age, grade) %>%\n\n  tbl_uvregression(\n    method = glm,\n    y = response,\n    method.args = list(family = binomial),\n    exponentiate = TRUE,\n    pvalue_fun = ~style_pvalue(.x, digits = 2)\n  ) %>%\n  add_global_p() %>%  # add global p-value \n  add_nevent() %>%    # add number of events of the outcome\n  add_q() %>%         # adjusts global p-values for multiple testing\n  bold_p() %>%        # bold p-values under a given threshold (default 0.05)\n  bold_p(t = 0.10, q = TRUE) %>% # now bold q-values under the threshold of 0.10\n  bold_labels()\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      N\n      Event N\n      OR1\n      95% CI1\n      p-value\n      q-value2\n    \n  \n  \n    Age\n183\n58\n1.02\n1.00, 1.04\n0.091\n0.18\n    Grade\n193\n61\n\n\n0.93\n0.93\n        I\n\n\n—\n—\n\n\n        II\n\n\n0.95\n0.45, 2.00\n\n\n        III\n\n\n1.10\n0.52, 2.29\n\n\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n    \n      2 False discovery rate correction for multiple testing"
  },
  {
    "objectID": "Akaike_Information_Criterion.html#example",
    "href": "Akaike_Information_Criterion.html#example",
    "title": "16  Akaike information criterion",
    "section": "16.1 Example",
    "text": "16.1 Example\nYou want to know whether drinking sugar-sweetened beverages influences body weight. You have collected secondary data from a national health survey that contains observations on sugar-sweetened beverage consumption, age, sex, and BMI (body mass index).\nTo find out which of these variables are important for predicting the relationship between sugar-sweetened beverage consumption and body weight, you create several possible models and compare them using AIC."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#when-to-use-aic",
    "href": "Akaike_Information_Criterion.html#when-to-use-aic",
    "title": "16  Akaike information criterion",
    "section": "16.2 When to use AIC",
    "text": "16.2 When to use AIC\nIn statistics, AIC is most often used for model selection. By calculating and comparing the AIC scores of several possible models, you can choose the one that is the best fit for the data.\nWhen testing a hypothesis, you might gather data on variables that you aren’t certain about, especially if you are exploring a new idea. You want to know which of the independent variables you have measured explain the variation in your dependent variable.\nA good way to find out is to create a set of models, each containing a different combination of the independent variables you have measured. These combinations should be based on:\nYour knowledge of the study system – avoid using parameters that are not logically connected, since you can find spurious correlations between almost anything!\nYour experimental design – for example, if you have split two treatments up among test subjects, then there is probably no reason to test for an interaction between the two treatments.\nOnce you’ve created several possible models, you can use AIC to compare them. Lower AIC scores are better, and AIC penalizes models that use more parameters. So if two models explain the same amount of variation, the one with fewer parameters will have a lower AIC score and will be the better-fit model."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#model-selection-example",
    "href": "Akaike_Information_Criterion.html#model-selection-example",
    "title": "16  Akaike information criterion",
    "section": "16.3 Model selection example",
    "text": "16.3 Model selection example\nIn a study of how hours spent studying and test format (multiple choice vs. written answers) affect test scores, you create two models:\nFinal test score in response to hours spent studying\nFinal test score in response to hours spent studying + test format\nYou find an r2 of 0.45 with a p-value less than 0.05 for model 1, and an r2 of 0.46 with a p-value less than 0.05 for model 2. Model 2 fits the data slightly better – but was it worth it to add another parameter just to get this small increase in model fit?\nYou run an AIC test to find out, which shows that model 1 has the lower AIC score because it requires less information to predict with almost the exact same level of precision. Another way to think of this is that the increased precision in model 2 could have happened by chance.\nFrom the AIC test, you decide that model 1 is the best model for your study."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#how-to-compare-models-using-aic",
    "href": "Akaike_Information_Criterion.html#how-to-compare-models-using-aic",
    "title": "16  Akaike information criterion",
    "section": "16.4 How to compare models using AIC",
    "text": "16.4 How to compare models using AIC\nAIC determines the relative information value of the model using the maximum likelihood estimate and the number of parameters (independent variables) in the model. The formula for AIC is:\nThe mathematical formula for calculating Akaike information criterion.\nK is the number of independent variables used and L is the log-likelihood estimate (a.k.a. the likelihood that the model could have produced your observed y-values). The default K is always 2, so if your model uses one independent variable your K will be 3, if it uses two independent variables your K will be 4, and so on.\nTo compare models using AIC, you need to calculate the AIC of each model. If a model is more than 2 AIC units lower than another, then it is considered significantly better than that model.\nYou can easily calculate AIC by hand if you have the log-likelihood of your model, but calculating log-likelihood is complicated! Most statistical software will include a function for calculating AIC. We will use R to run our AIC analysis."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#aic-in-r",
    "href": "Akaike_Information_Criterion.html#aic-in-r",
    "title": "16  Akaike information criterion",
    "section": "16.5 AIC in R",
    "text": "16.5 AIC in R\nTo compare several models, you can first create the full set of models you want to compare and then run aictab() on the set.\nFor the sugar-sweetened beverage data, we’ll create a set of models that include the three predictor variables (age, sex, and beverage consumption) in various combinations.\nLoad the dataset\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(knitr)\nbmi.data <- read_csv(\"data/bmi.data.csv\")\n\nRows: 500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (3): age, consumption, bmi\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCreate the models\nFirst, we can test how each variable performs separately.\n\nage.mod <- lm(bmi ~ age, data = bmi.data)\nsummary(age.mod)\n\n\nCall:\nlm(formula = bmi ~ age, data = bmi.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.71849 -1.19268  0.01787  1.22263  3.14220 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18.559837   0.137912  134.58   <2e-16 ***\nage          0.167221   0.002777   60.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.408 on 498 degrees of freedom\nMultiple R-squared:  0.8793,    Adjusted R-squared:  0.879 \nF-statistic:  3627 on 1 and 498 DF,  p-value: < 2.2e-16\n\n\n\nsex.mod <- lm(bmi ~ sex, data = bmi.data)\nsummary(sex.mod)\n\n\nCall:\nlm(formula = bmi ~ sex, data = bmi.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7079 -3.3001 -0.1313  3.3804  8.6094 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.4397     0.2654  99.605   <2e-16 ***\nsexMale      -0.9078     0.3612  -2.513   0.0123 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.026 on 498 degrees of freedom\nMultiple R-squared:  0.01252,   Adjusted R-squared:  0.01054 \nF-statistic: 6.316 on 1 and 498 DF,  p-value: 0.01228\n\n\n\nconsumption.mod <- lm(bmi ~ consumption, data = bmi.data)\nsummary(consumption.mod)\n\n\nCall:\nlm(formula = bmi ~ consumption, data = bmi.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5273 -3.3595 -0.0708  3.2778  8.1281 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25.2960     0.6443  39.261   <2e-16 ***\nconsumption   0.9416     0.8909   1.057    0.291    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.047 on 498 degrees of freedom\nMultiple R-squared:  0.002238,  Adjusted R-squared:  0.0002343 \nF-statistic: 1.117 on 1 and 498 DF,  p-value: 0.2911\n\n\nNext, we want to know if the combination of age and sex are better at describing variation in BMI on their own, without including beverage consumption.\n\nage.sex.mod <- lm(bmi ~ age + sex, data = bmi.data)\nsummary(age.sex.mod)\n\n\nCall:\nlm(formula = bmi ~ age + sex, data = bmi.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.58304 -1.20104 -0.01705  1.23201  2.97899 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 18.75529    0.15760 119.008   <2e-16 ***\nage          0.16668    0.00277  60.165   <2e-16 ***\nsexMale     -0.31748    0.12602  -2.519   0.0121 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.4 on 497 degrees of freedom\nMultiple R-squared:  0.8808,    Adjusted R-squared:  0.8803 \nF-statistic:  1836 on 2 and 497 DF,  p-value: < 2.2e-16\n\n\nWe also want to know whether the combination of age, sex, and beverage consumption is better at describing the variation in BMI than any of the previous models.\n\ncombination.mod <- lm(bmi ~ age + sex + consumption, data = bmi.data)\nsummary(combination.mod)\n\n\nCall:\nlm(formula = bmi ~ age + sex + consumption, data = bmi.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67918 -1.18845  0.00031  1.22444  2.48676 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 17.78422    0.26730  66.534  < 2e-16 ***\nage          0.16703    0.00272  61.398  < 2e-16 ***\nsexMale     -0.28402    0.12392  -2.292   0.0223 *  \nconsumption  1.35082    0.30323   4.455 1.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.374 on 496 degrees of freedom\nMultiple R-squared:  0.8854,    Adjusted R-squared:  0.8847 \nF-statistic:  1277 on 3 and 496 DF,  p-value: < 2.2e-16\n\n\nFinally, we can check whether the interaction of age, sex, and beverage consumption can explain BMI better than any of the previous models.\n\ninteraction.mod <- lm(bmi ~ age*sex*consumption, data = bmi.data)\nsummary(interaction.mod)\n\n\nCall:\nlm(formula = bmi ~ age * sex * consumption, data = bmi.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.60668 -1.15255  0.00039  1.24355  2.69427 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             17.97608    0.80180  22.420   <2e-16 ***\nage                      0.15671    0.01592   9.845   <2e-16 ***\nsexMale                 -0.42385    0.99792  -0.425    0.671    \nconsumption              1.16172    1.07167   1.084    0.279    \nage:sexMale              0.01417    0.02037   0.695    0.487    \nage:consumption          0.01289    0.02149   0.600    0.549    \nsexMale:consumption      0.05629    1.36076   0.041    0.967    \nage:sexMale:consumption -0.01721    0.02811  -0.612    0.541    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.377 on 492 degrees of freedom\nMultiple R-squared:  0.8858,    Adjusted R-squared:  0.8842 \nF-statistic: 545.2 on 7 and 492 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "Akaike_Information_Criterion.html#compare-the-models",
    "href": "Akaike_Information_Criterion.html#compare-the-models",
    "title": "16  Akaike information criterion",
    "section": "16.6 Compare the models",
    "text": "16.6 Compare the models\nTo compare these models and find which one is the best fit for the data, you can put them together into a list and use the aictab() command to compare all of them at once. To use aictab(), first load the library AICcmodavg.\n\nlibrary(AICcmodavg)\n\nThen put the models into a list (‘models’) and name each of them so the AIC table is easier to read (‘model.names’).\n\nmodels <- list(age.mod, sex.mod, consumption.mod, age.sex.mod, combination.mod, interaction.mod)\n\nmodel.names <- c('age.mod', 'sex.mod', 'consumption.mod', 'age.sex.mod', 'combination.mod', 'interaction.mod')\n\nFinally, run aictab() to do the comparison.\n\naictab(cand.set = models, modnames = model.names)\n\n\nModel selection based on AICc:\n\n                K    AICc Delta_AICc AICcWt Cum.Wt       LL\ncombination.mod 5 1743.02       0.00   0.96   0.96  -866.45\ninteraction.mod 9 1749.35       6.33   0.04   1.00  -865.49\nage.sex.mod     4 1760.59      17.57   0.00   1.00  -876.26\nage.mod         3 1764.91      21.89   0.00   1.00  -879.43\nsex.mod         3 2815.68    1072.66   0.00   1.00 -1404.82\nconsumption.mod 3 2820.86    1077.84   0.00   1.00 -1407.41"
  },
  {
    "objectID": "Akaike_Information_Criterion.html#interpreting-the-results",
    "href": "Akaike_Information_Criterion.html#interpreting-the-results",
    "title": "16  Akaike information criterion",
    "section": "16.7 Interpreting the results",
    "text": "16.7 Interpreting the results\nThe best-fit model is always listed first. The model selection table includes information on:\nK\nThe number of parameters in the model.\nThe default K is 2, so a model with one parameter will have a K of 2 + 1 = 3.\nAICc\nThe information score of the model (the lower-case ‘c’ indicates that the value has been calculated from the AIC test corrected for small sample sizes). The smaller the AIC value, the better the model fit.\nDelta_AICc\nThe difference in AIC score between the best model and the model being compared. In this table, the next-best model has a delta-AIC of 6.69 compared with the top model, and the third-best model has a delta-AIC of 15.96 compared with the top model.\nAICcWt\nAICc weight, which is the proportion of the total amount of predictive power provided by the full set of models contained in the model being assessed. In this case, the top model contains 97% of the total explanation that can be found in the full set of models.\nCum.Wt\nThe sum of the AICc weights. Here the top two models contain 100% of the cumulative AICc weight.\nLL\nLog-likelihood. This is the value describing how likely the model is, given the data. The AIC score is calculated from the LL and K.\nFrom this table we can see that the best model is the combination model – the model that includes every parameter but no interactions (bmi ~ age + sex + consumption).\nThe model is much better than all the others, as it carries 96% of the cumulative model weight and has the lowest AIC score. The next-best model is more than 2 AIC units higher than the best model (6.33 units) and carries only 4% of the cumulative model weight.\nBased on this comparison, we would choose the combination model to use in our data analysis."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#reporting-the-results",
    "href": "Akaike_Information_Criterion.html#reporting-the-results",
    "title": "16  Akaike information criterion",
    "section": "16.8 Reporting the results",
    "text": "16.8 Reporting the results\nIf you are using AIC model selection in your research, you can state this in your methods section. Report that you used AIC model selection, briefly explain the best-fit model you found, and state the AIC weight of the model."
  },
  {
    "objectID": "Akaike_Information_Criterion.html#example-methods",
    "href": "Akaike_Information_Criterion.html#example-methods",
    "title": "16  Akaike information criterion",
    "section": "16.9 Example methods",
    "text": "16.9 Example methods\nWe used AIC model selection to distinguish among a set of possible models describing the relationship between age, sex, sweetened beverage consumption, and body mass index. The best-fit model, carrying 97% of the cumulative model weight, included every parameter with no interaction effects.\nAfter finding the best-fit model you can go ahead and run the model and evaluate the results. The output of your model evaluation can be reported in the results section of your paper."
  },
  {
    "objectID": "photo_cleaning.html#section",
    "href": "photo_cleaning.html#section",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.1 ",
    "text": "6.1"
  },
  {
    "objectID": "photo_cleaning.html#background",
    "href": "photo_cleaning.html#background",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.2 Background",
    "text": "6.2 Background\nWhen sharing images, it is useful to be able to…\n\n6.2.0.1 1) Remove EXIF data, which can include GPS locations where the photos were taken\nImagemagick is used for this\n\n\n6.2.0.2 2) Rename files to have nonsensical names\nUsing either date or gdate on linux or OSX respectively. gdate is part of the coreutils package\nThis tutorial is aimed at OSX users and assumes you have homebrew (brew) installed. Using it on a PC through the linux subsystem for windows should be easy. Replace brew with apt-get or similar.\n\n\n6.2.0.3 3) Flag any files with very high luminance, for instance photos of labels, consent forms etc\nThis is also done with Imagemagick.\nThis script assumes that you’ve put a lot of pictures in the folder data/photos_for_cleaning"
  },
  {
    "objectID": "photo_cleaning.html#libraries",
    "href": "photo_cleaning.html#libraries",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.3 Libraries",
    "text": "6.3 Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(mixtools)\n\nmixtools package, version 2.0.0, Released 2022-12-04\nThis package is based upon work supported by the National Science Foundation under Grant No. SES-0518772 and the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).\n\nset.seed(23423346)"
  },
  {
    "objectID": "photo_cleaning.html#copy-some-photos-in-to-the-working-directory",
    "href": "photo_cleaning.html#copy-some-photos-in-to-the-working-directory",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.4 Copy some photos in to the working directory",
    "text": "6.4 Copy some photos in to the working directory\n\nsystem(\"rm data/photos_for_cleaning/*.JPG\")\nsystem(\"unzip data/photos_for_cleaning/photos_for_cleaning.zip -d data/photos_for_cleaning/\")"
  },
  {
    "objectID": "photo_cleaning.html#remove-exif-data",
    "href": "photo_cleaning.html#remove-exif-data",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.5 Remove EXIF data",
    "text": "6.5 Remove EXIF data\nInstall Imagemagick (hash this line if you already have Imagemagick unless you want a long wait)\n\n#system(\"brew install imagemagick\")\n\nUse imagemagick’s mogrify to make an in place copy of each file with the EXIF data stripped out. This has the benefit of making a new creation datestamp and md5 hash for each file.\nBefore running this on thousands of photos, we will test that this works by reading an EXIF metadata from a single file.\nLet’s see what files are available in the folder data/photos_for_cleaning\n\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nNow use the identify command to look the content of the EXIF for one file\n\nsystem(str_c(r\"(identify -format '%[EXIF:*]' )\", pictures[1]))\n\nYou’ll see a load of data from the EXIF. If you want to capture anything, like the aperture settings etc, now is the time to do it. It should be simple to design a loop and function that will capture this info.\nAfter applying the following command from ImageMagick\n\nsystem(str_c(\"mogrify -strip \",pictures[1]))\n\nYou should be able to rerun this\n\nsystem(str_c(r\"(identify -format '%[EXIF:*]')\", pictures[1]))\n\nand you should now see an empty EXIF, i.e. nothing will be shown in the console. If you still see EXIF data, something went wrong.\nTo apply the function to all files in the folder, we won’t use the built in mogrify -strip data/photos_for_cleaning/*.JPG because it halts when it hits a problem. Instead we will wrap it in an R code with ‘try’.\n\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nfor(i in pictures){\n                  (system(str_c(\"mogrify -strip \",i)))\n                  }"
  },
  {
    "objectID": "photo_cleaning.html#rename-all-files",
    "href": "photo_cleaning.html#rename-all-files",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.6 Rename all files",
    "text": "6.6 Rename all files\n\n6.6.1 Define a function to create a unique ID\n\n  randomid <- function(n = 1,path,file) {\n    a <- do.call(paste0, replicate(5, sample(LETTERS, n, TRUE), FALSE))\n    id = paste0(a, sprintf(\"%04d\", sample(9999, n, TRUE)), sample(LETTERS, n, TRUE))\n    file.rename(from=file, to=str_c(path,id,\".JPG\",sep=\"\"))\n  }\n\n\n\n6.6.2 Apply the function, renaming all photos\n\npath = \"data/photos_for_cleaning/\"\npictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T)\n\nfor(i in pictures){randomid(path = path,file = i)}\n\n\n\n6.6.3 Flag any files with high luminance\n\ndf = tibble(\n  pictures = list.files(path = \"data/photos_for_cleaning/\",pattern = \".JPG\",full.names = T),\n  luminance = NA\n)\n\nfor (i in 1:nrow(df)){\n\ndf$luminance[i] = as.numeric(system(str_c(r\"(convert )\",df$pictures[i],r\"( -colorspace LAB -channel r -separate +channel -format \"%[mean]\\n\" info: )\",sep=\"\"),intern = T))\n\n}\n\n\n\n6.6.4 Run an Expectation Maximisation to find clusters\n\nset.seed(12341)\nem<-normalmixEM(df$luminance, k=2,maxit = 3000)\n\nnumber of iterations= 6 \n\n\nPlot the results, with 99.99 Confidence interval\n\nggplot(df, aes(x = luminance)) +\n  geom_histogram(binwidth = 0.007,color=\"white\",fill=\"grey\") +\n  mapply(\n    function(mean, sd, lambda, n, binwidth) {\n      stat_function(\n        fun = function(x) {\n          (dnorm(x, mean = mean, sd = sd)) * n * binwidth * lambda\n        }\n      )\n    },\n    mean = em[[\"mu\"]], #mean\n    sd = em[[\"sigma\"]], #standard deviation\n    lambda = em[[\"lambda\"]], #amplitude\n    n = length(df$luminance), #sample size\n    binwidth = 0.007 #binwidth used for histogram\n  )+\n  geom_vline(xintercept = em$mu[1]+(3.29*em$sigma[1]),lty=2,lwd=1,col=\"red\")\n\n\n\n\n\n\n6.6.5 Rename files with high luminance\n\nfor (i in 1:nrow(df)){\n\nif (df$luminance[i] > 40000){file.rename(from = df$pictures[i],to = gsub(x = df$pictures[i],pattern = \".JPG\",replacement = \".light.JPG\"))}\n  \nmessage(str_c(df$pictures[i],\" luminance = \",df$luminance[i]))\n}\n\ndata/photos_for_cleaning//ERCVC3682S.JPG luminance = 21065.1\n\n\ndata/photos_for_cleaning//NPHTK8426K.JPG luminance = 20564.3\n\n\ndata/photos_for_cleaning//OYMPF5938B.JPG luminance = 10660.7\n\n\ndata/photos_for_cleaning//TGLWU3648Q.JPG luminance = 50894.2\n\n\ndata/photos_for_cleaning//VATLY3953X.JPG luminance = 52021\n\n\ndata/photos_for_cleaning//VRAXW6073Z.JPG luminance = 55293.9\n\n\ndata/photos_for_cleaning//VTEEG4048F.JPG luminance = 16634.8\n\n\ndata/photos_for_cleaning//WNQZB2575C.JPG luminance = 17054.4\n\n\ndata/photos_for_cleaning//YPZJU4391B.JPG luminance = 53881.3\n\n\ndata/photos_for_cleaning//YQFYO8330G.JPG luminance = 18454.5\n\n\ndata/photos_for_cleaning//ZIIYN8173Q.JPG luminance = 14055.4"
  },
  {
    "objectID": "photo_cleaning.html#results",
    "href": "photo_cleaning.html#results",
    "title": "6  Photo Cleaning For Sharing",
    "section": "6.7 Results",
    "text": "6.7 Results\nA file flagged as having high luminance\n\nknitr::include_graphics(gsub(x=df$pictures[which(df$luminance==max(df$luminance))],pattern = \".JPG\",replacement = \".light.JPG\"),error = T)\n\n\n\n\nA file flagged as having low luminance is displayed here\n\nknitr::include_graphics(df$pictures[which(df$luminance==min\n                                          (df$luminance))])"
  },
  {
    "objectID": "ODK_geofencing.html#example-part-1---creating-the-geofence",
    "href": "ODK_geofencing.html#example-part-1---creating-the-geofence",
    "title": "15  ODK Geofencing",
    "section": "15.1 Example : Part 1 - Creating the geofence",
    "text": "15.1 Example : Part 1 - Creating the geofence\nIn practice it wouldn’t make a lot of sense to make a grid for the whole of the Earth at very high resolution, as you’re likely to be working on a more controlled area and the data set would get very big. So let’s take Australia as an example. You’ll find Australia within the bounds of latitude -42.0 to -10,0.01 and longitude 110 to 155.0, so the matrix need only extend to these coordinates.\nAt 0.01 (1.1km) resolution, a decomposed grid of points covering the whole of Australia is about 12 million points.\nThis is the start point for an example where we extract points that fall within a multi-polygon shape file and feed these in to ODK as the basis of a geofence. The shape files used here come from the Significant Urban Areas, Urban Centres and Localities of Australia. These are a bunch of open source shapefiles that draw polygons around major urban areas in Australia. These data are provided by the Australian Bureau of Statistics. They can be accessed here.\nThe first steps are done in R and use the libraries tidyverse and sf.\n\nRead the polygon shapefile and remove any problematic polygons [here there are some that aren’t in any significant urban area]\n\n\ndownload.file(\"https://www.abs.gov.au/ausstats/subscriber.nsf/log?openagent&1270055004_sua_2016_aust_shape.zip&1270.0.55.004&Data%20Cubes&1E24D1FB300696D2CA2581B1000E15A5&0&July%202016&09.10.2017&Latest\",destfile = \"data/SUA_2016_AUST.zip\")\n\nunzip(zipfile = \"data/SUA_2016_AUST.zip\",exdir = \"data/\")\n\n\nmap = read_sf(\"data/SUA_2016_AUST.shp\")  \nmap <- filter(map,str_detect(map$SUA_NAME16,\"Not in any Significant Urban Area\")==FALSE)\n\n\nNow create a grid of all global points at 0.1 degree resolution. The expand_grid command is useful for making a matrix this way.\n\nglobal.0.1 <- expand_grid(lat = seq(-40.0,-10,0.01), lon = seq(110,150.0,0.01))\n\n\n\n\nConvert the points of the matrix to coordinates compatible with sf objects\n\npnts_sf <- st_as_sf(global.0.1, coords = c('lon', 'lat'), crs = st_crs(map))\n\n\n\n\nFind the intersections of the decomposed matrix points and the template polygons\n\nThis can take a while. In our tests, 12 million points took about 10 minutes to compute\n\n\npnts_sf <- st_as_sf(global.0.1, coords = c('lon', 'lat'), crs = st_crs(map))\n\n\npnts.intersection <- pnts_sf %>% mutate(\n                          intersection = as.integer(st_intersects(geometry, map)), \n                          area = if_else(is.na(intersection), '', map$SUA_NAME16[intersection])\n                                       )\n\n\n\n\nfilter the resulting table to include only data points that are inside polygons\nTidy this up so that lat and lon are in different fields, and provide the name of the area in the third column\n\ninside.polygons<-tibble(filter(pnts.intersection,area!=\"\")) %>%\n  mutate(\n    geometry = as.character(geometry),\n    geometry=str_sub(string = geometry,start = 3,end = str_length(geometry)-1),\n\n    area = as.factor(area)\n  ) %>%\n  separate(geometry, c(\"lon\", \"lat\"), \", \") %>%\n  mutate(\n    lon=format(round(as.numeric(lon), digits=2)) ,\n    lat=format(round(as.numeric(lat), digits=2)) ,\n    key = str_c(lon,lat,sep = \"|\")) %>%\n  select(-intersection)\n\n\n\n\nFinally, export the list to a csv file, which we will use in ODK\n\nwrite_csv(inside.polygons,\"output/geofence.data.csv\")"
  },
  {
    "objectID": "ODK_geofencing.html#geofence-in-decomposed-csv-format",
    "href": "ODK_geofencing.html#geofence-in-decomposed-csv-format",
    "title": "15  ODK Geofencing",
    "section": "15.2 Geofence in decomposed CSV format",
    "text": "15.2 Geofence in decomposed CSV format\nThe CSV file we just created for Australia at 0.01 degree resolution had 46,000 points that fell within polygons. The table below is a sample of lines"
  },
  {
    "objectID": "ODK_geofencing.html#example-part-2---creating-an-xls-form-that-uses-the-geofence",
    "href": "ODK_geofencing.html#example-part-2---creating-an-xls-form-that-uses-the-geofence",
    "title": "15  ODK Geofencing",
    "section": "15.3 Example : Part 2 - Creating an XLS Form that uses the geofence",
    "text": "15.3 Example : Part 2 - Creating an XLS Form that uses the geofence\nThe XLSForm is very simple. It consists of\n\nA geopoint question, which captures the point that will be tested against the polygons. Here it is a placement-map type, but this works with GPS collected data too.\nA pair of calculations, which extract the first (Latitude) and second (Longitude) data points from the geopoint. These are rounded to the same resolution as the polygon data (here 2 decimal places, 0.01 degrees)\nAnother calculation, which concatenates the resolution-matched geopoint data in the same format found in the geofence data set\nA note, which displays the resolution-matched geopoint\nA further calculation, which grabs exactly zero or one line of matching data from the geofence data set, which is sideloaded as a csv file\nA note, which displays the name of the matching polygon"
  },
  {
    "objectID": "ODK_geofencing.html#example-part-3---in-action",
    "href": "ODK_geofencing.html#example-part-3---in-action",
    "title": "15  ODK Geofencing",
    "section": "15.4 Example : Part 3 - In action",
    "text": "15.4 Example : Part 3 - In action\nThis image shows a negative result, with the geopoint just outside the limits of the Forster - Tuncurry area\n\nWhilst this one shows a positive result, with the geopoint well inside the limits of Forster - Tuncurry area"
  },
  {
    "objectID": "ODK_geofencing.html#performance",
    "href": "ODK_geofencing.html#performance",
    "title": "15  ODK Geofencing",
    "section": "15.5 Performance",
    "text": "15.5 Performance\nThis works fast with 46000 lines (CSV was 1.6 MB) in the geopoint data. We recommend never feeding this a massive data set, but to reduce the geographical scale as you increase the resolution. Remember that XLSForms store GPS data at 6 decimal places, but you aren’t going to get this system to work at that level!\nTo give an example of why not, look at this. Forster - Tuncurry is about 45 km^2. Not a huge area. But as resolution goes up, so the number of points in the matrix expands hugely\n\n\n\n\n\n\n\n\n\nresolution degrees\nGPS decimal places\nresolution (at equator)\npoints in matrix\n\n\n\n\n0.01\n2\n1.1 km\n273\n\n\n0.001\n3\n100 m\n24,926\n\n\n0.0001\n4\n10 m\n2,463,251\n\n\n0.00001\n5\n1 m\n246,020,500\n\n\n\nRemembering that the best resolution the average GPS receiver on a phone will get is about 10 m, we benchmarked performance with 10 metre resolution or 4 decimal places. Conveniently this is also the resolution of the maps that provide the shapefiles.\nThe CSV file is now ~26 MB. Pretty big\nThe form loads on Enketo in about 8 seconds.\nThe resolution is also great\nLook at the boundary of the polygon for Forster - Tuncurry. North of the bridge is inside. South is outside. \nIf we place a marker in ODK on the bridge, it detects the point as being inside the polygon\n\nBut about 10 m south, it detects the point as being outside the polygon\n\nOn ODK Collect, the first time you use this form it will pre-load the data from the CSV, which took about 20 seconds. On subsequent uses, it takes no time at all!\n\nsystem(\"rm data/SUA*\")"
  }
]